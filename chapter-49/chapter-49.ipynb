{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 49: Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code generates the numerical results for Examples 5, 6, 8 and 9  in Chapter 48: Value Function Approximation (vol. II)\n",
    "TEXT: A. H. Sayed, INFERENCE AND LEARNING FROM DATA, Cambridge University Press, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "DISCLAIMER:  This computer code is  provided  \"as is\"   without  any  guarantees.\n",
    "Practitioners  should  use it  at their own risk.  While  the  codes in  the text \n",
    "are useful for instructional purposes, they are not intended to serve as examples \n",
    "of full-blown or optimized designs. The author has made no attempt at optimizing \n",
    "the codes, perfecting them, or even checking them for absolute accuracy. In order \n",
    "to keep the codes at a level  that is  easy to follow by students, the author has \n",
    "often chosen to  sacrifice  performance or even programming elegance in  lieu  of \n",
    "simplicity. Students can use the computer codes to run variations of the examples \n",
    "shown in the text. \n",
    "</div>\n",
    "\n",
    "The Jupyter notebook and python codes are developed by Eduardo Faria Cabrera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "required libraries:\n",
    "    \n",
    "1. numpy\n",
    "2. matplotlib\n",
    "3. tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 49.5 (Playing a game over a grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the operation of the advantage actor--critic (A2C) algorithm (49.77)   by reconsidering the same grid problem from  Fig. 48.1. We assign one-hot encoded feature vectors with the actions $a\\in\\mathbb{A}$ and collect them into a matrix of size $5\\times 5$ (one row per action):\n",
    "\n",
    "$$\n",
    "A=\\begin{bmatrix}1&0&0&0&0\\\\0&1&0&0&0\\\\0&0&1&0&0\\\\0&0&0&1&0\\\\0&0&0&0&1\\end{bmatrix}\n",
    "\\begin{array}{l}\\leftarrow \\textnormal{ up}\\\\\n",
    "\\leftarrow \\textnormal{ down}\\\\\n",
    "\\leftarrow \\textnormal{ left}\\\\\n",
    "\\leftarrow \\textnormal{ right}\\\\\n",
    "\\leftarrow \\textnormal{ stop}\n",
    "\\end{array} \\tag{49.79}\n",
    "$$\n",
    "\n",
    "We continue to employ the same one-hot encoded feature vectors (48.14) from Example 48.1, which we collected into a $17\\times 17$ matrix $H$ with one row corresponding to each state:\n",
    "\n",
    "$$\n",
    "H=\\begin{array}{|ccccccccccccccccc|}\n",
    "1 &0 &0 &0&0&0&0&0&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &1 &0 &0&0&0&0&0&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &1 &0&0&0&0&0&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &1&0&0&0&0&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&1&0&0&0&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&1&0&0&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&1&0&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&1&0&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&1&0&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&1&0&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&0&1&0&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&0&0&1&0&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&0&0&0&1&0&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&0&0&0&0&1&0&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&0&0&0&0&0&1&0&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&0&0&0&0&0&0&1&0\\\\\n",
    "0 &0 &0 &0&0&0&0&0&0&0&0&0&0&0&0&0&0\n",
    "\\end{array}\\begin{array}{l}\\leftarrow 1\\\\\\leftarrow2\\\\\\leftarrow3\\\\\\leftarrow4\\\\\\leftarrow5\\\\\\leftarrow6\\\\\\leftarrow7\\\\\\leftarrow8\\\\\\leftarrow9\\\\\\leftarrow10\\\\\\leftarrow11\\\\\\leftarrow12\\\\\\leftarrow13\\\\\\leftarrow14\\\\\\leftarrow15\\\\\\leftarrow16\\\\\\leftarrow17\\end{array} \\tag{49.80}\n",
    "$$\n",
    "\n",
    "Using the matrices $H$ and $A$, we define feature vectors $f_{s,a}$ for the state--action pairs $(s,a)$ by computing the Kronecker product of $H$ and $A$ as follows:\n",
    "\n",
    "$$\n",
    "F \\overset{\\Delta}{=} H\\otimes A \\tag{49.81}\n",
    "$$\n",
    "\n",
    "The resulting matrix $F$ has dimensions $85\\times 85$, with each row in $F$ corresponding to the transpose of the feature vector $f_{s,a}$ associated with the state--action pair $(s,a)$. For any particular $(s,a)$, the row in $F$ that corresponds to it has index: \n",
    "\n",
    "$$\n",
    "\\textnormal{ row index}\\;=\\;\\Bigl\\{(s-1)\\times |\\mathbb{A}|\\;+\\;a\\Bigr\\},\\;\\;\\;\\begin{array}{l}\n",
    "s=1,2,\\ldots, |\\mathbb{S}|\\\\\n",
    "a=1,2,\\ldots, |\\mathbb{A}|\n",
    "\\end{array} \\tag{49.82}\n",
    "$$\n",
    "\n",
    "where $ |\\mathbb{S}|=17$ and $ |\\mathbb{A}|=5$. We run the A2C algorithm for 1,000,000 iterations using constant step sizes:\n",
    "\n",
    "$$\n",
    "\\mu_1=0.01,\\;\\; \\mu=0.0001, \\;\\;\\rho=0.001,\\;\\;\\gamma=0.9 \\tag{49.83}\n",
    "$$\n",
    "\n",
    "Each iteration involves a run over $E=100$ episodes to train the critic. At the end of the simulation, we evaluate the policies at each state and arrive at the  matrix $\\Pi$ in (49.84), with each row corresponding to one state and each column to one action (i.e., each row corresponds to the distribution $\\pi(a|s)$). \n",
    "The boxes highlight the largest entries in each column and are used to define the \"optimal\" policies at that state.  The resulting optimal actions are represented by arrows in Fig. 49.2. \n",
    "\n",
    "$$\n",
    "\\Pi=\\begin{array}{ccccc|l}\n",
    "\\textnormal{ up}&\\textnormal{ down}&\\textnormal{ left}&\\textnormal{ right}&\\textnormal{ stop}&\\textnormal{ state}\\\\\\hline\n",
    "0.0020  &  \\fbox{0.8653}  &  0.1253   & 0.0074     &    0&\\leftarrow s=1\\\\\n",
    "\\fbox{0.9254}  &  0.0144 &   0.0540 &   0.0061  &       0 &\\leftarrow s=2\\\\\n",
    "\\fbox{0.9156}  &  0.0250  &  0.0337  &  0.0257    &     0&\\leftarrow s=3\\\\\n",
    "0&0&0&0&0&\\leftarrow s=4\\\\\n",
    "\\fbox{0.9735} &   0.0080 &   0.0128  &  0.0057   &      0&\\leftarrow s=5\\\\\n",
    "0.0230 &   0.0094  & \\fbox{ 0.9429}  &  0.0247   &      0&\\leftarrow s=6\\\\\n",
    "\\fbox{0.9840} &   0.0026 &   0.0124   & 0.0011    &     0&\\leftarrow s=7\\\\\n",
    "0&0&0&0&\\fbox{1.0000}&\\leftarrow s=8\\\\\n",
    "\\fbox{0.9940} &   0.0008   & 0.0024   & 0.0027  &       0&\\leftarrow s=9\\\\\n",
    "\\fbox{0.9905}   & 0.0013  &  0.0032  &  0.0050   &      0&\\leftarrow s=10\\\\\n",
    "0&0&0&0&0&\\leftarrow s=11\\\\\n",
    "\\fbox{0.9878}  &  0.0025 &   0.0049  &  0.0049     &    0&\\leftarrow s=12\\\\\n",
    "0.0058  &  0.0027   & 0.0037   & \\fbox{0.9878}     &    0&\\leftarrow s=13\\\\\n",
    "0.0030 &   0.0030  &  0.0016  &  \\fbox{0.9924} &        0&\\leftarrow s=14\\\\\n",
    " 0.0019  &  0.0010   & 0.0009  &  \\fbox{0.9963}   &      0&\\leftarrow s=15\\\\\n",
    "0&0&0&0&\\fbox{1.0000}&\\leftarrow s=16\\\\\n",
    "0&0&0&0&\\fbox{1.0000}&\\leftarrow s=17\\end{array} \\tag{49.84}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid problem from example 1\n",
    "\n",
    "# states\n",
    "# we include the block locations 4 and 11 for convenience of coding; though they will never be reached\n",
    "states = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17] # s = 17 is the EXIT state\n",
    "NS = len(states) # number of states\n",
    "\n",
    "# actions\n",
    "actions = ['up', 'down', 'left', 'right', 'stop']\n",
    "NA = len(actions) # number of actions\n",
    "\n",
    "# rewards\n",
    "reward = -0.1*np.ones(NS)\n",
    "reward[7] = -10 # reward at state s = 8\n",
    "reward[15] = +10 # reward at state s = 16\n",
    "reward[16] = 0 # reward at exit satate s = 17\n",
    "\n",
    "# target policy pi(a|s)\n",
    "Pi = np.zeros((NA, NS)) # matrix Pi specifies the policy pi(a|s)\n",
    "                      # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Pi[0,j] = 1/4 # up\n",
    "        Pi[1,j] = 1/4 # down\n",
    "        Pi[2,j] = 1/4 # left\n",
    "        Pi[3,j] = 1/4 # right\n",
    "        Pi[4,j] = 0  # STOP\n",
    "    \n",
    "    else:\n",
    "        Pi[0,j] = 0 # up\n",
    "        Pi[1,j] = 0 # down\n",
    "        Pi[2,j] = 0 # left\n",
    "        Pi[3,j] = 0 # right\n",
    "        Pi[4,j] = 1 # STOP\n",
    "\n",
    "# transition kernel\n",
    "P = np.zeros((NS, NA, NS)) # entries are Prob(s, a, s')\n",
    "\n",
    "P[0, 0, 0] = 0.15 # start at s=1, move UP, end in state 1\n",
    "P[0, 0, 1] = 0.15\n",
    "P[0, 0, 7] = 0.7\n",
    "\n",
    "P[0, 1, 0] = 0.85 # start at s=1, move DOWN, end in state 1\n",
    "P[0, 1, 1] = 0.15\n",
    "\n",
    "P[0,2,0] = 0.15 # start at s=1, move LEFT, end in state 1\n",
    "P[0,2,1] = 0.70\n",
    "P[0,2,7] = 0.15\n",
    "\n",
    "P[0,3,0] = 0.85 # start at s=1, move RIGHT, end in state 1\n",
    "P[0,3,7] = 0.15\n",
    "\n",
    "P[1,0,0] = 0.15  # start at s=2, move UP, end in state 1\n",
    "P[1,0,2] = 0.15\n",
    "P[1,0,6] = 0.70\n",
    "\n",
    "P[1,1,0] = 0.15  # start at s=2, move DOWN, end in state 1\n",
    "P[1,1,2] = 0.15\n",
    "P[1,1,1] = 0.70\n",
    "                \n",
    "P[1,2,2] = 0.70  # start at s=2, move LEFT, end in state 3                  \n",
    "P[1,2,6] = 0.15\n",
    "P[1,2,1] = 0.15\n",
    "\n",
    "P[1,3,0] = 0.70   # start at s=2, move RIGHT, end in state 1 \n",
    "P[1,3,6] = 0.15\n",
    "P[1,3,1] = 0.15\n",
    "\n",
    "P[2,0,5] = 0.70   # start at s=3, move UP\n",
    "P[2,0,2] = 0.15\n",
    "P[2,0,1] = 0.15\n",
    "\n",
    "P[2,1,2] = 0.85  # start at s=3, move DOWN\n",
    "P[2,1,1] = 0.15\n",
    "\n",
    "P[2,2,2] = 0.85  # start at s=3, move LEFT\n",
    "P[2,2,5] = 0.15 \n",
    "\n",
    "P[2,3,1] = 0.70   # start at s=3, move RIGHT\n",
    "P[2,3,5] = 0.15\n",
    "P[2,3,4] = 0.15\n",
    "\n",
    "P[4,0,11] = 0.70  # start at s=5, move UP\n",
    "P[4,0,4]  = 0.15\n",
    "P[4,0,5]  = 0.15\n",
    "\n",
    "P[3,0,3] = 1 # values for location 4 this state is never reached\n",
    "P[3,1,3] = 1 # so these values are irrelevant\n",
    "P[3,2,3] = 1\n",
    "P[3,3,3] = 1\n",
    "P[3,4,3] = 1\n",
    "\n",
    "P[4,1,4] = 0.85  # start at s=5, move DOWN\n",
    "P[4,1,5] = 0.15\n",
    "\n",
    "P[4,2,4]  = 0.85  # start at s=5, move LEFT\n",
    "P[4,2,11] = 0.15\n",
    "\n",
    "P[4,3,5]  = 0.70  # start at s=5, move RIGHT\n",
    "P[4,3,11] = 0.15\n",
    "P[4,3,4]  = 0.15\n",
    "\n",
    "P[5,0,4] = 0.15   # start at s=6, move UP\n",
    "P[5,0,5] = 0.70\n",
    "P[5,0,6] = 0.15\n",
    "\n",
    "P[5,1,2] = 0.70    # start at s=6, move DOWN\n",
    "P[5,1,4] = 0.15\n",
    "P[5,1,6] = 0.15\n",
    "\n",
    "P[5,2,4] = 0.70   # start at s=6, move LEFT\n",
    "P[5,2,5] = 0.15\n",
    "P[5,2,2] = 0.15\n",
    "\n",
    "P[5,3,6] = 0.70   # start at s=6, move RIGHT\n",
    "P[5,3,5] = 0.15\n",
    "P[5,3,2] = 0.15\n",
    "\n",
    "P[6,0,9] = 0.70  # start at s=7, move UP\n",
    "P[6,0,5]  = 0.15\n",
    "P[6,0,7]  = 0.15\n",
    "\n",
    "P[6,1,1] = 0.70  # start at s=7, move DOWN\n",
    "P[6,1,5] = 0.15\n",
    "P[6,1,7] = 0.15\n",
    "\n",
    "P[6,2,5]  = 0.70 # start at s=7, move LEFT\n",
    "P[6,2,9] = 0.15\n",
    "P[6,2,1]  = 0.15\n",
    "\n",
    "P[6,3,7] = 0.70  # start at s=7, move RIGHT\n",
    "P[6,3,1] = 0.15\n",
    "P[6,3,9] = 0.15\n",
    "\n",
    "P[7,0,16] = 0   # start at s=8 [DANGER] EXIT\n",
    "P[7,1,16] = 0\n",
    "P[7,2,16] = 0\n",
    "P[7,3,16] = 0\n",
    "P[7,4,16] = 1 #STOP action\n",
    "\n",
    "P[8,0,15] = 0.70   # start at s=9 move UP\n",
    "P[8,0,9] = 0.15\n",
    "P[8,0,8]  = 0.15\n",
    "\n",
    "P[8,1,7]  = 0.70   # start at s=9 move DOWN\n",
    "P[8,1,9] = 0.15\n",
    "P[8,1,8]  = 0.15\n",
    "\n",
    "P[8,2,9] = 0.70  # start at s=9 move LEFT\n",
    "P[8,2,15] = 0.15\n",
    "P[8,2,7]  = 0.15\n",
    "\n",
    "P[8,3,8]  = 0.70  # start at s=9 move RIGHT\n",
    "P[8,3,7]  = 0.15\n",
    "P[8,3,15] = 0.15\n",
    "\n",
    "P[9,0,14] = 0.70   # start at s=10 move UP\n",
    "P[9,0,8]  = 0.15\n",
    "P[9,0,9] = 0.15\n",
    "\n",
    "P[9,1,6]  = 0.70  # start at s=10 move DOWN\n",
    "P[9,1,8]  = 0.15\n",
    "P[9,1,9] = 0.15\n",
    "\n",
    "P[9,2,9] = 0.70  # start at s=10 move LEFT\n",
    "P[9,2,14] = 0.15\n",
    "P[9,2,6]  = 0.15\n",
    "\n",
    "P[9,3,8]  = 0.70   # start at s=10 move RIGHT\n",
    "P[9,3,6]  = 0.15\n",
    "P[9,3,14] = 0.15\n",
    "\n",
    "P[10,0,3] = 1 # values for location 11 this state is never reached\n",
    "P[10,1,3] = 1 # so these values are irrelevant\n",
    "P[10,2,3] = 1\n",
    "P[10,3,3] = 1\n",
    "P[10,4,3] = 1\n",
    "\n",
    "P[11,0,12] = 0.70  # start at s=12 move UP\n",
    "P[11,0,11] = 0.30\n",
    "\n",
    "P[11,1,4]  = 0.70  # start at s=12 move DOWN\n",
    "P[11,1,11] = 0.30\n",
    "\n",
    "P[11,2,12] = 0.15  # start at s=12 move LEFT\n",
    "P[11,2,4]  = 0.15\n",
    "P[11,2,11] = 0.70\n",
    "\n",
    "P[11,3,11] = 0.70  # start at s=12 move RIGHT\n",
    "P[11,3,4]  = 0.15\n",
    "P[11,3,12] = 0.15\n",
    "\n",
    "P[12,0,12] = 0.85 # start at s=13 move UP\n",
    "P[12,0,13] = 0.15\n",
    "\n",
    "P[12,1,11] = 0.70  # start at s=13 move DOWN\n",
    "P[12,1,12] = 0.15\n",
    "P[12,1,13] = 0.15\n",
    "\n",
    "P[12,2,12] = 0.85 # start at s=13 move LEFT\n",
    "P[12,2,11] = 0.15\n",
    "\n",
    "P[12,3,13] = 0.70  # start at s=13 move RIGHT\n",
    "P[12,3,11] = 0.15\n",
    "P[12,3,12] = 0.15\n",
    "\n",
    "P[13,0,13] = 0.70 # start at s=14 move UP\n",
    "P[13,0,12] = 0.15\n",
    "P[13,0,14] = 0.15\n",
    "\n",
    "P[13,1,13] = 0.70  # start at s=14 move DOWN\n",
    "P[13,1,12] = 0.15\n",
    "P[13,1,14] = 0.15\n",
    "\n",
    "P[13,2,12] = 0.70  # start at s=14 move LEFT\n",
    "P[13,2,13] = 0.30\n",
    "\n",
    "P[13,3,14] = 0.70  # start at s=14 move RIGHT\n",
    "P[13,3,13] = 0.30\n",
    "\n",
    "P[14,0,14] = 0.70  # start at s=15 move UP\n",
    "P[14,0,13] = 0.15\n",
    "P[14,0,15] = 0.15\n",
    "\n",
    "P[14,1,9] = 0.70   # start at s=15 move DOWN\n",
    "P[14,1,13] = 0.15\n",
    "P[14,1,15] = 0.15\n",
    "\n",
    "P[14,2,13] = 0.70  # start at s=15 move LEFT\n",
    "P[14,2,9] = 0.15\n",
    "P[14,2,14] = 0.15\n",
    "\n",
    "P[14,3,15] = 0.70   # start at s=15 move RIGHT\n",
    "P[14,3,9] = 0.15\n",
    "P[14,3,14] = 0.15\n",
    "\n",
    "P[15,0,16] =0   # start at s=16 [REWARD] EXIT\n",
    "P[15,1,16] =0\n",
    "P[15,2,16] =0\n",
    "P[15,3,16] =0\n",
    "P[15,4,16] =1 # STOP action\n",
    "\n",
    "P[16,0,16] = 0\n",
    "P[16,1,16] = 0\n",
    "P[16,2,16] = 0\n",
    "P[16,3,16] = 0\n",
    "P[16,4,16] = 1 # EXIT state\n",
    "\n",
    "# Computing rpi(s)\n",
    "rpi = np.zeros(NS)\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for a in range(NA):\n",
    "        for sprime in range(NS):\n",
    "            rpi[s] += policy[a]*P[s, a, sprime]*reward[s]\n",
    "\n",
    "# Computing P^{\\pi}\n",
    "Ppi = np.zeros((NS, NS))\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for sprime in range(NS):\n",
    "        for a in range(NA):\n",
    "            Ppi[s, sprime] += policy[a]*P[s, a, sprime]\n",
    "\n",
    "# behavior policy phi(a|s) used to simulate off-policy algorithms\n",
    "Phi = np.zeros((NA, NS)) # matri Phi specifies the behavior policy phi(a|s)\n",
    "                         # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Phi[0,j] = 3/8 # up\n",
    "        Phi[1,j] = 1/8 # down\n",
    "        Phi[2,j] = 2/6 # left\n",
    "        Phi[3,j] = 1/6 # right\n",
    "        Phi[4,j] = 0  # STOP\n",
    "    else:\n",
    "        Phi[0,j] = 0 # up\n",
    "        Phi[1,j] = 0 # down\n",
    "        Phi[2,j] = 0 # left\n",
    "        Phi[3,j] = 0 # right\n",
    "        Phi[4,j] = 1  # STOP\n",
    "\n",
    "# one-hot encoding for the actions\n",
    "A = np.zeros((5, 5))\n",
    "A[0, :] = np.array([1, 0, 0, 0, 0]) # up\n",
    "A[1, :] = np.array([0, 1, 0, 0, 0]) # down\n",
    "A[2, :] = np.array([0, 0, 1, 0, 0]) # left\n",
    "A[3, :] = np.array([0, 0, 0, 1, 0]) # right\n",
    "A[4, :] = np.array([0, 0, 0, 0, 1]) # STOP\n",
    "\n",
    "# 4x1 reduced feature vectors with four binary entries\n",
    "# is agent on same row as SUCCESS\n",
    "# is agent on same row as DANGER\n",
    "# is agent in rightmost two columns\n",
    "# is agent in leftmost two columns\n",
    "\n",
    "# reduced features for state-value function\n",
    "# no offset is included in the feature vectors since v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Mr = 4\n",
    "Hr = np.zeros((NS, Mr))\n",
    "Hr[0,:]  = np.array([0, 0, 1, 0]) # state 1\n",
    "Hr[1,:]  = np.array([0, 0, 1, 0]) # state 2\n",
    "Hr[2,:]  = np.array([0, 0, 0, 1]) # state 3\n",
    "Hr[3,:]  = np.array([0, 0, 0, 0]) # not a valid state\n",
    "Hr[4,:]  = np.array([0, 1, 0, 1]) # state 5...\n",
    "Hr[5,:]  = np.array([0, 1, 0, 1])\n",
    "Hr[6,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[7,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[8,:]  = np.array([0, 0, 1, 0])\n",
    "Hr[9,:] = np.array([0, 0, 1, 0])\n",
    "Hr[10,:] = np.array([0, 0, 0, 0]) # not a valid state \n",
    "Hr[11,:] = np.array([0, 0, 0, 1])\n",
    "Hr[12,:] = np.array([1, 0, 0, 1])\n",
    "Hr[13,:] = np.array([1, 0, 0, 1])\n",
    "Hr[14,:] = np.array([1, 0, 1, 0])\n",
    "Hr[15,:] = np.array([1, 0, 1, 0]) # state 16\n",
    "Hr[16,:] = np.array([0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fr = np.kron(Hr, A) # Kronecker product of dimensions (NSxNA) x (MrxNA)\n",
    "Tr = Mr*NA\n",
    "\n",
    "# one-hot encoded feature vectors for state-value function\n",
    "# no offset is included in the feature vectors because v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Me = NS\n",
    "He = np.zeros((NS, Me))\n",
    "He[0,:]   = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 1\n",
    "He[1,:]   = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 2\n",
    "He[2,:]   = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 3\n",
    "He[3,:]   = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[4,:]   = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 5\n",
    "He[5,:]   = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # ...\n",
    "He[6,:]   = np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[7,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[8,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[9,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[10,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[11,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "He[12,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "He[13,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "He[14,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
    "He[15,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]) # state 16\n",
    "He[16,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fe = np.kron(He, A) # Kronecker product of dimensions (NSxNA) x (MexNA)\n",
    "Te = Me*NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [2:35:45<00:00,  3.21it/s]     \n"
     ]
    }
   ],
   "source": [
    "# Advantage actor-critic (A2C) algorithm for optimal policy design\n",
    "\n",
    "E = 100 # number of episodes within each iteration\n",
    "max_episode_duration = 50\n",
    "delta = 0\n",
    "gamma = 0.9\n",
    "mu = 0.01 # step-size for critic (w)\n",
    "mug = 0.001 # step-size for actor (theta)\n",
    "iter_ = 30000 # repeat for this many iterations to learn theta; each iteration has E episodes\n",
    "use_reduced_features = 0 # set to zero to use the one-hot encoded extended feature\n",
    "\n",
    "if use_reduced_features == 1:\n",
    "    H = Hr \n",
    "    M = Mr \n",
    "    F = Fr \n",
    "    T = Tr \n",
    "else:\n",
    "    H = He \n",
    "    M = Me \n",
    "    F = Fe \n",
    "    T = Te \n",
    "\n",
    "w = np.zeros(M) # linear value function model; includes bias coefficient\n",
    "theta = np.zeros(T) # parameter for Gibbs distribution\n",
    "\n",
    "saved_states = [] # need to save states, features, rewards actions during all episodes\n",
    "saved_features = []\n",
    "saved_rewards = []\n",
    "saved_actions = []\n",
    "\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for m in tqdm(range(iter_)): # each iteration involves multiple episodes\n",
    "    # Current Gibbs distribution using current model theta\n",
    "    Pi_theta = compute_policy(F,theta,T,NA,NS,Pi)\n",
    "    counter_transitions = 1 # counts # of state transitions during this iteration\n",
    "\n",
    "    for e in range(E): # iterates over episodes\n",
    "        counter = 0\n",
    "        sample = 1\n",
    "        while sample == 1:\n",
    "            idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "            if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "                s = states[idx-1]\n",
    "                sample = 0\n",
    "\n",
    "        while (s!=17) and (counter < max_episode_duration): # state s different from EXIT state\n",
    "            h = H[s, :].T # state feature vector \n",
    "            policy = Pi_theta[:, s] # pi(a|s; theta) at state s\n",
    "            act = select_action(policy)\n",
    "\n",
    "            for j in range(NS):\n",
    "                kernel[j] = P[s, act, j]\n",
    "            \n",
    "            sprime = select_next_state(kernel) # next state\n",
    "            hprime = H[sprime, :].T # next feature\n",
    "\n",
    "            # CRITIC for state value function learning\n",
    "            r = reward[s] # in this example reward is only state s-dependent\n",
    "            delta = r + (gamma*hprime-h).T@w\n",
    "            w += mu*delta*h \n",
    "\n",
    "            #saved visited states, actions, rewards\n",
    "            saved_features.append([h, hprime])\n",
    "            saved_states.append([s, sprime])\n",
    "            saved_rewards.append(r)\n",
    "            saved_actions.append(act)\n",
    "\n",
    "            s = sprime # set next sample\n",
    "            counter += 1\n",
    "            counter_transitions += 1 # plays role of N\n",
    "    wo = w.copy() # learned CRITIC MODEL\n",
    "\n",
    "    # ACTOR\n",
    "    N = counter_transitions - 1\n",
    "    sum_g = np.zeros(T)\n",
    "\n",
    "    for nn in range(N):\n",
    "        hx = saved_features[nn][0]\n",
    "        hxprime = saved_features[nn][1]\n",
    "        sx = saved_states[nn][0]\n",
    "        sxprime = saved_states[nn][1]\n",
    "        rx = saved_rewards[nn]\n",
    "        ax = saved_actions[nn]\n",
    "\n",
    "        deltax = rx + (gamma*hxprime - hx).T@wo \n",
    "        g_vec = compute_gradient_log_pi(F,Pi_theta,sx,ax,NA) # gradient of log pi\n",
    "        sum_g += deltax*g_vec\n",
    "    sum_g = sum_g/N \n",
    "    theta += mug*sum_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 up\n",
      "2 left\n",
      "3 up\n",
      "4 stop\n",
      "5 up\n",
      "6 left\n",
      "7 up\n",
      "8 stop\n",
      "9 up\n",
      "10 up\n",
      "11 stop\n",
      "12 up\n",
      "13 right\n",
      "14 right\n",
      "15 right\n",
      "16 stop\n",
      "17 stop\n",
      "policy AxS from ADVANTAGE (A2C) actor-critic algorithm\n",
      "[[0.25       0.24975992 0.25921322 0.         0.26622445 0.25720326\n",
      "  0.33403165 0.         0.31233695 0.30668719 0.         0.27029995\n",
      "  0.25375439 0.24507499 0.26901846 0.         0.        ]\n",
      " [0.25       0.24902915 0.24818634 0.         0.2518691  0.24656925\n",
      "  0.22680526 0.         0.1750736  0.18271397 0.         0.22957249\n",
      "  0.22975958 0.23821982 0.22405228 0.         0.        ]\n",
      " [0.25       0.28095767 0.25624102 0.         0.25546835 0.27963903\n",
      "  0.30373133 0.         0.23153432 0.25495129 0.         0.25105832\n",
      "  0.23354345 0.20869731 0.203011   0.         0.        ]\n",
      " [0.25       0.22025327 0.23635943 0.         0.22643811 0.21658846\n",
      "  0.13543176 0.         0.28105512 0.25564755 0.         0.24906925\n",
      "  0.28294258 0.30800787 0.30391826 0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         1.         0.         0.         1.         0.\n",
      "  0.         0.         0.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "thetao = theta \n",
    "Pi_theta = compute_policy(F,thetao,T,NA,NS,Pi) # optimal policy\n",
    "max_values = Pi_theta.max(axis=0)\n",
    "indexes = np.argmax(Pi_theta, axis=0)\n",
    "\n",
    "act = [0]*NS\n",
    "action_states = [0]*NS\n",
    "for s in range(NS):\n",
    "    act[s] = indexes[s] # indexes of the permissible action  \n",
    "    action_states[s] = actions[act[s]]\n",
    "    print(s+1, action_states[s])\n",
    "\n",
    "print(\"policy AxS from ADVANTAGE (A2C) actor-critic algorithm\")\n",
    "print(Pi_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 49.6 (Playing a game over a grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the operation of the natural-gradient actor--critic algorithm (49.91)   by reconsidering the same grid problem from  Fig. 48.1. We employ the same encoding for states $s$ and state--action pairs $(s,a)$ from Example 49.5.  We run the algorithm for 10,000 iterations using constant step sizes\n",
    "\n",
    "$$\n",
    "\\mu_1=0.01,\\;\\; \\mu=0.0001, \\;\\;\\rho=0.001,\\;\\;\\gamma=0.9 \\tag{49.92}\n",
    "$$\n",
    "\n",
    "Each iteration involves a run over $E=100$ episodes to train the critic. At the end of the simulation, we evaluate the policies at each state and arrive at the  matrix $\\Pi$ in (49.93), with each row corresponding to one state and each column to one action  (i.e., each row corresponds to the distribution $\\pi(a|s)$).  \n",
    "The boxes highlight the largest entries in each column and are used to define the \"optimal\" policies at that state.  The resulting optimal actions are represented by arrows in Fig. 49.3. \n",
    " \n",
    "$$\n",
    "\\Pi=\\begin{array}{ccccc|l}\n",
    "\\textnormal{ up}&\\textnormal{ down}&\\textnormal{ left}&\\textnormal{ right}&\\textnormal{ stop}&\\textnormal{ state}\\\\\\hline\n",
    "0.0000  &  \\fbox{0.9933}  &  0.0067  &  0.0000   &      0&\\leftarrow s=1\\\\\n",
    "\\fbox{0.9825}  &  0.0008  &  0.0168  &  0.0000  &       0&\\leftarrow s=2\\\\\n",
    "\\fbox{ 0.8348} &   0.0198 &   0.0341   & 0.1113 &        0&\\leftarrow s=3\\\\\n",
    "0    &     0    &     0   &      0    &     0&\\leftarrow s=4\\\\\n",
    "\\fbox{0.9831} &   0.0013 &   0.0155 &   0.0001   &      0&\\leftarrow s=5\\\\\n",
    "0.0657  &  0.0013 &   \\fbox{0.5922} &   0.3408   &      0&\\leftarrow s=6\\\\\n",
    "\\fbox{0.9983}  &  0.0000 &   0.0017 &   0.0000  &       0&\\leftarrow s=7\\\\\n",
    "0      &   0      &   0      &   0  &  \\fbox{1.0000}&\\leftarrow s=8\\\\\n",
    "\\fbox{1.0000} &   0.0000 &   0.0000 &   0.0000 &        0&\\leftarrow s=9\\\\\n",
    "\\fbox{0.9951} &   0.0000 &   0.0000 &   0.0049 &        0&\\leftarrow s=10\\\\\n",
    "0      &   0      &   0      &   0   &      0&\\leftarrow s=11\\\\\n",
    "\\fbox{0.9974} &   0.0000 &   0.0013 &   0.0013  &       0&\\leftarrow s=12\\\\\n",
    "0.0032 &   0.0000 &   0.0001 &   \\fbox{0.9967}  &       0&\\leftarrow s=13\\\\\n",
    "0.0001 &   0.0001 &   0.0000 &   \\fbox{0.9998} &        0&\\leftarrow s=14\\\\\n",
    "0.0000 &   0.0000 &   0.0000 &   \\fbox{1.0000} &        0&\\leftarrow s=15\\\\\n",
    "0      &   0      &   0      &   0   & \\fbox{1.0000}&\\leftarrow s=16\\\\\n",
    "0      &   0      &   0      &   0   & \\fbox{1.0000}&\\leftarrow s=17\n",
    "\\end{array} \\tag{49.93}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 457.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 up\n",
      "2 up\n",
      "3 right\n",
      "4 stop\n",
      "5 up\n",
      "6 right\n",
      "7 up\n",
      "8 stop\n",
      "9 up\n",
      "10 up\n",
      "11 stop\n",
      "12 up\n",
      "13 right\n",
      "14 right\n",
      "15 right\n",
      "16 stop\n",
      "17 stop\n",
      "policy AxS from NATURAL actor critic algorithm\n",
      "[[2.50000000e-01 9.96811025e-01 7.61399945e-02 0.00000000e+00\n",
      "  9.99995654e-01 6.33866948e-04 9.99985042e-01 0.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  1.80143837e-15 4.34798524e-21 7.70863360e-26 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.50000000e-01 2.34082025e-04 5.84616955e-03 0.00000000e+00\n",
      "  2.67094933e-07 1.02884545e-04 3.55168431e-14 0.00000000e+00\n",
      "  3.68332691e-39 1.24565357e-26 0.00000000e+00 6.87469592e-14\n",
      "  6.96842989e-18 1.97882261e-21 1.43232035e-30 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.50000000e-01 1.15305088e-04 4.13189217e-03 0.00000000e+00\n",
      "  3.79221104e-06 1.03735388e-03 1.49584110e-05 0.00000000e+00\n",
      "  4.21030226e-32 1.12328402e-21 0.00000000e+00 7.62091515e-12\n",
      "  2.37084012e-18 1.04396673e-25 6.44393882e-33 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.50000000e-01 2.83958740e-03 9.13881944e-01 0.00000000e+00\n",
      "  2.86299657e-07 9.98225895e-01 9.28852176e-24 0.00000000e+00\n",
      "  4.78814440e-33 9.18145806e-15 0.00000000e+00 2.00732538e-11\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Natural gradient actor-critic algorithm\n",
    "\n",
    "E = 100 # number of episodes within each iteration\n",
    "max_episode_duration = 50\n",
    "delta = 0\n",
    "gamma = 0.9\n",
    "mu = 0.01 # step-size for critic (w)\n",
    "mug = 0.001 # step-size for actor (theta)\n",
    "iter_ = 10000 # repeat for this many iterations to learn theta; each iteration has E episodes\n",
    "use_reduced_features = 0 # set to zero to use the one-hot encoded extended feature\n",
    "\n",
    "if use_reduced_features == 1:\n",
    "    H = Hr \n",
    "    M = Mr \n",
    "    F = Fr \n",
    "    T = Tr \n",
    "else:\n",
    "    H = He \n",
    "    M = Me \n",
    "    F = Fe \n",
    "    T = Te \n",
    "\n",
    "c = np.zeros(T) # linear value function model\n",
    "theta = np.zeros(T) # parameter for Gibbs distribution\n",
    "\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for m in tqdm(range(iter_)): # each iteration involves multiple episodes\n",
    "    # Current Gibbs distribution using current model theta\n",
    "    Pi_theta = compute_policy(F,theta,T,NA,NS,Pi)\n",
    "\n",
    "    for e in range(E): # iterates over episodes\n",
    "        counter = 0\n",
    "        sample = 1\n",
    "        while sample == 1:\n",
    "            idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "            if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "                s = states[idx-1]\n",
    "                sample = 0\n",
    "\n",
    "    h = H[s, :].T # initial state feature vector\n",
    "    policy = Pi_theta[:, s] # pi(a|s; theta) at this state s\n",
    "    act = select_action(policy) # initial conditions\n",
    "    idx = (s-1)*NA+act \n",
    "    f = F[int(idx), :].T # initial f vector\n",
    "\n",
    "    while (s!=17) and (counter < max_episode_duration): # state s different from EXIT state\n",
    "        for j in range(NS):\n",
    "            kernel[j] = P[s, act, j]\n",
    "        sprime = select_next_state(kernel) # next state\n",
    "        hprime = H[sprime, :].T # next feature\n",
    "        policyprime = Pi_theta[:, sprime] # pi(a|sprime; theta) at state prime\n",
    "        actprime = select_action(policyprime)\n",
    "        idx_prime = (sprime-1)*NA + actprime\n",
    "        fprime = F[int(idx_prime), :].T \n",
    "\n",
    "        # CRITIC for state-action value function learning\n",
    "        r = reward[s] # in this example, reward is only state s-dependent\n",
    "        beta = r + (gamma*fprime - f).T@c\n",
    "        c += mu*beta*f \n",
    "        s = sprime # set next sample\n",
    "        act = actprime \n",
    "        f = fprime \n",
    "        counter += 1\n",
    "    co = c.copy() # learned CRITIC MODEL\n",
    "\n",
    "    #ACTOR\n",
    "    theta += mug*co \n",
    "\n",
    "thetao = theta \n",
    "Pi_theta = compute_policy(F,thetao,T,NA,NS,Pi) # optimal policy\n",
    "max_values = Pi_theta.max(axis=0)\n",
    "indexes = np.argmax(Pi_theta, axis=0)\n",
    "\n",
    "act = [0]*NS\n",
    "action_states = [0]*NS\n",
    "for s in range(NS):\n",
    "    act[s] = indexes[s] # indexes of the permissible action  \n",
    "    action_states[s] = actions[act[s]]\n",
    "    print(s+1, action_states[s])\n",
    "\n",
    "print(\"policy AxS from NATURAL actor critic algorithm\")\n",
    "print(Pi_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 49.8 (Playing a game over a grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the operation of the PPO-clip method (49.163)   by reconsidering the same grid problem from  Fig. 48.1. We employ the same encoding for states $s$ and state--action pairs $(s,a)$ from Example 49.5.  For illustration purposes, we run the algorithm  for  1,000,000 iterations using constant step sizes throughout the simulation:\n",
    "\n",
    "$$\n",
    "\\mu_1=0.01,\\;\\; \\mu=0.0001, \\;\\;\\rho=0.001,\\;\\;\\gamma=0.9 \\tag{49.164}\n",
    "$$\n",
    "\n",
    "Each iteration involves a run over $E=100$ episodes to train the critic. At the end of the simulation, we evaluate the policies at each state and arrive at the  matrix $\\Pi$ in (49.165), with each row corresponding to one state and each column to one action  (i.e., each row corresponds to the distribution $\\pi(a|s)$).  \n",
    "The boxes highlight the largest entries in each column and are used to define the \"optimal\" policies at that state.  The resulting optimal actions are represented by arrows in Fig. 49.4. \n",
    " \n",
    "$$\n",
    "\\Pi=\\begin{array}{ccccc|l}\n",
    "\\textnormal{ up}&\\textnormal{ down}&\\textnormal{ left}&\\textnormal{ right}&\\textnormal{ stop}&\\textnormal{ state}\\\\\\hline\n",
    "0.0469  &  0.2984 &   \\fbox{0.5393} &   0.1154 &        0&\\leftarrow s=1\\\\\n",
    "\\fbox{0.4030} &   0.1399 &   0.3889 &   0.0681 &        0&\\leftarrow s=2\\\\\n",
    "\\fbox{0.4543} &   0.1747 &   0.2237 &   0.1472 &        0&\\leftarrow s=3\\\\\n",
    "0      &   0      &   0      &   0      &   0&\\leftarrow s=4\\\\\n",
    "\\fbox{0.6573} &   0.1065 &   0.1546  &  0.0817 &        0&\\leftarrow s=5\\\\\n",
    "0.2107 &   0.1208 &   \\fbox{0.4925} &   0.1760 &        0&\\leftarrow s=6\\\\\n",
    "\\fbox{0.8450} &   0.0313 &   0.1090  &  0.0147 &        0&\\leftarrow s=7\\\\\n",
    "0      &   0      &   0       &  0 &   \\fbox{1.0000}&\\leftarrow s=8\\\\\n",
    "\\fbox{0.9136} &   0.0137 &   0.0352  &  0.0375 &        0&\\leftarrow s=9\\\\\n",
    "\\fbox{0.8824} &   0.0196 &   0.0440  &  0.0541 &        0&\\leftarrow s=10\\\\\n",
    "0      &   0      &   0       &  0      &   0&\\leftarrow s=11\\\\\n",
    "\\fbox{0.8148} &   0.0393 &   0.0730  &  0.0729 &        0&\\leftarrow s=12\\\\\n",
    "0.0823 &   0.0417 &   0.0545 &   \\fbox{0.8215} &        0&\\leftarrow s=13\\\\\n",
    "0.0445 &   0.0445 &   0.0238  &  \\fbox{0.8872} &        0&\\leftarrow s=14\\\\\n",
    "0.0278 &   0.0144 &   0.0134  &  \\fbox{0.9445} &        0&\\leftarrow s=15\\\\\n",
    "0      &   0      &   0      &   0   & \\fbox{1.0000}&\\leftarrow s=16\\\\\n",
    "0      &   0      &   0      &   0  &  \\fbox{1.0000}&\\leftarrow s=17\n",
    "\\end{array} \\tag{49.165}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:51:49<00:00,  1.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 up\n",
      "2 left\n",
      "3 left\n",
      "4 stop\n",
      "5 up\n",
      "6 left\n",
      "7 left\n",
      "8 stop\n",
      "9 up\n",
      "10 up\n",
      "11 stop\n",
      "12 up\n",
      "13 right\n",
      "14 right\n",
      "15 right\n",
      "16 stop\n",
      "17 stop\n",
      "policy AxS from PPO Clipped Cost algorithm\n",
      "[[0.25       0.25007042 0.25017397 0.         0.25035637 0.25017277\n",
      "  0.25117583 0.         0.25201217 0.25124575 0.         0.25064599\n",
      "  0.25025333 0.24981959 0.25073408 0.         0.        ]\n",
      " [0.25       0.25011921 0.25010356 0.         0.25004421 0.25025705\n",
      "  0.25044727 0.         0.24846733 0.24879504 0.         0.2495117\n",
      "  0.24967574 0.24968928 0.24865468 0.         0.        ]\n",
      " [0.25       0.25110673 0.25032682 0.         0.25028457 0.25080987\n",
      "  0.25207201 0.         0.24929606 0.25000439 0.         0.24986916\n",
      "  0.24973089 0.24927058 0.24920315 0.         0.        ]\n",
      " [0.25       0.24870364 0.24939565 0.         0.24931485 0.2487603\n",
      "  0.24630489 0.         0.25022444 0.24995482 0.         0.24997315\n",
      "  0.25034004 0.25122055 0.25140809 0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         1.         0.         0.         1.         0.\n",
      "  0.         0.         0.         1.         1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PPO Clipped Cost Method for optimal policy design\n",
    "\n",
    "E = 100 # number of episodes within each iteration\n",
    "max_episode_duration = 50\n",
    "epsilon = 0.02 # bound of KL divergence\n",
    "ar = np.random.randint(4)+1 # arbitrary reference action from {UP, DOWN, LEFT, RIGHT}\n",
    "delta = 0\n",
    "gamma = 0.9\n",
    "mu = 0.01 # step-size for critic (w)\n",
    "mug = 0.0001 # step-size for actor (theta)\n",
    "iter_ = 10000 # repeat this many iterations to learn theta; each iteration has E episodes\n",
    "use_reduced_features = 0 # set to zero to use the one-hot encoded extended feature\n",
    "\n",
    "if use_reduced_features == 1:\n",
    "    H = Hr \n",
    "    M = Mr \n",
    "    F = Fr \n",
    "    T = Tr \n",
    "else:\n",
    "    H = He \n",
    "    M = Me \n",
    "    F = Fe \n",
    "    T = Te \n",
    "\n",
    "w = np.zeros(M) # linear value funciton model\n",
    "theta = np.zeros(T) # parameter for Gibbs distribution\n",
    "\n",
    "saved_states = [] # neet to save states, features, rewards, actions during all episodes\n",
    "saved_features = []\n",
    "saved_rewards = []\n",
    "saved_actions = []\n",
    "\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for m in tqdm(range(iter_)): # each iterations involves multiple episodes\n",
    "    # Current Gibbs distribution using current model theta\n",
    "    Pi_theta = compute_policy(F, theta, T, NA, NS, Pi)\n",
    "    counter_transitions = 0\n",
    "\n",
    "    for e in range(E): # iterates over episodes\n",
    "        counter = 0\n",
    "        sample = 1 # select initial sample of episodes to feed into network\n",
    "\n",
    "        while sample == 1:\n",
    "            idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "            if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "                s = states[idx-1]\n",
    "                sample = 0\n",
    "\n",
    "        while (s!=17) and (counter < max_episode_duration): # state s different from EXIT\n",
    "            h = H[s, :].T # state feature vector\n",
    "            policy = Pi_theta[:, s] # pi(a|s;theta) at state s\n",
    "            act = select_action(policy)\n",
    "\n",
    "            for j in range(NS):\n",
    "                kernel[j] = P[s, act, j]\n",
    "            \n",
    "            sprime = select_next_state(kernel) # next state\n",
    "            hprime = H[sprime, :].T # next feature\n",
    "\n",
    "            # CRITIC for state value function learning\n",
    "            r = reward[s] # in this example reward is only state s-dependent\n",
    "            delta = r + (gamma*hprime - h).T@w \n",
    "            w += mu*delta*h \n",
    "\n",
    "            # save visited states, actions, rewards, internal signals within neural network\n",
    "            saved_features.append([h, hprime])\n",
    "            saved_states.append([s, sprime])\n",
    "            saved_rewards.append(r)\n",
    "            saved_actions.append(act)\n",
    "\n",
    "            s = sprime # set next sample\n",
    "            counter += 1\n",
    "            counter_transitions += 1\n",
    "    wo = w\n",
    "\n",
    "    # ACTOR\n",
    "    N = counter_transitions - 1\n",
    "    g_vec = np.zeros(T) # g_{m-1}\n",
    "    z_vec = np.zeros(T) # z_{m-1}\n",
    "\n",
    "    # determining the gradient vectors g_{m-1} and z_{m-1}\n",
    "    for nn in range(N):\n",
    "        hx = saved_features[nn][0]\n",
    "        hxprime = saved_features[nn][1]\n",
    "        sx = saved_states[nn][0]\n",
    "        sxprime = saved_states[nn][1]\n",
    "        rx = saved_rewards[nn]\n",
    "        ax = saved_actions[nn]\n",
    "\n",
    "        # determing g_{m-1}\n",
    "        deltax = rx + (gamma*hxprime - hx).T@wo\n",
    "        x_vec = compute_gradient_log_pi(F,Pi_theta,sx,ax,NA) # gradient of log pi =(f-f_bar)= grad ell\n",
    "        g_vec += deltax*x_vec \n",
    "\n",
    "        # determining z_{m-1}\n",
    "        Ax = deltax # estimated advantage value\n",
    "        if Ax >= 0:\n",
    "            cx = (1+epsilon)*Ax \n",
    "        else:\n",
    "            cx = (1-epsilon)*Ax \n",
    "        \n",
    "        diffx = Ax - cx # since ell(theta) = A\n",
    "        if diffx >= 0:\n",
    "            signx = +1\n",
    "        else:\n",
    "            signx = -1\n",
    "        \n",
    "        y_vec = .5*x_vec*Ax*(1-signx) # x_vec is f-f_bar\n",
    "        z_vec += y_vec \n",
    "    \n",
    "    g_vec = g_vec/N \n",
    "    z_vec = z_vec/N \n",
    "    theta_hat = theta + mug*z_vec \n",
    "\n",
    "    KL_div = 0\n",
    "\n",
    "    # determining average KL divergence\n",
    "    for nn in range(N):\n",
    "        sx = saved_states[nn][0]\n",
    "        sum_y = 0\n",
    "        sum_yprime = 0\n",
    "        sum_expyprime = 0\n",
    "        sum_expy = 0\n",
    "\n",
    "        idx_sa_r = (sx-1)*NA + ar \n",
    "        f_sa_r = F[int(idx_sa_r), :].T # ar is reference action\n",
    "\n",
    "        y = np.zeros(NA)\n",
    "        yprime = np.zeros(NA)\n",
    "        expy = np.zeros(NA)\n",
    "        expyprime = np.zeros(NA)\n",
    "        expy2 = np.zeros(NA)\n",
    "        for a in range(NA):\n",
    "            idx_sa = (sx-1)*NA + a \n",
    "            f_sa = F[int(idx_sa), :]\n",
    "            y[a] = (f_sa - f_sa_r).T@theta \n",
    "            yprime[a] = (f_sa - f_sa_r).T@theta_hat\n",
    "            expy[a] = np.exp(y[a])\n",
    "            expyprime[a] = np.exp(yprime[a])\n",
    "            expy2[a] = (np.exp(y[a]))*(y[a]-yprime[a])\n",
    "\n",
    "        sum_expy = expy.sum()\n",
    "        sum_expyprime = expyprime.sum()\n",
    "        sum_expy2 = expy2.sum()\n",
    "\n",
    "        KL_div += (np.log(sum_expyprime/sum_expy)) + (sum_expy2/sum_expy)\n",
    "    KL_div = KL_div / N \n",
    "\n",
    "    if (g_vec.T@theta_hat >= 0) and (KL_div <= epsilon):\n",
    "        theta = theta_hat\n",
    "\n",
    "thetao = theta \n",
    "Pi_theta = compute_policy(F,thetao,T,NA,NS,Pi) # optimal policy\n",
    "max_values = Pi_theta.max(axis=0)\n",
    "indexes = np.argmax(Pi_theta, axis=0)\n",
    "\n",
    "act = [0]*NS\n",
    "action_states = [0]*NS\n",
    "for s in range(NS):\n",
    "    act[s] = indexes[s] # indexes of the permissible action  \n",
    "    action_states[s] = actions[act[s]]\n",
    "    print(s+1, action_states[s])\n",
    "\n",
    "print(\"policy AxS from PPO Clipped Cost algorithm\")\n",
    "print(Pi_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 49.9 (Playing a game over a grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the operation of the deep learning A2C implementation (49.179)  by reconsidering the same grid problem from  Fig. 48.1. We consider a feedforward neural network with $L=4$ layers (including one input layer, one output layer, and two hidden layers). All neurons employ the ReLU activation function except for the output layer, which employs a softmax mapping. The step-size, regularization, and discount factor parameters are set to:\n",
    "\n",
    "$$\n",
    "\\mu_1=0.01,\\;\\;\\;\\mu=0.00001, \\;\\;\\rho=0.001,\\;\\;\\gamma=0.9 \\tag{49.180}\n",
    "$$\n",
    "\n",
    "We employ the same one-hot encoded feature vectors  (49.80). We run the deep learning algorithm for 500,000 iterations to learn the parameters $\\{W_{\\ell},\\theta_{\\ell}\\}$. Each iteration involves a run over $E=100$ episodes to train the critic. At the end of the simulation, we evaluate the policies at each state (we feed feature vector $h_s$ and determine $\\widehat{\\gamma}_s=\\pi^{\\star}(a|s)$). We arrive at the  matrix $\\Pi$ in (49.181), with each row corresponding to one state and each column to one action (i.e., each row corresponds to the distribution $\\pi^{\\star}(a|s)$). \n",
    "The boxes highlight the largest entries in each column and are used to define the \"optimal\" policies at that state.  The resulting optimal actions are represented by arrows in Fig. 49.6.\n",
    " \n",
    "$$\n",
    "\\Pi=\\begin{array}{ccccc|l}\n",
    "\\textnormal{ up}&\\textnormal{ down}&\\textnormal{ left}&\\textnormal{ right}&\\textnormal{ stop}&\\textnormal{ state}\\\\\\hline\n",
    "0.0077  &  0.0014 &   \\fbox{0.9619} &   0.0290 &        0&\\leftarrow s=1\\\\\n",
    "\\fbox{0.9658} &   0.0001 &   0.0342 &   0.000 &        0&\\leftarrow s=2\\\\\n",
    "\\fbox{0.9792} &   0.000 &   0.0208 &   0.0000 &        0&\\leftarrow s=3\\\\\n",
    "0      &   0      &   0      &   0      &   0&\\leftarrow s=4\\\\\n",
    "\\fbox{0.9911} &   0.000 &   0.0089  &  0.000 &        0&\\leftarrow s=5\\\\\n",
    "0.0178 &   0.0016 &   \\fbox{0.9631} &   0.0175 &        0&\\leftarrow s=6\\\\\n",
    "\\fbox{0.9855} &   0.000 &   0.0145  &  0.000 &        0&\\leftarrow s=7\\\\\n",
    "0      &   0      &   0       &  0 &   \\fbox{1.0000}&\\leftarrow s=8\\\\\n",
    "\\fbox{0.9980} &   0.000 &   0.0020  &  0.000 &        0&\\leftarrow s=9\\\\\n",
    "\\fbox{0.9975} &   0.000 &   0.0025  &  0.000 &        0&\\leftarrow s=10\\\\\n",
    "0      &   0      &   0       &  0      &   0&\\leftarrow s=11\\\\\n",
    "\\fbox{0.9960} &   0.000 &   0.0040  &  0.000 &        0&\\leftarrow s=12\\\\\n",
    "0.000 &   0.000 &   0.0043 &   \\fbox{0.9957} &        0&\\leftarrow s=13\\\\\n",
    "0.000 &   0.000 &   0.0019  &  \\fbox{0.9981} &        0&\\leftarrow s=14\\\\\n",
    "0.000 &   0.000 &   0.0012  &  \\fbox{0.9988} &        0&\\leftarrow s=15\\\\\n",
    "0      &   0      &   0      &   0   & \\fbox{1.0000}&\\leftarrow s=16\\\\\n",
    "0      &   0      &   0      &   0  &  \\fbox{1.0000}&\\leftarrow s=17\n",
    "\\end{array} \\tag{49.181}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep advantage actor-critic (A2C) algorithm for optimal policy design\n",
    "\n",
    "E = 100 # number of episodes within each iteration\n",
    "max_episode_duration = 50\n",
    "delta = 0\n",
    "gamma = 0.9\n",
    "epsilon = 0.1 # for epsilon-greedy exploration\n",
    "mu = 0.01 # step-size for critic (w)\n",
    "mug = 0.00001 # step-size for actor (theta)\n",
    "rho = 0.001 # regularization parameter\n",
    "iter_ = 10000 # repeat for this many iterations to learn theta; each iteration has E episodes\n",
    "use_reduced_features = 0 # set to zero to use one-hot encoded etended feature\n",
    "\n",
    "if use_reduced_features == 1:\n",
    "    H = Hr \n",
    "    M = Mr \n",
    "    F = Fr \n",
    "    T = Tr \n",
    "else:\n",
    "    H = He \n",
    "    M = Me \n",
    "    F = Fe \n",
    "    T = Te \n",
    "\n",
    "w = np.zeros(M) # critic model\n",
    "\n",
    "# parameters of neural network\n",
    "L = 4 # number of neural network layers\n",
    "n1 = M # number of neurons in input layer = size of feature vectors without bias entry\n",
    "n2 = 32 # number of neurons in first hidden layer\n",
    "n3 = 32 # number of neurons in second hidden layer\n",
    "n4 = NA # number of neurons in output layer = number of actions available\n",
    "type_activation = [-1, 3, 3, 4] # -1 irrelevant (not used), 0: linear, 1: sigmoid, 2: tanh, 3: rectifier; 4: softmax\n",
    "\n",
    "W1 = (1/np.sqrt(n1))*np.random.randn(n2, n1) # weight matrices\n",
    "W2 = (1/np.sqrt(n2))*np.random.randn(n3, n2) \n",
    "W3 = (1/np.sqrt(n3))*np.random.randn(n4, n3)\n",
    "Wcell = [W1, W2, W3] # a cell array containing the weight matrices of different dimensions \n",
    "Wcell_before = Wcell.copy()\n",
    "\n",
    "theta1 = np.random.randn(n2) # bias vectors; NOT USED in this simulation\n",
    "theta2 = np.random.randn(n3)\n",
    "theta3 = np.random.randn(n4)\n",
    "ThetaCell = [theta1, theta2, theta3] # a cell array for the thetas\n",
    "ThetaCell_before = ThetaCell.copy()\n",
    "\n",
    "use_theta = 1 # need to use theta when output is softmax, otherwise h = 0 maps into uniform distribution for state 17\n",
    "\n",
    "saved_states = [] # neet to save states, features, rewards, actions during all episodes\n",
    "saved_features = []\n",
    "saved_rewards = []\n",
    "saved_actions = []\n",
    "saved_y_values = []\n",
    "saved_z_values = []\n",
    "\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for m in tqdm(range(iter_)): # each iteration involves multiple episodes \n",
    "    counter_transitions = 1 # counts # of state transitions \n",
    "    \n",
    "    for e in range(E): # iterates over episodes\n",
    "        counter = 0\n",
    "        sample = 1 # select initial sample of episodes to feed into network\n",
    "\n",
    "        while sample == 1:\n",
    "            idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "            if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "                s = states[idx-1]\n",
    "                sample = 0\n",
    "\n",
    "        while (s!=17) and (counter < max_episode_duration): # state s different from EXIT state\n",
    "            h = H[s, :].T # state feature vector\n",
    "            policy = Pi[:, s] # true policy --> helps identify permissible actions at state s\n",
    "            flag = policy > 0\n",
    "            yCellf, zCellf = feed_forward_permissible(Wcell,ThetaCell,L,type_activation,h,use_theta,flag)\n",
    "            policy_hat = yCellf[-1]\n",
    "            act = select_action(policy_hat)\n",
    "\n",
    "            for j in range(NS):\n",
    "                kernel[j] = P[s, act, j]\n",
    "            \n",
    "            sprime = select_next_state(kernel) # next state\n",
    "            hprime = H[sprime, :].T # next feature\n",
    "\n",
    "            # CRITIC for state value function learning\n",
    "            r = reward[s] # in this example reward is only state s-dependent\n",
    "            delta = r + (gamma*hprime - h).T@w \n",
    "            w += mu*delta*h \n",
    "\n",
    "            # save visited states, actions, rewards, internal signals within neural network\n",
    "            saved_features.append([h, hprime])\n",
    "            saved_states.append([s, sprime])\n",
    "            saved_rewards.append(r)\n",
    "            saved_actions.append(act)\n",
    "            saved_y_values.append(yCellf)\n",
    "            saved_z_values.append(zCellf)\n",
    "\n",
    "            s = sprime # set next sample\n",
    "            counter += 1\n",
    "            counter_transitions += 1 # plays role of N\n",
    "\n",
    "    wo = w.copy() # learned CRITIC model\n",
    "\n",
    "    # ACTOR\n",
    "    N = counter_transitions - 1\n",
    "    sum_g = np.zeros(T)\n",
    "    saved_sigma = [[np.zeros(1)]*L]*N \n",
    "\n",
    "    for nn in range(N):\n",
    "        hx = saved_features[nn][0]\n",
    "        hxprime = saved_features[nn][1]\n",
    "        sx = saved_states[nn][0]\n",
    "        sxprime = saved_states[nn][1]\n",
    "        rx = saved_rewards[nn]\n",
    "        ax = saved_actions[nn]\n",
    "\n",
    "        yCellf = saved_y_values[nn]\n",
    "        gamma_hat = yCellf[-1]\n",
    "        ea = np.zeros(NA)\n",
    "        ea[ax] = 1\n",
    "\n",
    "        deltax = rx + (gamma*hxprime - hx).T@wo \n",
    "        saved_sigma[nn][-1] = deltax*(ea-gamma_hat)\n",
    "\n",
    "    Wcellf, ThetaCellf = feed_back_batch(Wcell,ThetaCell,saved_y_values,saved_z_values,saved_sigma,L,type_activation,mu,rho,use_theta,N)\n",
    "    Wcell = Wcellf.copy()\n",
    "    ThetaCell = ThetaCellf.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 up\n",
      "2 left\n",
      "3 up\n",
      "5 up\n",
      "6 up\n",
      "7 down\n",
      "8 stop\n",
      "9 right\n",
      "10 down\n",
      "12 down\n",
      "13 left\n",
      "14 left\n",
      "15 down\n",
      "16 stop\n",
      "17 stop\n",
      "[[0.25952025 0.28297748 0.22339837 0.23410391 0.        ]\n",
      " [0.25934858 0.26722509 0.18410842 0.28931791 0.        ]\n",
      " [0.23027523 0.27020438 0.22197813 0.27754226 0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.25575507 0.24816901 0.22949104 0.26658488 0.        ]\n",
      " [0.25339495 0.27534248 0.23662773 0.23463484 0.        ]\n",
      " [0.24113869 0.25493585 0.24435725 0.25956821 0.        ]\n",
      " [0.         0.         0.         0.         1.        ]\n",
      " [0.24406735 0.27513779 0.24466741 0.23612744 0.        ]\n",
      " [0.25136462 0.26438274 0.23200873 0.2522439  0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.24559798 0.26951402 0.22689835 0.25798965 0.        ]\n",
      " [0.23590059 0.28488897 0.23828841 0.24092203 0.        ]\n",
      " [0.24972788 0.2697781  0.22243727 0.25805675 0.        ]\n",
      " [0.23100449 0.27117174 0.20172471 0.29609906 0.        ]\n",
      " [0.         0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "Wstar = Wcell.copy()\n",
    "Thetastar = ThetaCell.copy()\n",
    "\n",
    "Pi_hat =np.zeros((NA, NS))\n",
    "act_vec = np.zeros(NA)\n",
    "\n",
    "\n",
    "action_states = [None]*NS\n",
    "for s in range(NS):\n",
    "    if (s!=3) and (s!=10): # not valid states\n",
    "        h = H[s, :].T # feature vector\n",
    "        policy = Pi[:, s] # true policy --> helps identify permissible actions at state s\n",
    "        flag = policy > 0\n",
    "        yCell, zCell = feed_forward_permissible(Wstar,Thetastar,L,type_activation,h,use_theta,flag)\n",
    "        policy_hat = yCell[-1]\n",
    "        act = select_action(policy_hat)\n",
    "        action_states[s] = actions[act]\n",
    "        print(s+1, action_states[s])\n",
    "        Pi_hat[:, s] = policy_hat\n",
    "\n",
    "print(Pi_hat.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
