{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid problem from example 1\n",
    "\n",
    "# states\n",
    "# we include the block locations 4 and 11 for convenience of coding; though they will never be reached\n",
    "states = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17] # s = 17 is the EXIT state\n",
    "NS = len(states) # number of states\n",
    "\n",
    "# actions\n",
    "actions = ['up', 'down', 'left', 'right', 'stop']\n",
    "NA = len(actions) # number of actions\n",
    "\n",
    "# rewards\n",
    "reward = -0.1*np.ones(NS)\n",
    "reward[7] = -10 # reward at state s = 8\n",
    "reward[15] = +10 # reward at state s = 16\n",
    "reward[16] = 0 # reward at exit satate s = 17\n",
    "\n",
    "# target policy pi(a|s)\n",
    "Pi = np.zeros((NA, NS)) # matrix Pi specifies the policy pi(a|s)\n",
    "                      # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Pi[0,j] = 1/4 # up\n",
    "        Pi[1,j] = 1/4 # down\n",
    "        Pi[2,j] = 1/4 # left\n",
    "        Pi[3,j] = 1/4 # right\n",
    "        Pi[4,j] = 0  # STOP\n",
    "    \n",
    "    else:\n",
    "        Pi[0,j] = 0 # up\n",
    "        Pi[1,j] = 0 # down\n",
    "        Pi[2,j] = 0 # left\n",
    "        Pi[3,j] = 0 # right\n",
    "        Pi[4,j] = 1 # STOP\n",
    "\n",
    "# transition kernel\n",
    "P = np.zeros((NS, NA, NS)) # entries are Prob(s, a, s')\n",
    "\n",
    "P[0, 0, 0] = 0.15 # start at s=1, move UP, end in state 1\n",
    "P[0, 0, 1] = 0.15\n",
    "P[0, 0, 7] = 0.7\n",
    "\n",
    "P[0, 1, 0] = 0.85 # start at s=1, move DOWN, end in state 1\n",
    "P[0, 1, 1] = 0.15\n",
    "\n",
    "P[0,2,0] = 0.15 # start at s=1, move LEFT, end in state 1\n",
    "P[0,2,1] = 0.70\n",
    "P[0,2,7] = 0.15\n",
    "\n",
    "P[0,3,0] = 0.85 # start at s=1, move RIGHT, end in state 1\n",
    "P[0,3,7] = 0.15\n",
    "\n",
    "P[1,0,0] = 0.15  # start at s=2, move UP, end in state 1\n",
    "P[1,0,2] = 0.15\n",
    "P[1,0,6] = 0.70\n",
    "\n",
    "P[1,1,0] = 0.15  # start at s=2, move DOWN, end in state 1\n",
    "P[1,1,2] = 0.15\n",
    "P[1,1,1] = 0.70\n",
    "                \n",
    "P[1,2,2] = 0.70  # start at s=2, move LEFT, end in state 3                  \n",
    "P[1,2,6] = 0.15\n",
    "P[1,2,1] = 0.15\n",
    "\n",
    "P[1,3,0] = 0.70   # start at s=2, move RIGHT, end in state 1 \n",
    "P[1,3,6] = 0.15\n",
    "P[1,3,1] = 0.15\n",
    "\n",
    "P[2,0,5] = 0.70   # start at s=3, move UP\n",
    "P[2,0,2] = 0.15\n",
    "P[2,0,1] = 0.15\n",
    "\n",
    "P[2,1,2] = 0.85  # start at s=3, move DOWN\n",
    "P[2,1,1] = 0.15\n",
    "\n",
    "P[2,2,2] = 0.85  # start at s=3, move LEFT\n",
    "P[2,2,5] = 0.15 \n",
    "\n",
    "P[2,3,1] = 0.70   # start at s=3, move RIGHT\n",
    "P[2,3,5] = 0.15\n",
    "P[2,3,4] = 0.15\n",
    "\n",
    "P[4,0,11] = 0.70  # start at s=5, move UP\n",
    "P[4,0,4]  = 0.15\n",
    "P[4,0,5]  = 0.15\n",
    "\n",
    "P[3,0,3] = 1 # values for location 4 this state is never reached\n",
    "P[3,1,3] = 1 # so these values are irrelevant\n",
    "P[3,2,3] = 1\n",
    "P[3,3,3] = 1\n",
    "P[3,4,3] = 1\n",
    "\n",
    "P[4,1,4] = 0.85  # start at s=5, move DOWN\n",
    "P[4,1,5] = 0.15\n",
    "\n",
    "P[4,2,4]  = 0.85  # start at s=5, move LEFT\n",
    "P[4,2,11] = 0.15\n",
    "\n",
    "P[4,3,5]  = 0.70  # start at s=5, move RIGHT\n",
    "P[4,3,11] = 0.15\n",
    "P[4,3,4]  = 0.15\n",
    "\n",
    "P[5,0,4] = 0.15   # start at s=6, move UP\n",
    "P[5,0,5] = 0.70\n",
    "P[5,0,6] = 0.15\n",
    "\n",
    "P[5,1,2] = 0.70    # start at s=6, move DOWN\n",
    "P[5,1,4] = 0.15\n",
    "P[5,1,6] = 0.15\n",
    "\n",
    "P[5,2,4] = 0.70   # start at s=6, move LEFT\n",
    "P[5,2,5] = 0.15\n",
    "P[5,2,2] = 0.15\n",
    "\n",
    "P[5,3,6] = 0.70   # start at s=6, move RIGHT\n",
    "P[5,3,5] = 0.15\n",
    "P[5,3,2] = 0.15\n",
    "\n",
    "P[6,0,9] = 0.70  # start at s=7, move UP\n",
    "P[6,0,5]  = 0.15\n",
    "P[6,0,7]  = 0.15\n",
    "\n",
    "P[6,1,1] = 0.70  # start at s=7, move DOWN\n",
    "P[6,1,5] = 0.15\n",
    "P[6,1,7] = 0.15\n",
    "\n",
    "P[6,2,5]  = 0.70 # start at s=7, move LEFT\n",
    "P[6,2,9] = 0.15\n",
    "P[6,2,1]  = 0.15\n",
    "\n",
    "P[6,3,7] = 0.70  # start at s=7, move RIGHT\n",
    "P[6,3,1] = 0.15\n",
    "P[6,3,9] = 0.15\n",
    "\n",
    "P[7,0,16] = 0   # start at s=8 [DANGER] EXIT\n",
    "P[7,1,16] = 0\n",
    "P[7,2,16] = 0\n",
    "P[7,3,16] = 0\n",
    "P[7,4,16] = 1 #STOP action\n",
    "\n",
    "P[8,0,15] = 0.70   # start at s=9 move UP\n",
    "P[8,0,9] = 0.15\n",
    "P[8,0,8]  = 0.15\n",
    "\n",
    "P[8,1,7]  = 0.70   # start at s=9 move DOWN\n",
    "P[8,1,9] = 0.15\n",
    "P[8,1,8]  = 0.15\n",
    "\n",
    "P[8,2,9] = 0.70  # start at s=9 move LEFT\n",
    "P[8,2,15] = 0.15\n",
    "P[8,2,7]  = 0.15\n",
    "\n",
    "P[8,3,8]  = 0.70  # start at s=9 move RIGHT\n",
    "P[8,3,7]  = 0.15\n",
    "P[8,3,15] = 0.15\n",
    "\n",
    "P[9,0,14] = 0.70   # start at s=10 move UP\n",
    "P[9,0,8]  = 0.15\n",
    "P[9,0,9] = 0.15\n",
    "\n",
    "P[9,1,6]  = 0.70  # start at s=10 move DOWN\n",
    "P[9,1,8]  = 0.15\n",
    "P[9,1,9] = 0.15\n",
    "\n",
    "P[9,2,9] = 0.70  # start at s=10 move LEFT\n",
    "P[9,2,14] = 0.15\n",
    "P[9,2,6]  = 0.15\n",
    "\n",
    "P[9,3,8]  = 0.70   # start at s=10 move RIGHT\n",
    "P[9,3,6]  = 0.15\n",
    "P[9,3,14] = 0.15\n",
    "\n",
    "P[10,0,3] = 1 # values for location 11 this state is never reached\n",
    "P[10,1,3] = 1 # so these values are irrelevant\n",
    "P[10,2,3] = 1\n",
    "P[10,3,3] = 1\n",
    "P[10,4,3] = 1\n",
    "\n",
    "P[11,0,12] = 0.70  # start at s=12 move UP\n",
    "P[11,0,11] = 0.30\n",
    "\n",
    "P[11,1,4]  = 0.70  # start at s=12 move DOWN\n",
    "P[11,1,11] = 0.30\n",
    "\n",
    "P[11,2,12] = 0.15  # start at s=12 move LEFT\n",
    "P[11,2,4]  = 0.15\n",
    "P[11,2,11] = 0.70\n",
    "\n",
    "P[11,3,11] = 0.70  # start at s=12 move RIGHT\n",
    "P[11,3,4]  = 0.15\n",
    "P[11,3,12] = 0.15\n",
    "\n",
    "P[12,0,12] = 0.85 # start at s=13 move UP\n",
    "P[12,0,13] = 0.15\n",
    "\n",
    "P[12,1,11] = 0.70  # start at s=13 move DOWN\n",
    "P[12,1,12] = 0.15\n",
    "P[12,1,13] = 0.15\n",
    "\n",
    "P[12,2,12] = 0.85 # start at s=13 move LEFT\n",
    "P[12,2,11] = 0.15\n",
    "\n",
    "P[12,3,13] = 0.70  # start at s=13 move RIGHT\n",
    "P[12,3,11] = 0.15\n",
    "P[12,3,12] = 0.15\n",
    "\n",
    "P[13,0,13] = 0.70 # start at s=14 move UP\n",
    "P[13,0,12] = 0.15\n",
    "P[13,0,14] = 0.15\n",
    "\n",
    "P[13,1,13] = 0.70  # start at s=14 move DOWN\n",
    "P[13,1,12] = 0.15\n",
    "P[13,1,14] = 0.15\n",
    "\n",
    "P[13,2,12] = 0.70  # start at s=14 move LEFT\n",
    "P[13,2,13] = 0.30\n",
    "\n",
    "P[13,3,14] = 0.70  # start at s=14 move RIGHT\n",
    "P[13,3,13] = 0.30\n",
    "\n",
    "P[14,0,14] = 0.70  # start at s=15 move UP\n",
    "P[14,0,13] = 0.15\n",
    "P[14,0,15] = 0.15\n",
    "\n",
    "P[14,1,9] = 0.70   # start at s=15 move DOWN\n",
    "P[14,1,13] = 0.15\n",
    "P[14,1,15] = 0.15\n",
    "\n",
    "P[14,2,13] = 0.70  # start at s=15 move LEFT\n",
    "P[14,2,9] = 0.15\n",
    "P[14,2,14] = 0.15\n",
    "\n",
    "P[14,3,15] = 0.70   # start at s=15 move RIGHT\n",
    "P[14,3,9] = 0.15\n",
    "P[14,3,14] = 0.15\n",
    "\n",
    "P[15,0,16] =0   # start at s=16 [REWARD] EXIT\n",
    "P[15,1,16] =0\n",
    "P[15,2,16] =0\n",
    "P[15,3,16] =0\n",
    "P[15,4,16] =1 # STOP action\n",
    "\n",
    "P[16,0,16] = 0\n",
    "P[16,1,16] = 0\n",
    "P[16,2,16] = 0\n",
    "P[16,3,16] = 0\n",
    "P[16,4,16] = 1 # EXIT state\n",
    "\n",
    "# Computing rpi(s)\n",
    "rpi = np.zeros(NS)\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for a in range(NA):\n",
    "        for sprime in range(NS):\n",
    "            rpi[s] += policy[a]*P[s, a, sprime]*reward[s]\n",
    "\n",
    "# Computing P^{\\pi}\n",
    "Ppi = np.zeros((NS, NS))\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for sprime in range(NS):\n",
    "        for a in range(NA):\n",
    "            Ppi[s, sprime] += policy[a]*P[s, a, sprime]\n",
    "\n",
    "# behavior policy phi(a|s) used to simulate off-policy algorithms\n",
    "Phi = np.zeros((NA, NS)) # matri Phi specifies the behavior policy phi(a|s)\n",
    "                         # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Phi[0,j] = 3/8 # up\n",
    "        Phi[1,j] = 1/8 # down\n",
    "        Phi[2,j] = 2/6 # left\n",
    "        Phi[3,j] = 1/6 # right\n",
    "        Phi[4,j] = 0  # STOP\n",
    "    else:\n",
    "        Phi[0,j] = 0 # up\n",
    "        Phi[1,j] = 0 # down\n",
    "        Phi[2,j] = 0 # left\n",
    "        Phi[3,j] = 0 # right\n",
    "        Phi[4,j] = 1  # STOP\n",
    "\n",
    "# one-hot encoding for the actions\n",
    "A = np.zeros((5, 5))\n",
    "A[0, :] = np.array([1, 0, 0, 0, 0]) # up\n",
    "A[1, :] = np.array([0, 1, 0, 0, 0]) # down\n",
    "A[2, :] = np.array([0, 0, 1, 0, 0]) # left\n",
    "A[3, :] = np.array([0, 0, 0, 1, 0]) # right\n",
    "A[4, :] = np.array([0, 0, 0, 0, 1]) # STOP\n",
    "\n",
    "# 4x1 reduced feature vectors with four binary entries\n",
    "# is agent on same row as SUCCESS\n",
    "# is agent on same row as DANGER\n",
    "# is agent in rightmost two columns\n",
    "# is agent in leftmost two columns\n",
    "\n",
    "# reduced features for state-value function\n",
    "# no offset is included in the feature vectors since v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Mr = 4\n",
    "Hr = np.zeros((NS, Mr))\n",
    "Hr[0,:]  = np.array([0, 0, 1, 0]) # state 1\n",
    "Hr[1,:]  = np.array([0, 0, 1, 0]) # state 2\n",
    "Hr[2,:]  = np.array([0, 0, 0, 1]) # state 3\n",
    "Hr[3,:]  = np.array([0, 0, 0, 0]) # not a valid state\n",
    "Hr[4,:]  = np.array([0, 1, 0, 1]) # state 5...\n",
    "Hr[5,:]  = np.array([0, 1, 0, 1])\n",
    "Hr[6,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[7,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[8,:]  = np.array([0, 0, 1, 0])\n",
    "Hr[9,:] = np.array([0, 0, 1, 0])\n",
    "Hr[10,:] = np.array([0, 0, 0, 0]) # not a valid state \n",
    "Hr[11,:] = np.array([0, 0, 0, 1])\n",
    "Hr[12,:] = np.array([1, 0, 0, 1])\n",
    "Hr[13,:] = np.array([1, 0, 0, 1])\n",
    "Hr[14,:] = np.array([1, 0, 1, 0])\n",
    "Hr[15,:] = np.array([1, 0, 1, 0]) # state 16\n",
    "Hr[16,:] = np.array([0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fr = np.kron(Hr, A) # Kronecker product of dimensions (NSxNA) x (MrxNA)\n",
    "Tr = Mr*NA\n",
    "\n",
    "# one-hot encoded feature vectors for state-value function\n",
    "# no offset is included in the feature vectors because v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Me = NS\n",
    "He = np.zeros((NS, Me))\n",
    "He[0,:]   = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 1\n",
    "He[1,:]   = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 2\n",
    "He[2,:]   = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 3\n",
    "He[3,:]   = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[4,:]   = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 5\n",
    "He[5,:]   = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # ...\n",
    "He[6,:]   = np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[7,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[8,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[9,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[10,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[11,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "He[12,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "He[13,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "He[14,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
    "He[15,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]) # state 16\n",
    "He[16,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fe = np.kron(He, A) # Kronecker product of dimensions (NSxNA) x (MexNA)\n",
    "Te = Me*NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy(F,theta,T,NA,NS,Pi):\n",
    "\n",
    "    # F: matrix of feature vectors f_{s,a}\n",
    "    # theta: parameter for pi(a|h;theta)\n",
    "    # T: size of theta\n",
    "    # NA: number of actions\n",
    "    # NS: number of states\n",
    "    # Pi: original pi(a|s) used to determine which actions are permissible\n",
    "\n",
    "    sum_ = np.zeros(NS)\n",
    "    Pi_theta = np.zeros((NA, NS))\n",
    "    for s in range(NS):\n",
    "        pi_vec = Pi[:, s] # helps reveal which actions are permissible at state s\n",
    "        for a in range(NA): # iterate over actions\n",
    "            if pi_vec[a] > 0:\n",
    "                row_idx = (s-1)*NA + a # index of row in F corresponding to (s, a)\n",
    "                f = F[int(row_idx), :].T # feature vector\n",
    "                sum_[s] += np.exp(f.T@theta)\n",
    "    \n",
    "    for s in range(NS):\n",
    "        pi_vec = Pi[:, s]\n",
    "        for a in range(NA):\n",
    "            if pi_vec[a] > 0:\n",
    "                row_idx = (s-1)*NA + a #index of row in F corresponding to (s, a)\n",
    "                f = F[int(row_idx), :].T\n",
    "                Pi_theta[a, s] = np.exp(f.T@theta)/sum_[s]\n",
    "    \n",
    "    return Pi_theta\n",
    "\n",
    "def compute_gradient_log_pi(F,Pi_theta,s,a,NA):\n",
    "    # F: matrix of feature vectors f_{s,a}\n",
    "    # Pi_theta: Gibbs distribution matrix NA x NS\n",
    "    # s: state\n",
    "    # action\n",
    "    # NA: number of actions\n",
    "\n",
    "    pi_vec = Pi_theta[:, s] # pi(a|s; theta)\n",
    "    row_idx = (s-1)*NA + a # index of row in F corresponding to (s, a)\n",
    "    f = F[int(row_idx), :].T # feature vector is at row of index sxa; f_{s,a}\n",
    "    fs_bar = 0\n",
    "    for aprime in range(NA):\n",
    "        row_idx_prime = (s-1)*NA + aprime\n",
    "        faprime = F[int(row_idx_prime), :].T # f_{s, a'}\n",
    "        fs_bar += pi_vec[aprime]*faprime\n",
    "    g_vec = f - fs_bar\n",
    "    return g_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantage actor-critic (A2C) algorithm for optimal policy design\n",
    "\n",
    "E = 100 # number of episodes within each iteration\n",
    "max_episode_duration = 50\n",
    "delta = 0\n",
    "gamma = 0.9\n",
    "mu = 0.01 # step-size for critic (w)\n",
    "mug = 0.001 # step-size for actor (theta)\n",
    "iter_ = 4000 # repeat for this many iterations to learn theta; each iteration has E episodes\n",
    "use_reduced_features = 0 # set to zero to use the one-hot encoded extended feature\n",
    "\n",
    "if use_reduced_features == 1:\n",
    "    H = Hr \n",
    "    M = Mr \n",
    "    F = Fr \n",
    "    T = Tr \n",
    "else:\n",
    "    H = He \n",
    "    M = Me \n",
    "    F = Fe \n",
    "    T = Te \n",
    "\n",
    "w = np.zeros(M) # linear value function model; includes bias coefficient\n",
    "theta = np.zeros(T) # parameter for Gibbs distribution\n",
    "\n",
    "saved_states = [] # need to save states, features, rewards actions during all episodes\n",
    "saved_features = []\n",
    "saved_rewards = []\n",
    "saved_actions = []\n",
    "\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for m in tqdm(range(iter_)): # each iteration involves multiple episodes\n",
    "    # Current Gibbs distribution using current model theta\n",
    "    Pi_theta = compute_policy(F,theta,T,NA,NS,Pi)\n",
    "    counter_transitions = 1 # counts # of state transitions during this iteration\n",
    "\n",
    "    for e in range(E): # iterates over episodes\n",
    "        counter = 0\n",
    "        sample = 1\n",
    "        while sample == 1:\n",
    "            idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "            if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "                s = states[idx-1]\n",
    "                sample = 0\n",
    "\n",
    "        while (s!=17) and (counter < max_episode_duration): # state s different from EXIT state\n",
    "            h = H[s, :].T # state feature vector \n",
    "            policy = Pi_theta[:, s] # pi(a|s; theta) at state s\n",
    "            act = select_action(policy)\n",
    "\n",
    "            for j in range(NS):\n",
    "                kernel[j] = P[s, act, j]\n",
    "            \n",
    "            sprime = select_next_state(kernel) # next state\n",
    "            hprime = H[sprime, :].T # next feature\n",
    "\n",
    "            # CRITIC for state value function learning\n",
    "            r = reward[s] # in this example reward is only state s-dependent\n",
    "            delta = r + (gamma*hprime-h).T@w\n",
    "            w += mu*delta*h \n",
    "\n",
    "            #saved visited states, actions, rewards\n",
    "            saved_features.append([h, hprime])\n",
    "            saved_states.append([s, sprime])\n",
    "            saved_rewards.append(r)\n",
    "            saved_actions.append(act)\n",
    "\n",
    "            s = sprime # set next sample\n",
    "            counter += 1\n",
    "            counter_transitions += 1 # plays role of N\n",
    "    wo = w.copy() # learned CRITIC MODEL\n",
    "\n",
    "    # ACTOR\n",
    "    N = counter_transitions - 1\n",
    "    sum_g = np.zeros(T)\n",
    "\n",
    "    for nn in range(N):\n",
    "        hx = saved_features[nn][0]\n",
    "        hxprime = saved_features[nn][1]\n",
    "        sx = saved_states[nn][0]\n",
    "        sxprime = saved_states[nn][1]\n",
    "        rx = saved_rewards[nn]\n",
    "        ax = saved_actions[nn]\n",
    "\n",
    "        deltax = rx + (gamma*hxprime - hx).T@wo \n",
    "        g_vec = compute_gradient_log_pi(F,Pi_theta,sx,ax,NA) # gradient of log pi\n",
    "        sum_g += deltax*g_vec\n",
    "    sum_g = sum_g/N \n",
    "    theta += mug*sum_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 up\n",
      "2 left\n",
      "3 left\n",
      "4 stop\n",
      "5 up\n",
      "6 left\n",
      "7 left\n",
      "8 stop\n",
      "9 up\n",
      "10 up\n",
      "11 stop\n",
      "12 up\n",
      "13 right\n",
      "14 right\n",
      "15 right\n",
      "16 stop\n",
      "17 stop\n"
     ]
    }
   ],
   "source": [
    "thetao = theta \n",
    "Pi_theta = compute_policy(F,thetao,T,NA,NS,Pi) # optimal policy\n",
    "max_values = Pi_theta.max(axis=0)\n",
    "indexes = np.argmax(Pi_theta, axis=0)\n",
    "\n",
    "act = [0]*NS\n",
    "action_states = [0]*NS\n",
    "for s in range(NS):\n",
    "    act[s] = indexes[s] # indexes of the permissible action  \n",
    "    action_states[s] = actions[act[s]]\n",
    "    print(s+1, action_states[s])\n",
    "\n",
    "print(\"policy AxS from ADVANTAGE (A2C) actor-critic algorithm\")\n",
    "print(Pi_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 457.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 up\n",
      "2 up\n",
      "3 right\n",
      "4 stop\n",
      "5 up\n",
      "6 right\n",
      "7 up\n",
      "8 stop\n",
      "9 up\n",
      "10 up\n",
      "11 stop\n",
      "12 up\n",
      "13 right\n",
      "14 right\n",
      "15 right\n",
      "16 stop\n",
      "17 stop\n",
      "policy AxS from NATURAL actor critic algorithm\n",
      "[[2.50000000e-01 9.96811025e-01 7.61399945e-02 0.00000000e+00\n",
      "  9.99995654e-01 6.33866948e-04 9.99985042e-01 0.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  1.80143837e-15 4.34798524e-21 7.70863360e-26 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.50000000e-01 2.34082025e-04 5.84616955e-03 0.00000000e+00\n",
      "  2.67094933e-07 1.02884545e-04 3.55168431e-14 0.00000000e+00\n",
      "  3.68332691e-39 1.24565357e-26 0.00000000e+00 6.87469592e-14\n",
      "  6.96842989e-18 1.97882261e-21 1.43232035e-30 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.50000000e-01 1.15305088e-04 4.13189217e-03 0.00000000e+00\n",
      "  3.79221104e-06 1.03735388e-03 1.49584110e-05 0.00000000e+00\n",
      "  4.21030226e-32 1.12328402e-21 0.00000000e+00 7.62091515e-12\n",
      "  2.37084012e-18 1.04396673e-25 6.44393882e-33 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.50000000e-01 2.83958740e-03 9.13881944e-01 0.00000000e+00\n",
      "  2.86299657e-07 9.98225895e-01 9.28852176e-24 0.00000000e+00\n",
      "  4.78814440e-33 9.18145806e-15 0.00000000e+00 2.00732538e-11\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Natural gradient actor-critic algorithm\n",
    "\n",
    "E = 100 # number of episodes within each iteration\n",
    "max_episode_duration = 50\n",
    "delta = 0\n",
    "gamma = 0.9\n",
    "mu = 0.01 # step-size for critic (w)\n",
    "mug = 0.001 # step-size for actor (theta)\n",
    "iter_ = 10000 # repeat for this many iterations to learn theta; each iteration has E episodes\n",
    "use_reduced_features = 0 # set to zero to use the one-hot encoded extended feature\n",
    "\n",
    "if use_reduced_features == 1:\n",
    "    H = Hr \n",
    "    M = Mr \n",
    "    F = Fr \n",
    "    T = Tr \n",
    "else:\n",
    "    H = He \n",
    "    M = Me \n",
    "    F = Fe \n",
    "    T = Te \n",
    "\n",
    "c = np.zeros(T) # linear value function model\n",
    "theta = np.zeros(T) # parameter for Gibbs distribution\n",
    "\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for m in tqdm(range(iter_)): # each iteration involves multiple episodes\n",
    "    # Current Gibbs distribution using current model theta\n",
    "    Pi_theta = compute_policy(F,theta,T,NA,NS,Pi)\n",
    "\n",
    "    for e in range(E): # iterates over episodes\n",
    "        counter = 0\n",
    "        sample = 1\n",
    "        while sample == 1:\n",
    "            idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "            if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "                s = states[idx-1]\n",
    "                sample = 0\n",
    "\n",
    "    h = H[s, :].T # initial state feature vector\n",
    "    policy = Pi_theta[:, s] # pi(a|s; theta) at this state s\n",
    "    act = select_action(policy) # initial conditions\n",
    "    idx = (s-1)*NA+act \n",
    "    f = F[int(idx), :].T # initial f vector\n",
    "\n",
    "    while (s!=17) and (counter < max_episode_duration): # state s different from EXIT state\n",
    "        for j in range(NS):\n",
    "            kernel[j] = P[s, act, j]\n",
    "        sprime = select_next_state(kernel) # next state\n",
    "        hprime = H[sprime, :].T # next feature\n",
    "        policyprime = Pi_theta[:, sprime] # pi(a|sprime; theta) at state prime\n",
    "        actprime = select_action(policyprime)\n",
    "        idx_prime = (sprime-1)*NA + actprime\n",
    "        fprime = F[int(idx_prime), :].T \n",
    "\n",
    "        # CRITIC for state-action value function learning\n",
    "        r = reward[s] # in this example, reward is only state s-dependent\n",
    "        beta = r + (gamma*fprime - f).T@c\n",
    "        c += mu*beta*f \n",
    "        s = sprime # set next sample\n",
    "        act = actprime \n",
    "        f = fprime \n",
    "        counter += 1\n",
    "    co = c.copy() # learned CRITIC MODEL\n",
    "\n",
    "    #ACTOR\n",
    "    theta += mug*co \n",
    "\n",
    "thetao = theta \n",
    "Pi_theta = compute_policy(F,thetao,T,NA,NS,Pi) # optimal policy\n",
    "max_values = Pi_theta.max(axis=0)\n",
    "indexes = np.argmax(Pi_theta, axis=0)\n",
    "\n",
    "act = [0]*NS\n",
    "action_states = [0]*NS\n",
    "for s in range(NS):\n",
    "    act[s] = indexes[s] # indexes of the permissible action  \n",
    "    action_states[s] = actions[act[s]]\n",
    "    print(s+1, action_states[s])\n",
    "\n",
    "print(\"policy AxS from NATURAL actor critic algorithm\")\n",
    "print(Pi_theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
