{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid problem from example 1\n",
    "\n",
    "# states\n",
    "# we include the block locations 4 and 11 for convenience of coding; though they will never be reached\n",
    "states = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17] # s = 17 is the EXIT state\n",
    "NS = len(states) # number of states\n",
    "\n",
    "# actions\n",
    "actions = ['up', 'down', 'left', 'right', 'stop']\n",
    "NA = len(actions) # number of actions\n",
    "\n",
    "# rewards\n",
    "reward = -0.1*np.ones(NS)\n",
    "reward[7] = -10 # reward at state s = 8\n",
    "reward[15] = +10 # reward at state s = 16\n",
    "reward[16] = 0 # reward at exit satate s = 17\n",
    "\n",
    "# target policy pi(a|s)\n",
    "Pi = np.zeros((NA, NS)) # matrix Pi specifies the policy pi(a|s)\n",
    "                      # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Pi[0,j] = 1/4 # up\n",
    "        Pi[1,j] = 1/4 # down\n",
    "        Pi[2,j] = 1/4 # left\n",
    "        Pi[3,j] = 1/4 # right\n",
    "        Pi[4,j] = 0  # STOP\n",
    "    \n",
    "    else:\n",
    "        Pi[0,j] = 0 # up\n",
    "        Pi[1,j] = 0 # down\n",
    "        Pi[2,j] = 0 # left\n",
    "        Pi[3,j] = 0 # right\n",
    "        Pi[4,j] = 1 # STOP\n",
    "\n",
    "# transition kernel\n",
    "P = np.zeros((NS, NA, NS)) # entries are Prob(s, a, s')\n",
    "\n",
    "P[0, 0, 0] = 0.15 # start at s=1, move UP, end in state 1\n",
    "P[0, 0, 1] = 0.15\n",
    "P[0, 0, 7] = 0.7\n",
    "\n",
    "P[0, 1, 0] = 0.85 # start at s=1, move DOWN, end in state 1\n",
    "P[0, 1, 1] = 0.15\n",
    "\n",
    "P[0,2,0] = 0.15 # start at s=1, move LEFT, end in state 1\n",
    "P[0,2,1] = 0.70\n",
    "P[0,2,7] = 0.15\n",
    "\n",
    "P[0,3,0] = 0.85 # start at s=1, move RIGHT, end in state 1\n",
    "P[0,3,7] = 0.15\n",
    "\n",
    "P[1,0,0] = 0.15  # start at s=2, move UP, end in state 1\n",
    "P[1,0,2] = 0.15\n",
    "P[1,0,6] = 0.70\n",
    "\n",
    "P[1,1,0] = 0.15  # start at s=2, move DOWN, end in state 1\n",
    "P[1,1,2] = 0.15\n",
    "P[1,1,1] = 0.70\n",
    "                \n",
    "P[1,2,2] = 0.70  # start at s=2, move LEFT, end in state 3                  \n",
    "P[1,2,6] = 0.15\n",
    "P[1,2,1] = 0.15\n",
    "\n",
    "P[1,3,0] = 0.70   # start at s=2, move RIGHT, end in state 1 \n",
    "P[1,3,6] = 0.15\n",
    "P[1,3,1] = 0.15\n",
    "\n",
    "P[2,0,5] = 0.70   # start at s=3, move UP\n",
    "P[2,0,2] = 0.15\n",
    "P[2,0,1] = 0.15\n",
    "\n",
    "P[2,1,2] = 0.85  # start at s=3, move DOWN\n",
    "P[2,1,1] = 0.15\n",
    "\n",
    "P[2,2,2] = 0.85  # start at s=3, move LEFT\n",
    "P[2,2,5] = 0.15 \n",
    "\n",
    "P[2,3,1] = 0.70   # start at s=3, move RIGHT\n",
    "P[2,3,5] = 0.15\n",
    "P[2,3,4] = 0.15\n",
    "\n",
    "P[4,0,11] = 0.70  # start at s=5, move UP\n",
    "P[4,0,4]  = 0.15\n",
    "P[4,0,5]  = 0.15\n",
    "\n",
    "P[3,0,3] = 1 # values for location 4 this state is never reached\n",
    "P[3,1,3] = 1 # so these values are irrelevant\n",
    "P[3,2,3] = 1\n",
    "P[3,3,3] = 1\n",
    "P[3,4,3] = 1\n",
    "\n",
    "P[4,1,4] = 0.85  # start at s=5, move DOWN\n",
    "P[4,1,5] = 0.15\n",
    "\n",
    "P[4,2,4]  = 0.85  # start at s=5, move LEFT\n",
    "P[4,2,11] = 0.15\n",
    "\n",
    "P[4,3,5]  = 0.70  # start at s=5, move RIGHT\n",
    "P[4,3,11] = 0.15\n",
    "P[4,3,4]  = 0.15\n",
    "\n",
    "P[5,0,4] = 0.15   # start at s=6, move UP\n",
    "P[5,0,5] = 0.70\n",
    "P[5,0,6] = 0.15\n",
    "\n",
    "P[5,1,2] = 0.70    # start at s=6, move DOWN\n",
    "P[5,1,4] = 0.15\n",
    "P[5,1,6] = 0.15\n",
    "\n",
    "P[5,2,4] = 0.70   # start at s=6, move LEFT\n",
    "P[5,2,5] = 0.15\n",
    "P[5,2,2] = 0.15\n",
    "\n",
    "P[5,3,6] = 0.70   # start at s=6, move RIGHT\n",
    "P[5,3,5] = 0.15\n",
    "P[5,3,2] = 0.15\n",
    "\n",
    "P[6,0,9] = 0.70  # start at s=7, move UP\n",
    "P[6,0,5]  = 0.15\n",
    "P[6,0,7]  = 0.15\n",
    "\n",
    "P[6,1,1] = 0.70  # start at s=7, move DOWN\n",
    "P[6,1,5] = 0.15\n",
    "P[6,1,7] = 0.15\n",
    "\n",
    "P[6,2,5]  = 0.70 # start at s=7, move LEFT\n",
    "P[6,2,9] = 0.15\n",
    "P[6,2,1]  = 0.15\n",
    "\n",
    "P[6,3,7] = 0.70  # start at s=7, move RIGHT\n",
    "P[6,3,1] = 0.15\n",
    "P[6,3,9] = 0.15\n",
    "\n",
    "P[7,0,16] = 0   # start at s=8 [DANGER] EXIT\n",
    "P[7,1,16] = 0\n",
    "P[7,2,16] = 0\n",
    "P[7,3,16] = 0\n",
    "P[7,4,16] = 1 #STOP action\n",
    "\n",
    "P[8,0,15] = 0.70   # start at s=9 move UP\n",
    "P[8,0,9] = 0.15\n",
    "P[8,0,8]  = 0.15\n",
    "\n",
    "P[8,1,7]  = 0.70   # start at s=9 move DOWN\n",
    "P[8,1,9] = 0.15\n",
    "P[8,1,8]  = 0.15\n",
    "\n",
    "P[8,2,9] = 0.70  # start at s=9 move LEFT\n",
    "P[8,2,15] = 0.15\n",
    "P[8,2,7]  = 0.15\n",
    "\n",
    "P[8,3,8]  = 0.70  # start at s=9 move RIGHT\n",
    "P[8,3,7]  = 0.15\n",
    "P[8,3,15] = 0.15\n",
    "\n",
    "P[9,0,14] = 0.70   # start at s=10 move UP\n",
    "P[9,0,8]  = 0.15\n",
    "P[9,0,9] = 0.15\n",
    "\n",
    "P[9,1,6]  = 0.70  # start at s=10 move DOWN\n",
    "P[9,1,8]  = 0.15\n",
    "P[9,1,9] = 0.15\n",
    "\n",
    "P[9,2,9] = 0.70  # start at s=10 move LEFT\n",
    "P[9,2,14] = 0.15\n",
    "P[9,2,6]  = 0.15\n",
    "\n",
    "P[9,3,8]  = 0.70   # start at s=10 move RIGHT\n",
    "P[9,3,6]  = 0.15\n",
    "P[9,3,14] = 0.15\n",
    "\n",
    "P[10,0,3] = 1 # values for location 11 this state is never reached\n",
    "P[10,1,3] = 1 # so these values are irrelevant\n",
    "P[10,2,3] = 1\n",
    "P[10,3,3] = 1\n",
    "P[10,4,3] = 1\n",
    "\n",
    "P[11,0,12] = 0.70  # start at s=12 move UP\n",
    "P[11,0,11] = 0.30\n",
    "\n",
    "P[11,1,4]  = 0.70  # start at s=12 move DOWN\n",
    "P[11,1,11] = 0.30\n",
    "\n",
    "P[11,2,12] = 0.15  # start at s=12 move LEFT\n",
    "P[11,2,4]  = 0.15\n",
    "P[11,2,11] = 0.70\n",
    "\n",
    "P[11,3,11] = 0.70  # start at s=12 move RIGHT\n",
    "P[11,3,4]  = 0.15\n",
    "P[11,3,12] = 0.15\n",
    "\n",
    "P[12,0,12] = 0.85 # start at s=13 move UP\n",
    "P[12,0,13] = 0.15\n",
    "\n",
    "P[12,1,11] = 0.70  # start at s=13 move DOWN\n",
    "P[12,1,12] = 0.15\n",
    "P[12,1,13] = 0.15\n",
    "\n",
    "P[12,2,12] = 0.85 # start at s=13 move LEFT\n",
    "P[12,2,11] = 0.15\n",
    "\n",
    "P[12,3,13] = 0.70  # start at s=13 move RIGHT\n",
    "P[12,3,11] = 0.15\n",
    "P[12,3,12] = 0.15\n",
    "\n",
    "P[13,0,13] = 0.70 # start at s=14 move UP\n",
    "P[13,0,12] = 0.15\n",
    "P[13,0,14] = 0.15\n",
    "\n",
    "P[13,1,13] = 0.70  # start at s=14 move DOWN\n",
    "P[13,1,12] = 0.15\n",
    "P[13,1,14] = 0.15\n",
    "\n",
    "P[13,2,12] = 0.70  # start at s=14 move LEFT\n",
    "P[13,2,13] = 0.30\n",
    "\n",
    "P[13,3,14] = 0.70  # start at s=14 move RIGHT\n",
    "P[13,3,13] = 0.30\n",
    "\n",
    "P[14,0,14] = 0.70  # start at s=15 move UP\n",
    "P[14,0,13] = 0.15\n",
    "P[14,0,15] = 0.15\n",
    "\n",
    "P[14,1,9] = 0.70   # start at s=15 move DOWN\n",
    "P[14,1,13] = 0.15\n",
    "P[14,1,15] = 0.15\n",
    "\n",
    "P[14,2,13] = 0.70  # start at s=15 move LEFT\n",
    "P[14,2,9] = 0.15\n",
    "P[14,2,14] = 0.15\n",
    "\n",
    "P[14,3,15] = 0.70   # start at s=15 move RIGHT\n",
    "P[14,3,9] = 0.15\n",
    "P[14,3,14] = 0.15\n",
    "\n",
    "P[15,0,16] =0   # start at s=16 [REWARD] EXIT\n",
    "P[15,1,16] =0\n",
    "P[15,2,16] =0\n",
    "P[15,3,16] =0\n",
    "P[15,4,16] =1 # STOP action\n",
    "\n",
    "P[16,0,16] = 0\n",
    "P[16,1,16] = 0\n",
    "P[16,2,16] = 0\n",
    "P[16,3,16] = 0\n",
    "P[16,4,16] = 1 # EXIT state\n",
    "\n",
    "# Computing rpi(s)\n",
    "rpi = np.zeros(NS)\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for a in range(NA):\n",
    "        for sprime in range(NS):\n",
    "            rpi[s] += policy[a]*P[s, a, sprime]*reward[s]\n",
    "\n",
    "# Computing P^{\\pi}\n",
    "Ppi = np.zeros((NS, NS))\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for sprime in range(NS):\n",
    "        for a in range(NA):\n",
    "            Ppi[s, sprime] += policy[a]*P[s, a, sprime]\n",
    "\n",
    "# behavior policy phi(a|s) used to simulate off-policy algorithms\n",
    "Phi = np.zeros((NA, NS)) # matri Phi specifies the behavior policy phi(a|s)\n",
    "                         # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Phi[0,j] = 3/8 # up\n",
    "        Phi[1,j] = 1/8 # down\n",
    "        Phi[2,j] = 2/6 # left\n",
    "        Phi[3,j] = 1/6 # right\n",
    "        Phi[4,j] = 0  # STOP\n",
    "    else:\n",
    "        Phi[0,j] = 0 # up\n",
    "        Phi[1,j] = 0 # down\n",
    "        Phi[2,j] = 0 # left\n",
    "        Phi[3,j] = 0 # right\n",
    "        Phi[4,j] = 1  # STOP\n",
    "\n",
    "# one-hot encoding for the actions\n",
    "A = np.zeros((5, 5))\n",
    "A[0, :] = np.array([1, 0, 0, 0, 0]) # up\n",
    "A[1, :] = np.array([0, 1, 0, 0, 0]) # down\n",
    "A[2, :] = np.array([0, 0, 1, 0, 0]) # left\n",
    "A[3, :] = np.array([0, 0, 0, 1, 0]) # right\n",
    "A[4, :] = np.array([0, 0, 0, 0, 1]) # STOP\n",
    "\n",
    "# 4x1 reduced feature vectors with four binary entries\n",
    "# is agent on same row as SUCCESS\n",
    "# is agent on same row as DANGER\n",
    "# is agent in rightmost two columns\n",
    "# is agent in leftmost two columns\n",
    "\n",
    "# reduced features for state-value function\n",
    "# no offset is included in the feature vectors since v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Mr = 4\n",
    "Hr = np.zeros((NS, Mr))\n",
    "Hr[0,:]  = np.array([0, 0, 1, 0]) # state 1\n",
    "Hr[1,:]  = np.array([0, 0, 1, 0]) # state 2\n",
    "Hr[2,:]  = np.array([0, 0, 0, 1]) # state 3\n",
    "Hr[3,:]  = np.array([0, 0, 0, 0]) # not a valid state\n",
    "Hr[4,:]  = np.array([0, 1, 0, 1]) # state 5...\n",
    "Hr[5,:]  = np.array([0, 1, 0, 1])\n",
    "Hr[6,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[7,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[8,:]  = np.array([0, 0, 1, 0])\n",
    "Hr[9,:] = np.array([0, 0, 1, 0])\n",
    "Hr[10,:] = np.array([0, 0, 0, 0]) # not a valid state \n",
    "Hr[11,:] = np.array([0, 0, 0, 1])\n",
    "Hr[12,:] = np.array([1, 0, 0, 1])\n",
    "Hr[13,:] = np.array([1, 0, 0, 1])\n",
    "Hr[14,:] = np.array([1, 0, 1, 0])\n",
    "Hr[15,:] = np.array([1, 0, 1, 0]) # state 16\n",
    "Hr[16,:] = np.array([0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fr = np.kron(Hr, A) # Kronecker product of dimensions (NSxNA) x (MrxNA)\n",
    "Tr = Mr*NA\n",
    "\n",
    "# one-hot encoded feature vectors for state-value function\n",
    "# no offset is included in the feature vectors because v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Me = NS\n",
    "He = np.zeros((NS, Me))\n",
    "He[0,:]   = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 1\n",
    "He[1,:]   = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 2\n",
    "He[2,:]   = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 3\n",
    "He[3,:]   = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[4,:]   = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 5\n",
    "He[5,:]   = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # ...\n",
    "He[6,:]   = np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[7,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[8,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[9,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[10,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[11,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "He[12,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "He[13,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "He[14,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
    "He[15,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]) # state 16\n",
    "He[16,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fe = np.kron(He, A) # Kronecker product of dimensions (NSxNA) x (MexNA)\n",
    "Te = Me*NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning under epsilon-greedy exploration\n",
    "\n",
    "E = 50000 # number of episodes\n",
    "gamma = 0.9\n",
    "mu = 0.01\n",
    "epsilon = 0.1 # parameter for epsilon-greedy exploration\n",
    "rmin = 0.1\n",
    "rmax = 10\n",
    "Q = (max(rmin, rmax)/ (1-gamma))*np.ones((NS, NA)) # Q(s, a) matrix; optimistic initialization\n",
    "beta = np.zeros((NS, NA))\n",
    "max_episode_duration = 50\n",
    "\n",
    "q_vec = np.zeros(NA)\n",
    "q_prime_vec = np.zeros(NA)\n",
    "a_vec = np.zeros(NA)\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for e in range(E): # iterates over episodes\n",
    "    counter = 0\n",
    "    sample = 1\n",
    "    while sample == 1:\n",
    "        idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "        if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "            s = states[idx-1]\n",
    "            sample = 0\n",
    "    \n",
    "    while (s != 17) and (counter < max_episode_duration): # state s different from EXIT state\n",
    "        q = Q[s, :] # row in Q corresponding to state s\n",
    "\n",
    "        pi_vec = Pi[:, s] # policy vector at state s --> determines which actions are possible ar s\n",
    "        counter2 = 0\n",
    "        for j in range(NA):\n",
    "            if pi_vec[j] > 0: # the j-th action is possible\n",
    "                q_vec[counter2] = q[j] # q-value\n",
    "                a_vec[counter2] = j # corresponding valid action\n",
    "                counter2 += 1\n",
    "        ax = np.argmax(q_vec[0:counter2]) # permissible action at s with largest q-value\n",
    "        idx = (q_vec[0:counter2]).max()\n",
    "        act1 = int(a_vec[ax]) # index of the permissible action\n",
    "\n",
    "        y = np.random.rand() # epsilon-greed strategy\n",
    "        if y <= epsilon:\n",
    "            ay = np.random.randint(counter2) # choose from actions permissible at state s\n",
    "            act = int(a_vec[ay])\n",
    "        else:\n",
    "            act = act1\n",
    "        \n",
    "        for j in range(NS):\n",
    "            kernel[j] = P[s, act, j]\n",
    "        \n",
    "        sprime = select_next_state(kernel)\n",
    "        r = reward[s]\n",
    "\n",
    "        pi_prime_vec = Pi[:, sprime] # again, we find max Q(s', a') over permissible actions at s'\n",
    "        counter3 = 0\n",
    "        q_prime = Q[sprime, :]\n",
    "        for j in range(NA):\n",
    "            if pi_prime_vec[j] > 0: # the h-th action is possible\n",
    "                q_prime_vec[counter3] = q_prime[j] # q_value\n",
    "                counter3 += 1\n",
    "        \n",
    "        max_value = (q_prime_vec[0:counter3]).max() # maximum over permissible actions at s' \n",
    "\n",
    "        beta[s, act] = r + max_value - Q[s, act]\n",
    "        Q[s, act] += mu*beta[s, act]\n",
    "        s = sprime\n",
    "        counter += 1\n",
    "\n",
    "# after convergence, we determine the optimal policy from the resulting Q martrix\n",
    "\n",
    "act = np.zeros(NS)\n",
    "action_state = [None]*NS\n",
    "act[3] = -1 # no action since 4 is not a valud state\n",
    "act[10] = - 1 # 11 is not a valid state\n",
    "action_state[3] = 'NA' # no action; not applicable since 4 and 11 are not valid states\n",
    "action_state[10] = 'NA'\n",
    "\n",
    "for s in range(NS):\n",
    "    if (s != 4) and (s != 11): # not valid states; exclude them\n",
    "        q = Q[s, :] # row in Q corresponding to state s\n",
    "        pi_vec = Pi[:, s] # policy vector at state s --> determines which actions are possible ar s\n",
    "        counter = 0\n",
    "        for j in range(NA):\n",
    "            if pi_vec[j] > 0: # the j-th action in possible\n",
    "                q_vec[counter] = q[j] # q-value\n",
    "                a_vec[counter] = j # corresponding valid action\n",
    "                counter += 1\n",
    "        ax = np.argmax(q_vec[0:counter]) # permissible action at s with largest q-value\n",
    "        idx = (q_vec[0:counter]).max()\n",
    "        act[s] = int(a_vec[ax]) # index of the permissible action\n",
    "        action_state[s] = actions[int(act[s])]\n",
    "        print(s, action_state[s])\n",
    "\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
