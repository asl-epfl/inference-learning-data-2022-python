{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 47: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code generates the data results for Example 4 in Chapter 47: Q-Learning  (vol. II)\n",
    "TEXT: A. H. Sayed, INFERENCE AND LEARNING FROM DATA, Cambridge University Press, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "DISCLAIMER:  This computer code is  provided  \"as is\"   without  any  guarantees.\n",
    "Practitioners  should  use it  at their own risk.  While  the  codes in  the text \n",
    "are useful for instructional purposes, they are not intended to serve as examples \n",
    "of full-blown or optimized designs. The author has made no attempt at optimizing \n",
    "the codes, perfecting them, or even checking them for absolute accuracy. In order \n",
    "to keep the codes at a level  that is  easy to follow by students, the author has \n",
    "often chosen to  sacrifice  performance or even programming elegance in  lieu  of \n",
    "simplicity. Students can use the computer codes to run variations of the examples \n",
    "shown in the text. \n",
    "</div>\n",
    "\n",
    "The Jupyter notebook and python codes are developed by Eduardo Faria Cabrera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "required libraries:\n",
    "    \n",
    "1. numpy\n",
    "2. matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 47.4 (Optimal policy for a game over a grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the operation of the $Q-$learning algorithm under $\\epsilon-$greedy exploration  by reconsidering the earlier grid problem from Fig. 44.4. Recall that the grid consisted of $16$ squares labeled $\\#1$ through $\\#16$. Four squares are special; these are squares $\\#4, \\#8, \\#11,$ and $\\#16$. Square $\\#8$ (danger) and $\\#16$ (stars) are terminal states. If the agent reaches one of these terminal states, the agent moves to an EXIT state and the game stops. The agent collects either a reward of $+10$ at square $\\#16$ or a reward of $-10$ at square $\\#8$. The agent collects a reward of $-0.1$ at all other states. We iterate the $Q-$learning \n",
    "iteration (47.52) over 50,000 episodes using $\\epsilon=0.1$, $\\gamma=0.9$, and a constant step size $\\mu=0.01$. We employ the optimistic initialization (47.49) for the state--action value function, namely, \n",
    "\n",
    "$ q_{-1}(s,a)=\\frac{\\max\\{0.1,10\\}}{1-0.9}=100,\\;\\;\\;\\forall\\;(s,a)\\in\\mathbb{S}\\times\\mathbb{A}$\n",
    "\n",
    "In the implementation of the algorithm, the maximization of $q_{n-1}(s_n,a')$ and $q_{n-1}(s',a')$ is performed over the actions that are $\\textit{permissible}$ at the respective states, $s_n$ and $s'$, respectively. After convergence, we examine the resulting state--action value function $q(s,a)$, whose values we collect into a matrix $Q$ with each row corresponding to a state value and each column to an action:\n",
    "\n",
    "$\n",
    "Q=\\begin{array}{c|rrrrr}\n",
    "&\\textnormal{ up}&\\textnormal{ down}&\\textnormal{ left}&\\textnormal{ right}&\\textnormal{ stop}\\\\\\hline\n",
    "s=1&94.8026 & \\fbox{108.0683} & 105.7811 & 105.9368 & 100.0000\\\\\n",
    "s=2&\\fbox{108.7510}&  108.2464&  108.4248 & 107.9651 & 100.0000\\\\\n",
    "s=3&\\fbox{108.8600} & 108.4572 & 108.3800&  108.3750 & 100.0000\\\\\n",
    "s=4&- & -& - & - & -\\\\\n",
    "s=5&\\fbox{109.2077} & 109.0464 & 109.0972 & 108.9634&  100.0000\\\\\n",
    "s=6&108.9250 & 108.7999 & \\fbox{109.0351} & 108.8364&  100.0000\\\\\n",
    "s=7&106.4342 & 105.5675 & \\fbox{109.0016} &  94.6887&  100.0000\\\\\n",
    "s=8&- & - & - & -&   \\fbox{90.0000}\\\\\n",
    "s=9&\\fbox{109.8279} &  95.5984&  107.0995 & 106.5895&  100.0000\\\\\n",
    "s=10&\\fbox{109.7102} & 108.9413 & 109.4142&  109.5597&  100.0000\\\\\n",
    "s=11&- & - & - & -&  -\\\\\n",
    "s=12&\\fbox{109.3789} & 109.1449 & 109.2611 & 109.2649 & 100.0000\\\\\n",
    "s=13&109.4312 & 109.3414 & 109.3911 & \\fbox{109.5225} & 100.0000\\\\\n",
    "s=14&109.5837 & 109.5774 & 109.4614 & \\fbox{109.6903}&  100.0000\\\\\n",
    "s=15&109.7302 & 109.6244 & 109.6038 & \\fbox{109.8346} & 100.0000\\\\\n",
    "s=16&- & - & - & -&  \\fbox{110.0000}\\\\\n",
    "s=17&- & - & - & - & \\fbox{100.0000}\n",
    "\\end{array}$\n",
    "\n",
    "For each state $s$, we determine the action $a$ that maximizes $q(s,a)$. These results are indicated by boxes in the above expression, leading to  the following deterministic optimal policy:\n",
    "\n",
    "$\n",
    "\\pi(a=\\textnormal{ \"down\"}|s=1)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"up\"}|s=2)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"up\"}|s=3)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"up\"}|s=5)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"left\"}|s=6)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"left\"}|s=7)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"stop\"}|s=8)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"up\"}|s=9)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"up\"}|s=10)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"up\"}|s=12)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"right\"}|s=13)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"right\"}|s=14)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"right\"}|s=15)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"stop\"}|s=16)=1\\\\\n",
    "\\pi(a=\\textnormal{ \"stop\"}|\\textnormal{ s=\"EXIT\"})=1\n",
    "$\n",
    "\n",
    "The optimal actions are represented by arrows in Fig. 47.1. Observe in particular that the optimal action at state $s=7$ is to move left. By doing so, the agent has a $70\\%$ chance to move to state $s=6$ and $15\\%$ chance each to move to states $s=2$ and $s=10$. Note that there is no chance to end up in the danger state $s=8$. Likewise, the optimal action selected for state $s=1$ avoids any possibility of ending up in the danger state $s=1$. This is because by choosing to move downward, the agent has $85\\%$ chance of staying at state $s=1$ and $15\\%$ chance of moving to state $s=2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid problem from example 1\n",
    "\n",
    "# states\n",
    "# we include the block locations 4 and 11 for convenience of coding; though they will never be reached\n",
    "states = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17] # s = 17 is the EXIT state\n",
    "NS = len(states) # number of states\n",
    "\n",
    "# actions\n",
    "actions = ['up', 'down', 'left', 'right', 'stop']\n",
    "NA = len(actions) # number of actions\n",
    "\n",
    "# rewards\n",
    "reward = -0.1*np.ones(NS)\n",
    "reward[7] = -10 # reward at state s = 8\n",
    "reward[15] = +10 # reward at state s = 16\n",
    "reward[16] = 0 # reward at exit satate s = 17\n",
    "\n",
    "# target policy pi(a|s)\n",
    "Pi = np.zeros((NA, NS)) # matrix Pi specifies the policy pi(a|s)\n",
    "                      # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Pi[0,j] = 1/4 # up\n",
    "        Pi[1,j] = 1/4 # down\n",
    "        Pi[2,j] = 1/4 # left\n",
    "        Pi[3,j] = 1/4 # right\n",
    "        Pi[4,j] = 0  # STOP\n",
    "    \n",
    "    else:\n",
    "        Pi[0,j] = 0 # up\n",
    "        Pi[1,j] = 0 # down\n",
    "        Pi[2,j] = 0 # left\n",
    "        Pi[3,j] = 0 # right\n",
    "        Pi[4,j] = 1 # STOP\n",
    "\n",
    "# transition kernel\n",
    "P = np.zeros((NS, NA, NS)) # entries are Prob(s, a, s')\n",
    "\n",
    "P[0, 0, 0] = 0.15 # start at s=1, move UP, end in state 1\n",
    "P[0, 0, 1] = 0.15\n",
    "P[0, 0, 7] = 0.7\n",
    "\n",
    "P[0, 1, 0] = 0.85 # start at s=1, move DOWN, end in state 1\n",
    "P[0, 1, 1] = 0.15\n",
    "\n",
    "P[0,2,0] = 0.15 # start at s=1, move LEFT, end in state 1\n",
    "P[0,2,1] = 0.70\n",
    "P[0,2,7] = 0.15\n",
    "\n",
    "P[0,3,0] = 0.85 # start at s=1, move RIGHT, end in state 1\n",
    "P[0,3,7] = 0.15\n",
    "\n",
    "P[1,0,0] = 0.15  # start at s=2, move UP, end in state 1\n",
    "P[1,0,2] = 0.15\n",
    "P[1,0,6] = 0.70\n",
    "\n",
    "P[1,1,0] = 0.15  # start at s=2, move DOWN, end in state 1\n",
    "P[1,1,2] = 0.15\n",
    "P[1,1,1] = 0.70\n",
    "                \n",
    "P[1,2,2] = 0.70  # start at s=2, move LEFT, end in state 3                  \n",
    "P[1,2,6] = 0.15\n",
    "P[1,2,1] = 0.15\n",
    "\n",
    "P[1,3,0] = 0.70   # start at s=2, move RIGHT, end in state 1 \n",
    "P[1,3,6] = 0.15\n",
    "P[1,3,1] = 0.15\n",
    "\n",
    "P[2,0,5] = 0.70   # start at s=3, move UP\n",
    "P[2,0,2] = 0.15\n",
    "P[2,0,1] = 0.15\n",
    "\n",
    "P[2,1,2] = 0.85  # start at s=3, move DOWN\n",
    "P[2,1,1] = 0.15\n",
    "\n",
    "P[2,2,2] = 0.85  # start at s=3, move LEFT\n",
    "P[2,2,5] = 0.15 \n",
    "\n",
    "P[2,3,1] = 0.70   # start at s=3, move RIGHT\n",
    "P[2,3,5] = 0.15\n",
    "P[2,3,4] = 0.15\n",
    "\n",
    "P[4,0,11] = 0.70  # start at s=5, move UP\n",
    "P[4,0,4]  = 0.15\n",
    "P[4,0,5]  = 0.15\n",
    "\n",
    "P[3,0,3] = 1 # values for location 4 this state is never reached\n",
    "P[3,1,3] = 1 # so these values are irrelevant\n",
    "P[3,2,3] = 1\n",
    "P[3,3,3] = 1\n",
    "P[3,4,3] = 1\n",
    "\n",
    "P[4,1,4] = 0.85  # start at s=5, move DOWN\n",
    "P[4,1,5] = 0.15\n",
    "\n",
    "P[4,2,4]  = 0.85  # start at s=5, move LEFT\n",
    "P[4,2,11] = 0.15\n",
    "\n",
    "P[4,3,5]  = 0.70  # start at s=5, move RIGHT\n",
    "P[4,3,11] = 0.15\n",
    "P[4,3,4]  = 0.15\n",
    "\n",
    "P[5,0,4] = 0.15   # start at s=6, move UP\n",
    "P[5,0,5] = 0.70\n",
    "P[5,0,6] = 0.15\n",
    "\n",
    "P[5,1,2] = 0.70    # start at s=6, move DOWN\n",
    "P[5,1,4] = 0.15\n",
    "P[5,1,6] = 0.15\n",
    "\n",
    "P[5,2,4] = 0.70   # start at s=6, move LEFT\n",
    "P[5,2,5] = 0.15\n",
    "P[5,2,2] = 0.15\n",
    "\n",
    "P[5,3,6] = 0.70   # start at s=6, move RIGHT\n",
    "P[5,3,5] = 0.15\n",
    "P[5,3,2] = 0.15\n",
    "\n",
    "P[6,0,9] = 0.70  # start at s=7, move UP\n",
    "P[6,0,5]  = 0.15\n",
    "P[6,0,7]  = 0.15\n",
    "\n",
    "P[6,1,1] = 0.70  # start at s=7, move DOWN\n",
    "P[6,1,5] = 0.15\n",
    "P[6,1,7] = 0.15\n",
    "\n",
    "P[6,2,5]  = 0.70 # start at s=7, move LEFT\n",
    "P[6,2,9] = 0.15\n",
    "P[6,2,1]  = 0.15\n",
    "\n",
    "P[6,3,7] = 0.70  # start at s=7, move RIGHT\n",
    "P[6,3,1] = 0.15\n",
    "P[6,3,9] = 0.15\n",
    "\n",
    "P[7,0,16] = 0   # start at s=8 [DANGER] EXIT\n",
    "P[7,1,16] = 0\n",
    "P[7,2,16] = 0\n",
    "P[7,3,16] = 0\n",
    "P[7,4,16] = 1 #STOP action\n",
    "\n",
    "P[8,0,15] = 0.70   # start at s=9 move UP\n",
    "P[8,0,9] = 0.15\n",
    "P[8,0,8]  = 0.15\n",
    "\n",
    "P[8,1,7]  = 0.70   # start at s=9 move DOWN\n",
    "P[8,1,9] = 0.15\n",
    "P[8,1,8]  = 0.15\n",
    "\n",
    "P[8,2,9] = 0.70  # start at s=9 move LEFT\n",
    "P[8,2,15] = 0.15\n",
    "P[8,2,7]  = 0.15\n",
    "\n",
    "P[8,3,8]  = 0.70  # start at s=9 move RIGHT\n",
    "P[8,3,7]  = 0.15\n",
    "P[8,3,15] = 0.15\n",
    "\n",
    "P[9,0,14] = 0.70   # start at s=10 move UP\n",
    "P[9,0,8]  = 0.15\n",
    "P[9,0,9] = 0.15\n",
    "\n",
    "P[9,1,6]  = 0.70  # start at s=10 move DOWN\n",
    "P[9,1,8]  = 0.15\n",
    "P[9,1,9] = 0.15\n",
    "\n",
    "P[9,2,9] = 0.70  # start at s=10 move LEFT\n",
    "P[9,2,14] = 0.15\n",
    "P[9,2,6]  = 0.15\n",
    "\n",
    "P[9,3,8]  = 0.70   # start at s=10 move RIGHT\n",
    "P[9,3,6]  = 0.15\n",
    "P[9,3,14] = 0.15\n",
    "\n",
    "P[10,0,3] = 1 # values for location 11 this state is never reached\n",
    "P[10,1,3] = 1 # so these values are irrelevant\n",
    "P[10,2,3] = 1\n",
    "P[10,3,3] = 1\n",
    "P[10,4,3] = 1\n",
    "\n",
    "P[11,0,12] = 0.70  # start at s=12 move UP\n",
    "P[11,0,11] = 0.30\n",
    "\n",
    "P[11,1,4]  = 0.70  # start at s=12 move DOWN\n",
    "P[11,1,11] = 0.30\n",
    "\n",
    "P[11,2,12] = 0.15  # start at s=12 move LEFT\n",
    "P[11,2,4]  = 0.15\n",
    "P[11,2,11] = 0.70\n",
    "\n",
    "P[11,3,11] = 0.70  # start at s=12 move RIGHT\n",
    "P[11,3,4]  = 0.15\n",
    "P[11,3,12] = 0.15\n",
    "\n",
    "P[12,0,12] = 0.85 # start at s=13 move UP\n",
    "P[12,0,13] = 0.15\n",
    "\n",
    "P[12,1,11] = 0.70  # start at s=13 move DOWN\n",
    "P[12,1,12] = 0.15\n",
    "P[12,1,13] = 0.15\n",
    "\n",
    "P[12,2,12] = 0.85 # start at s=13 move LEFT\n",
    "P[12,2,11] = 0.15\n",
    "\n",
    "P[12,3,13] = 0.70  # start at s=13 move RIGHT\n",
    "P[12,3,11] = 0.15\n",
    "P[12,3,12] = 0.15\n",
    "\n",
    "P[13,0,13] = 0.70 # start at s=14 move UP\n",
    "P[13,0,12] = 0.15\n",
    "P[13,0,14] = 0.15\n",
    "\n",
    "P[13,1,13] = 0.70  # start at s=14 move DOWN\n",
    "P[13,1,12] = 0.15\n",
    "P[13,1,14] = 0.15\n",
    "\n",
    "P[13,2,12] = 0.70  # start at s=14 move LEFT\n",
    "P[13,2,13] = 0.30\n",
    "\n",
    "P[13,3,14] = 0.70  # start at s=14 move RIGHT\n",
    "P[13,3,13] = 0.30\n",
    "\n",
    "P[14,0,14] = 0.70  # start at s=15 move UP\n",
    "P[14,0,13] = 0.15\n",
    "P[14,0,15] = 0.15\n",
    "\n",
    "P[14,1,9] = 0.70   # start at s=15 move DOWN\n",
    "P[14,1,13] = 0.15\n",
    "P[14,1,15] = 0.15\n",
    "\n",
    "P[14,2,13] = 0.70  # start at s=15 move LEFT\n",
    "P[14,2,9] = 0.15\n",
    "P[14,2,14] = 0.15\n",
    "\n",
    "P[14,3,15] = 0.70   # start at s=15 move RIGHT\n",
    "P[14,3,9] = 0.15\n",
    "P[14,3,14] = 0.15\n",
    "\n",
    "P[15,0,16] =0   # start at s=16 [REWARD] EXIT\n",
    "P[15,1,16] =0\n",
    "P[15,2,16] =0\n",
    "P[15,3,16] =0\n",
    "P[15,4,16] =1 # STOP action\n",
    "\n",
    "P[16,0,16] = 0\n",
    "P[16,1,16] = 0\n",
    "P[16,2,16] = 0\n",
    "P[16,3,16] = 0\n",
    "P[16,4,16] = 1 # EXIT state\n",
    "\n",
    "# Computing rpi(s)\n",
    "rpi = np.zeros(NS)\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for a in range(NA):\n",
    "        for sprime in range(NS):\n",
    "            rpi[s] += policy[a]*P[s, a, sprime]*reward[s]\n",
    "\n",
    "# Computing P^{\\pi}\n",
    "Ppi = np.zeros((NS, NS))\n",
    "for s in range(NS):\n",
    "    policy = Pi[:, s]\n",
    "    for sprime in range(NS):\n",
    "        for a in range(NA):\n",
    "            Ppi[s, sprime] += policy[a]*P[s, a, sprime]\n",
    "\n",
    "# behavior policy phi(a|s) used to simulate off-policy algorithms\n",
    "Phi = np.zeros((NA, NS)) # matri Phi specifies the behavior policy phi(a|s)\n",
    "                         # each row is an action; each column is a state\n",
    "\n",
    "for j in range(NS):\n",
    "    s = states[j]\n",
    "    if s in [1, 2, 3, 5, 6, 7, 9, 10, 12, 13, 14, 15]:\n",
    "        Phi[0,j] = 3/8 # up\n",
    "        Phi[1,j] = 1/8 # down\n",
    "        Phi[2,j] = 2/6 # left\n",
    "        Phi[3,j] = 1/6 # right\n",
    "        Phi[4,j] = 0  # STOP\n",
    "    else:\n",
    "        Phi[0,j] = 0 # up\n",
    "        Phi[1,j] = 0 # down\n",
    "        Phi[2,j] = 0 # left\n",
    "        Phi[3,j] = 0 # right\n",
    "        Phi[4,j] = 1  # STOP\n",
    "\n",
    "# one-hot encoding for the actions\n",
    "A = np.zeros((5, 5))\n",
    "A[0, :] = np.array([1, 0, 0, 0, 0]) # up\n",
    "A[1, :] = np.array([0, 1, 0, 0, 0]) # down\n",
    "A[2, :] = np.array([0, 0, 1, 0, 0]) # left\n",
    "A[3, :] = np.array([0, 0, 0, 1, 0]) # right\n",
    "A[4, :] = np.array([0, 0, 0, 0, 1]) # STOP\n",
    "\n",
    "# 4x1 reduced feature vectors with four binary entries\n",
    "# is agent on same row as SUCCESS\n",
    "# is agent on same row as DANGER\n",
    "# is agent in rightmost two columns\n",
    "# is agent in leftmost two columns\n",
    "\n",
    "# reduced features for state-value function\n",
    "# no offset is included in the feature vectors since v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Mr = 4\n",
    "Hr = np.zeros((NS, Mr))\n",
    "Hr[0,:]  = np.array([0, 0, 1, 0]) # state 1\n",
    "Hr[1,:]  = np.array([0, 0, 1, 0]) # state 2\n",
    "Hr[2,:]  = np.array([0, 0, 0, 1]) # state 3\n",
    "Hr[3,:]  = np.array([0, 0, 0, 0]) # not a valid state\n",
    "Hr[4,:]  = np.array([0, 1, 0, 1]) # state 5...\n",
    "Hr[5,:]  = np.array([0, 1, 0, 1])\n",
    "Hr[6,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[7,:]  = np.array([0, 1, 1, 0])\n",
    "Hr[8,:]  = np.array([0, 0, 1, 0])\n",
    "Hr[9,:] = np.array([0, 0, 1, 0])\n",
    "Hr[10,:] = np.array([0, 0, 0, 0]) # not a valid state \n",
    "Hr[11,:] = np.array([0, 0, 0, 1])\n",
    "Hr[12,:] = np.array([1, 0, 0, 1])\n",
    "Hr[13,:] = np.array([1, 0, 0, 1])\n",
    "Hr[14,:] = np.array([1, 0, 1, 0])\n",
    "Hr[15,:] = np.array([1, 0, 1, 0]) # state 16\n",
    "Hr[16,:] = np.array([0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fr = np.kron(Hr, A) # Kronecker product of dimensions (NSxNA) x (MrxNA)\n",
    "Tr = Mr*NA\n",
    "\n",
    "# one-hot encoded feature vectors for state-value function\n",
    "# no offset is included in the feature vectors because v^{\\pi}=0 at state 17\n",
    "# v^{\\pi}(s) = h'*w\n",
    "\n",
    "Me = NS\n",
    "He = np.zeros((NS, Me))\n",
    "He[0,:]   = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 1\n",
    "He[1,:]   = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 2\n",
    "He[2,:]   = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 3\n",
    "He[3,:]   = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[4,:]   = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # state 5\n",
    "He[5,:]   = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # ...\n",
    "He[6,:]   = np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[7,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[8,:]   = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[9,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "He[10,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]) # not valid state\n",
    "He[11,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "He[12,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "He[13,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "He[14,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
    "He[15,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]) # state 16\n",
    "He[16,:]  = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # EXIT state\n",
    "\n",
    "Fe = np.kron(He, A) # Kronecker product of dimensions (NSxNA) x (MexNA)\n",
    "Te = Me*NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 down\n",
      "2 up\n",
      "3 up\n",
      "5 up\n",
      "6 left\n",
      "7 left\n",
      "8 stop\n",
      "9 up\n",
      "10 up\n",
      "12 up\n",
      "13 right\n",
      "14 right\n",
      "15 right\n",
      "16 stop\n",
      "17 stop\n",
      "[[  94.67839089  107.77649441  104.88192131  103.40640133  100.        ]\n",
      " [ 108.64136084  107.56378441  107.7978384   107.17389268  100.        ]\n",
      " [ 108.82349707  107.99568507  107.97622014  107.95527212  100.        ]\n",
      " [ 100.          100.          100.          100.         -261.441     ]\n",
      " [ 109.20840562  108.52212306  108.54057623  108.590292    100.        ]\n",
      " [ 108.59504551  108.44819562  109.04015803  108.48017103  100.        ]\n",
      " [ 106.58022704  105.2559485   108.96588207   95.52035231  100.        ]\n",
      " [ 100.          100.          100.          100.           90.        ]\n",
      " [ 109.83254345   95.67699701  105.24131119  105.22059385  100.        ]\n",
      " [ 109.70864799  108.75325587  109.38233091  109.54897926  100.        ]\n",
      " [ 100.          100.          100.          100.         -251.08260866]\n",
      " [ 109.38376481  108.23449245  108.43942792  108.59703859  100.        ]\n",
      " [ 109.06907061  108.87544989  108.96084094  109.51417951  100.        ]\n",
      " [ 109.45669824  109.52388078  109.30646705  109.68140767  100.        ]\n",
      " [ 109.74204276  109.65504431  109.60792267  109.83774036  100.        ]\n",
      " [ 100.          100.          100.          100.          110.        ]\n",
      " [ 100.          100.          100.          100.          100.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning under epsilon-greedy exploration\n",
    "\n",
    "E = 50000 # number of episodes\n",
    "gamma = 0.9\n",
    "mu = 0.01\n",
    "epsilon = 0.1 # parameter for epsilon-greedy exploration\n",
    "rmin = 0.1\n",
    "rmax = 10\n",
    "Q = (max(rmin, rmax)/ (1-gamma))*np.ones((NS, NA)) # Q(s, a) matrix; optimistic initialization\n",
    "beta = np.zeros((NS, NA))\n",
    "max_episode_duration = 50\n",
    "\n",
    "q_vec = np.zeros(NA)\n",
    "q_prime_vec = np.zeros(NA)\n",
    "a_vec = np.zeros(NA)\n",
    "kernel = np.zeros(NS)\n",
    "\n",
    "for e in range(E): # iterates over episodes\n",
    "    counter = 0\n",
    "    sample = 1\n",
    "    while sample == 1:\n",
    "        idx = np.random.randint(NS-1)+1 # select a random non-exit state index\n",
    "        if (idx != 4) and (idx != 11) and (idx != 17): # excluding the block locations and exit state\n",
    "            s = states[idx-1]\n",
    "            sample = 0\n",
    "    \n",
    "    while (s != 17) and (counter < max_episode_duration): # state s different from EXIT state\n",
    "        q = Q[s, :] # row in Q corresponding to state s\n",
    "\n",
    "        pi_vec = Pi[:, s] # policy vector at state s --> determines which actions are possible ar s\n",
    "        counter2 = 0\n",
    "        for j in range(NA):\n",
    "            if pi_vec[j] > 0: # the j-th action is possible\n",
    "                q_vec[counter2] = q[j] # q-value\n",
    "                a_vec[counter2] = j # corresponding valid action\n",
    "                counter2 += 1\n",
    "        ax = np.argmax(q_vec[0:counter2]) # permissible action at s with largest q-value\n",
    "        idx = (q_vec[0:counter2]).max()\n",
    "        act1 = int(a_vec[ax]) # index of the permissible action\n",
    "\n",
    "        y = np.random.rand() # epsilon-greed strategy\n",
    "        if y <= epsilon:\n",
    "            ay = np.random.randint(counter2) # choose from actions permissible at state s\n",
    "            act = int(a_vec[ay])\n",
    "        else:\n",
    "            act = act1\n",
    "        \n",
    "        for j in range(NS):\n",
    "            kernel[j] = P[s, act, j]\n",
    "        \n",
    "        sprime = select_next_state(kernel)\n",
    "        r = reward[s]\n",
    "\n",
    "        pi_prime_vec = Pi[:, sprime] # again, we find max Q(s', a') over permissible actions at s'\n",
    "        counter3 = 0\n",
    "        q_prime = Q[sprime, :]\n",
    "        for j in range(NA):\n",
    "            if pi_prime_vec[j] > 0: # the h-th action is possible\n",
    "                q_prime_vec[counter3] = q_prime[j] # q_value\n",
    "                counter3 += 1\n",
    "        \n",
    "        max_value = (q_prime_vec[0:counter3]).max() # maximum over permissible actions at s' \n",
    "\n",
    "        beta[s, act] = r + max_value - Q[s, act]\n",
    "        Q[s, act] += mu*beta[s, act]\n",
    "        s = sprime\n",
    "        counter += 1\n",
    "\n",
    "# after convergence, we determine the optimal policy from the resulting Q martrix\n",
    "\n",
    "act = np.zeros(NS)\n",
    "action_state = [None]*NS\n",
    "act[3] = -1 # no action since 4 is not a valud state\n",
    "act[10] = - 1 # 11 is not a valid state\n",
    "action_state[3] = 'NA' # no action; not applicable since 4 and 11 are not valid states\n",
    "action_state[10] = 'NA'\n",
    "\n",
    "for s in range(NS):\n",
    "    if (s != 3) and (s != 10): # not valid states; exclude them\n",
    "        q = Q[s, :] # row in Q corresponding to state s\n",
    "        pi_vec = Pi[:, s] # policy vector at state s --> determines which actions are possible ar s\n",
    "        counter = 0\n",
    "        for j in range(NA):\n",
    "            if pi_vec[j] > 0: # the j-th action in possible\n",
    "                q_vec[counter] = q[j] # q-value\n",
    "                a_vec[counter] = j # corresponding valid action\n",
    "                counter += 1\n",
    "        ax = np.argmax(q_vec[0:counter]) # permissible action at s with largest q-value\n",
    "        idx = (q_vec[0:counter]).max()\n",
    "        act[s] = int(a_vec[ax]) # index of the permissible action\n",
    "        action_state[s] = actions[int(act[s])]\n",
    "        print(s+1, action_state[s])\n",
    "\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
