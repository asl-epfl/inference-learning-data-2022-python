{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"data/datafile.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = data[\"D\"].item()\n",
    "n_test = data[\"T\"].item()\n",
    "word_types = data[\"TW\"].item()\n",
    "features_size = data[\"M\"].item()\n",
    "P = 30 # size of hidden layer for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_train = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"H_train\"]])\n",
    "H_test = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"H_test\"]])\n",
    "\n",
    "labels_train = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"labels_train\"]])\n",
    "labels_test = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"labels_test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAnalysis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size, word_types)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceAnalysis()\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4000. Training Loss: 2.537. Training Accuracy: 0.0. Test Loss: 2.493. Test Accuracy: 0.0.\n",
      "Epoch: 100/4000. Training Loss: 1.698. Training Accuracy: 8.919. Test Loss: 1.608. Test Accuracy: 11.806.\n",
      "Epoch: 200/4000. Training Loss: 1.508. Training Accuracy: 18.915. Test Loss: 1.385. Test Accuracy: 23.611.\n",
      "Epoch: 300/4000. Training Loss: 1.338. Training Accuracy: 21.785. Test Loss: 1.201. Test Accuracy: 29.514.\n",
      "Epoch: 400/4000. Training Loss: 1.193. Training Accuracy: 27.797. Test Loss: 1.071. Test Accuracy: 35.764.\n",
      "Epoch: 500/4000. Training Loss: 1.086. Training Accuracy: 42.079. Test Loss: 0.959. Test Accuracy: 50.347.\n",
      "Epoch: 600/4000. Training Loss: 1.007. Training Accuracy: 46.693. Test Loss: 0.863. Test Accuracy: 61.806.\n",
      "Epoch: 700/4000. Training Loss: 0.94. Training Accuracy: 49.497. Test Loss: 0.8. Test Accuracy: 64.583.\n",
      "Epoch: 800/4000. Training Loss: 0.885. Training Accuracy: 52.006. Test Loss: 0.775. Test Accuracy: 70.833.\n",
      "Epoch: 900/4000. Training Loss: 0.833. Training Accuracy: 54.884. Test Loss: 0.737. Test Accuracy: 70.833.\n",
      "Epoch: 1000/4000. Training Loss: 0.787. Training Accuracy: 55.44. Test Loss: 0.715. Test Accuracy: 70.833.\n",
      "Epoch: 1100/4000. Training Loss: 0.74. Training Accuracy: 60.054. Test Loss: 0.689. Test Accuracy: 73.958.\n",
      "Epoch: 1200/4000. Training Loss: 0.695. Training Accuracy: 60.61. Test Loss: 0.674. Test Accuracy: 77.083.\n",
      "Epoch: 1300/4000. Training Loss: 0.65. Training Accuracy: 61.957. Test Loss: 0.653. Test Accuracy: 77.083.\n",
      "Epoch: 1400/4000. Training Loss: 0.605. Training Accuracy: 63.724. Test Loss: 0.653. Test Accuracy: 77.083.\n",
      "Epoch: 1500/4000. Training Loss: 0.56. Training Accuracy: 65.492. Test Loss: 0.657. Test Accuracy: 77.083.\n",
      "Epoch: 1600/4000. Training Loss: 0.517. Training Accuracy: 67.209. Test Loss: 0.646. Test Accuracy: 77.083.\n",
      "Epoch: 1700/4000. Training Loss: 0.479. Training Accuracy: 69.324. Test Loss: 0.638. Test Accuracy: 82.639.\n",
      "Epoch: 1800/4000. Training Loss: 0.444. Training Accuracy: 73.637. Test Loss: 0.636. Test Accuracy: 79.861.\n",
      "Epoch: 1900/4000. Training Loss: 0.415. Training Accuracy: 76.317. Test Loss: 0.635. Test Accuracy: 79.861.\n",
      "Epoch: 2000/4000. Training Loss: 0.388. Training Accuracy: 82.593. Test Loss: 0.621. Test Accuracy: 79.861.\n",
      "Epoch: 2100/4000. Training Loss: 0.363. Training Accuracy: 83.199. Test Loss: 0.612. Test Accuracy: 79.861.\n",
      "Epoch: 2200/4000. Training Loss: 0.342. Training Accuracy: 83.292. Test Loss: 0.61. Test Accuracy: 79.861.\n",
      "Epoch: 2300/4000. Training Loss: 0.321. Training Accuracy: 83.292. Test Loss: 0.615. Test Accuracy: 82.639.\n",
      "Epoch: 2400/4000. Training Loss: 0.301. Training Accuracy: 85.985. Test Loss: 0.6. Test Accuracy: 82.639.\n",
      "Epoch: 2500/4000. Training Loss: 0.28. Training Accuracy: 85.591. Test Loss: 0.607. Test Accuracy: 79.861.\n",
      "Epoch: 2600/4000. Training Loss: 0.264. Training Accuracy: 86.197. Test Loss: 0.602. Test Accuracy: 77.083.\n",
      "Epoch: 2700/4000. Training Loss: 0.245. Training Accuracy: 88.15. Test Loss: 0.603. Test Accuracy: 77.083.\n",
      "Epoch: 2800/4000. Training Loss: 0.228. Training Accuracy: 89.497. Test Loss: 0.603. Test Accuracy: 79.861.\n",
      "Epoch: 2900/4000. Training Loss: 0.214. Training Accuracy: 90.052. Test Loss: 0.606. Test Accuracy: 79.861.\n",
      "Epoch: 3000/4000. Training Loss: 0.202. Training Accuracy: 91.214. Test Loss: 0.618. Test Accuracy: 77.083.\n",
      "Epoch: 3100/4000. Training Loss: 0.19. Training Accuracy: 91.214. Test Loss: 0.627. Test Accuracy: 79.861.\n",
      "Epoch: 3200/4000. Training Loss: 0.177. Training Accuracy: 93.362. Test Loss: 0.625. Test Accuracy: 82.986.\n",
      "Epoch: 3300/4000. Training Loss: 0.167. Training Accuracy: 93.362. Test Loss: 0.62. Test Accuracy: 82.986.\n",
      "Epoch: 3400/4000. Training Loss: 0.156. Training Accuracy: 94.769. Test Loss: 0.643. Test Accuracy: 83.333.\n",
      "Epoch: 3500/4000. Training Loss: 0.146. Training Accuracy: 94.769. Test Loss: 0.656. Test Accuracy: 86.111.\n",
      "Epoch: 3600/4000. Training Loss: 0.139. Training Accuracy: 94.769. Test Loss: 0.697. Test Accuracy: 80.556.\n",
      "Epoch: 3700/4000. Training Loss: 0.13. Training Accuracy: 95.992. Test Loss: 0.73. Test Accuracy: 80.556.\n",
      "Epoch: 3800/4000. Training Loss: 0.123. Training Accuracy: 95.931. Test Loss: 0.732. Test Accuracy: 80.556.\n",
      "Epoch: 3900/4000. Training Loss: 0.115. Training Accuracy: 96.764. Test Loss: 0.803. Test Accuracy: 80.556.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model.train()\n",
    "    for x, y in zip(H_train, labels_train):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "        loss_out.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in zip(H_test, labels_test):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word; Word Id; Label; Label Hat\n",
      "START 3 1 1\n",
      "It 48 5 5\n",
      "was 77 3 3\n",
      "a 4 2 2\n",
      "great 36 6 6\n",
      "play 58 4 4\n",
      ". 1 10 10\n",
      "END 2 11 11\n"
     ]
    }
   ],
   "source": [
    "sentence = data[\"sentences\"][0][0].item().split(\" \")\n",
    "sentence_idx = H_train[0][:8]\n",
    "labels = labels_train[0][:8]\n",
    "labels_hat = torch.round(nn.functional.softmax(model(sentence_idx.float().unsqueeze(-1)), dim=-1))\n",
    "\n",
    "print(\"Word; Word Id; Label; Label Hat\")\n",
    "for word, word_id, label, label_hat in zip(sentence, sentence_idx, labels, labels_hat):\n",
    "    print(word, word_id.item(), label.item(), (label_hat==1).nonzero().item()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_sentiment = torch.tensor([sentence.item() for sentence in data[\"sentiment_train\"]])\n",
    "labels_test_sentiment = torch.tensor([sentence.item() for sentence in data[\"sentiment_test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0][-1])\n",
    "        y = self.sigmoid(logits)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment = SentimentAnalysis()\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(model_sentiment.parameters(), lr=lr)\n",
    "loss = nn.BCELoss()\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000. Training Loss: 0.702. Training Accuracy: 46.667. Test Loss: 0.697. Test Accuracy: 50.0.\n",
      "Epoch: 100/1000. Training Loss: 0.593. Training Accuracy: 66.667. Test Loss: 0.561. Test Accuracy: 75.0.\n",
      "Epoch: 200/1000. Training Loss: 0.343. Training Accuracy: 93.333. Test Loss: 0.365. Test Accuracy: 75.0.\n",
      "Epoch: 300/1000. Training Loss: 0.053. Training Accuracy: 100.0. Test Loss: 0.239. Test Accuracy: 75.0.\n",
      "Epoch: 400/1000. Training Loss: 0.004. Training Accuracy: 100.0. Test Loss: 0.598. Test Accuracy: 75.0.\n",
      "Epoch: 500/1000. Training Loss: 0.001. Training Accuracy: 100.0. Test Loss: 1.002. Test Accuracy: 75.0.\n",
      "Epoch: 600/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.04. Test Accuracy: 75.0.\n",
      "Epoch: 700/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.091. Test Accuracy: 75.0.\n",
      "Epoch: 800/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.247. Test Accuracy: 75.0.\n",
      "Epoch: 900/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.086. Test Accuracy: 75.0.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model_sentiment.train()\n",
    "    for x, y in zip(H_train, labels_train_sentiment):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "\n",
    "        y_hat = model_sentiment(x)\n",
    "\n",
    "        loss_out = loss(y_hat, y.unsqueeze(-1).float())\n",
    "        loss_out.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(y_hat)\n",
    "\n",
    "        errors = (y != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[-1] - errors)/x.shape[-1]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model_sentiment.eval()\n",
    "    for x, y in zip(H_test, labels_test_sentiment):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "\n",
    "        y_hat = model_sentiment(x)\n",
    "\n",
    "        loss_out = loss(y_hat, y.unsqueeze(-1).float())\n",
    "\n",
    "        class_hat = torch.round(y_hat)\n",
    "\n",
    "        errors = (y != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[-1] - errors)/x.shape[-1]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data[\"sentences\"][0][0].item()\n",
    "sentence_idx = H_train[0][:8]\n",
    "label = labels_train_sentiment[0]\n",
    "labels_hat = torch.round(model_sentiment(sentence_idx.float().unsqueeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START It was a great play . END 1 1.0\n",
      "START I had a bad experience with the rental car . END 0 0.0\n",
      "START Her flight was delayed for over five hours . END 0 0.0\n",
      "START She passed her exam easily . END 1 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    sentence = data[\"sentences\"][0][i].item()\n",
    "    final_len = (H_train[i] == 2).nonzero().squeeze().item()+1\n",
    "    sentence_idx = H_train[i][:final_len]\n",
    "    label = labels_train_sentiment[i]\n",
    "    label_hat = torch.round(model_sentiment(sentence_idx.float().unsqueeze(-1)))\n",
    "    print(sentence, label.item(), label_hat.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAnalysisLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size, word_types)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SentenceAnalysisLSTM()\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4000. Training Loss: 2.419. Training Accuracy: 0.0. Test Loss: 2.399. Test Accuracy: 0.0.\n",
      "Epoch: 100/4000. Training Loss: 2.01. Training Accuracy: 0.0. Test Loss: 1.973. Test Accuracy: 0.0.\n",
      "Epoch: 200/4000. Training Loss: 1.734. Training Accuracy: 10.131. Test Loss: 1.642. Test Accuracy: 11.806.\n",
      "Epoch: 300/4000. Training Loss: 1.57. Training Accuracy: 19.656. Test Loss: 1.471. Test Accuracy: 23.611.\n",
      "Epoch: 400/4000. Training Loss: 1.447. Training Accuracy: 19.656. Test Loss: 1.35. Test Accuracy: 23.611.\n",
      "Epoch: 500/4000. Training Loss: 1.338. Training Accuracy: 23.09. Test Loss: 1.228. Test Accuracy: 26.389.\n",
      "Epoch: 600/4000. Training Loss: 1.245. Training Accuracy: 25.826. Test Loss: 1.114. Test Accuracy: 29.167.\n",
      "Epoch: 700/4000. Training Loss: 1.161. Training Accuracy: 31.281. Test Loss: 1.027. Test Accuracy: 40.972.\n",
      "Epoch: 800/4000. Training Loss: 1.081. Training Accuracy: 32.443. Test Loss: 0.955. Test Accuracy: 44.097.\n",
      "Epoch: 900/4000. Training Loss: 1.01. Training Accuracy: 44.847. Test Loss: 0.894. Test Accuracy: 59.028.\n",
      "Epoch: 1000/4000. Training Loss: 0.947. Training Accuracy: 47.146. Test Loss: 0.828. Test Accuracy: 62.153.\n",
      "Epoch: 1100/4000. Training Loss: 0.889. Training Accuracy: 48.368. Test Loss: 0.781. Test Accuracy: 62.153.\n",
      "Epoch: 1200/4000. Training Loss: 0.836. Training Accuracy: 48.924. Test Loss: 0.74. Test Accuracy: 62.153.\n",
      "Epoch: 1300/4000. Training Loss: 0.792. Training Accuracy: 50.136. Test Loss: 0.714. Test Accuracy: 62.153.\n",
      "Epoch: 1400/4000. Training Loss: 0.751. Training Accuracy: 54.176. Test Loss: 0.696. Test Accuracy: 68.056.\n",
      "Epoch: 1500/4000. Training Loss: 0.712. Training Accuracy: 57.132. Test Loss: 0.672. Test Accuracy: 71.181.\n",
      "Epoch: 1600/4000. Training Loss: 0.684. Training Accuracy: 57.687. Test Loss: 0.658. Test Accuracy: 71.181.\n",
      "Epoch: 1700/4000. Training Loss: 0.644. Training Accuracy: 59.733. Test Loss: 0.645. Test Accuracy: 73.958.\n",
      "Epoch: 1800/4000. Training Loss: 0.617. Training Accuracy: 62.056. Test Loss: 0.631. Test Accuracy: 73.958.\n",
      "Epoch: 1900/4000. Training Loss: 0.585. Training Accuracy: 64.625. Test Loss: 0.623. Test Accuracy: 73.958.\n",
      "Epoch: 2000/4000. Training Loss: 0.55. Training Accuracy: 67.472. Test Loss: 0.607. Test Accuracy: 71.181.\n",
      "Epoch: 2100/4000. Training Loss: 0.524. Training Accuracy: 68.684. Test Loss: 0.616. Test Accuracy: 71.181.\n",
      "Epoch: 2200/4000. Training Loss: 0.498. Training Accuracy: 71.378. Test Loss: 0.609. Test Accuracy: 73.958.\n",
      "Epoch: 2300/4000. Training Loss: 0.473. Training Accuracy: 72.526. Test Loss: 0.596. Test Accuracy: 71.181.\n",
      "Epoch: 2400/4000. Training Loss: 0.446. Training Accuracy: 76.529. Test Loss: 0.583. Test Accuracy: 73.958.\n",
      "Epoch: 2500/4000. Training Loss: 0.423. Training Accuracy: 78.196. Test Loss: 0.572. Test Accuracy: 73.958.\n",
      "Epoch: 2600/4000. Training Loss: 0.401. Training Accuracy: 78.061. Test Loss: 0.553. Test Accuracy: 74.306.\n",
      "Epoch: 2700/4000. Training Loss: 0.379. Training Accuracy: 78.894. Test Loss: 0.548. Test Accuracy: 74.306.\n",
      "Epoch: 2800/4000. Training Loss: 0.358. Training Accuracy: 78.061. Test Loss: 0.544. Test Accuracy: 74.306.\n",
      "Epoch: 2900/4000. Training Loss: 0.339. Training Accuracy: 82.601. Test Loss: 0.551. Test Accuracy: 74.306.\n",
      "Epoch: 3000/4000. Training Loss: 0.316. Training Accuracy: 83.115. Test Loss: 0.552. Test Accuracy: 74.306.\n",
      "Epoch: 3100/4000. Training Loss: 0.298. Training Accuracy: 83.948. Test Loss: 0.556. Test Accuracy: 71.181.\n",
      "Epoch: 3200/4000. Training Loss: 0.28. Training Accuracy: 87.285. Test Loss: 0.597. Test Accuracy: 71.181.\n",
      "Epoch: 3300/4000. Training Loss: 0.267. Training Accuracy: 88.446. Test Loss: 0.601. Test Accuracy: 74.306.\n",
      "Epoch: 3400/4000. Training Loss: 0.251. Training Accuracy: 89.002. Test Loss: 0.622. Test Accuracy: 74.306.\n",
      "Epoch: 3500/4000. Training Loss: 0.233. Training Accuracy: 90.348. Test Loss: 0.608. Test Accuracy: 77.083.\n",
      "Epoch: 3600/4000. Training Loss: 0.221. Training Accuracy: 90.904. Test Loss: 0.596. Test Accuracy: 77.083.\n",
      "Epoch: 3700/4000. Training Loss: 0.207. Training Accuracy: 92.251. Test Loss: 0.572. Test Accuracy: 79.861.\n",
      "Epoch: 3800/4000. Training Loss: 0.195. Training Accuracy: 91.973. Test Loss: 0.607. Test Accuracy: 80.208.\n",
      "Epoch: 3900/4000. Training Loss: 0.183. Training Accuracy: 94.029. Test Loss: 0.634. Test Accuracy: 79.861.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model.train()\n",
    "    for x, y in zip(H_train, labels_train):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "        loss_out.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in zip(H_test, labels_test):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word; Word Id; Label; Label Hat\n",
      "START 3 1 1\n",
      "It 48 5 5\n",
      "was 77 3 3\n",
      "a 4 2 2\n",
      "great 36 6 6\n",
      "play 58 4 4\n",
      ". 1 10 10\n",
      "END 2 11 11\n"
     ]
    }
   ],
   "source": [
    "sentence = data[\"sentences\"][0][0].item().split(\" \")\n",
    "sentence_idx = H_train[0][:8]\n",
    "labels = labels_train[0][:8]\n",
    "labels_hat = torch.round(nn.functional.softmax(lstm_model(sentence_idx.float().unsqueeze(-1)), dim=-1))\n",
    "\n",
    "print(\"Word; Word Id; Label; Label Hat\")\n",
    "for word, word_id, label, label_hat in zip(sentence, sentence_idx, labels, labels_hat):\n",
    "    print(word, word_id.item(), label.item(), (label_hat==1).nonzero().item()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAnalysisBiderectionalLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1, bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size*2, word_types)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_lstm_model = SentenceAnalysisBiderectionalLSTM()\n",
    "lr = 1e-5\n",
    "bidirectional_lstm_optimizer = torch.optim.Adam(bidirectional_lstm_model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4000. Training Loss: 2.37. Training Accuracy: 0.0. Test Loss: 2.385. Test Accuracy: 0.0.\n",
      "Epoch: 100/4000. Training Loss: 1.763. Training Accuracy: 0.0. Test Loss: 1.704. Test Accuracy: 0.0.\n",
      "Epoch: 200/4000. Training Loss: 1.386. Training Accuracy: 29.18. Test Loss: 1.271. Test Accuracy: 35.417.\n",
      "Epoch: 300/4000. Training Loss: 1.191. Training Accuracy: 30.898. Test Loss: 1.072. Test Accuracy: 38.194.\n",
      "Epoch: 400/4000. Training Loss: 1.057. Training Accuracy: 41.152. Test Loss: 0.956. Test Accuracy: 55.903.\n",
      "Epoch: 500/4000. Training Loss: 0.955. Training Accuracy: 45.817. Test Loss: 0.864. Test Accuracy: 55.903.\n",
      "Epoch: 600/4000. Training Loss: 0.876. Training Accuracy: 49.363. Test Loss: 0.81. Test Accuracy: 62.153.\n",
      "Epoch: 700/4000. Training Loss: 0.81. Training Accuracy: 52.242. Test Loss: 0.764. Test Accuracy: 62.153.\n",
      "Epoch: 800/4000. Training Loss: 0.751. Training Accuracy: 55.028. Test Loss: 0.74. Test Accuracy: 62.153.\n",
      "Epoch: 900/4000. Training Loss: 0.697. Training Accuracy: 59.775. Test Loss: 0.729. Test Accuracy: 71.181.\n",
      "Epoch: 1000/4000. Training Loss: 0.646. Training Accuracy: 61.048. Test Loss: 0.711. Test Accuracy: 71.181.\n",
      "Epoch: 1100/4000. Training Loss: 0.597. Training Accuracy: 64.03. Test Loss: 0.694. Test Accuracy: 71.181.\n",
      "Epoch: 1200/4000. Training Loss: 0.55. Training Accuracy: 65.191. Test Loss: 0.697. Test Accuracy: 71.181.\n",
      "Epoch: 1300/4000. Training Loss: 0.508. Training Accuracy: 70.009. Test Loss: 0.713. Test Accuracy: 71.181.\n",
      "Epoch: 1400/4000. Training Loss: 0.467. Training Accuracy: 73.309. Test Loss: 0.718. Test Accuracy: 71.181.\n",
      "Epoch: 1500/4000. Training Loss: 0.43. Training Accuracy: 79.482. Test Loss: 0.739. Test Accuracy: 71.181.\n",
      "Epoch: 1600/4000. Training Loss: 0.394. Training Accuracy: 81.495. Test Loss: 0.755. Test Accuracy: 71.181.\n",
      "Epoch: 1700/4000. Training Loss: 0.358. Training Accuracy: 82.051. Test Loss: 0.765. Test Accuracy: 74.306.\n",
      "Epoch: 1800/4000. Training Loss: 0.326. Training Accuracy: 83.869. Test Loss: 0.765. Test Accuracy: 74.306.\n",
      "Epoch: 1900/4000. Training Loss: 0.295. Training Accuracy: 84.536. Test Loss: 0.782. Test Accuracy: 71.528.\n",
      "Epoch: 2000/4000. Training Loss: 0.265. Training Accuracy: 86.549. Test Loss: 0.805. Test Accuracy: 71.528.\n",
      "Epoch: 2100/4000. Training Loss: 0.238. Training Accuracy: 88.941. Test Loss: 0.842. Test Accuracy: 71.528.\n",
      "Epoch: 2200/4000. Training Loss: 0.212. Training Accuracy: 89.608. Test Loss: 0.844. Test Accuracy: 71.528.\n",
      "Epoch: 2300/4000. Training Loss: 0.188. Training Accuracy: 91.386. Test Loss: 0.879. Test Accuracy: 71.528.\n",
      "Epoch: 2400/4000. Training Loss: 0.167. Training Accuracy: 93.608. Test Loss: 0.897. Test Accuracy: 71.528.\n",
      "Epoch: 2500/4000. Training Loss: 0.147. Training Accuracy: 95.992. Test Loss: 0.912. Test Accuracy: 71.528.\n",
      "Epoch: 2600/4000. Training Loss: 0.129. Training Accuracy: 97.566. Test Loss: 0.946. Test Accuracy: 71.528.\n",
      "Epoch: 2700/4000. Training Loss: 0.111. Training Accuracy: 98.727. Test Loss: 0.952. Test Accuracy: 71.528.\n",
      "Epoch: 2800/4000. Training Loss: 0.096. Training Accuracy: 99.394. Test Loss: 0.967. Test Accuracy: 71.528.\n",
      "Epoch: 2900/4000. Training Loss: 0.082. Training Accuracy: 100.0. Test Loss: 0.997. Test Accuracy: 71.528.\n",
      "Epoch: 3000/4000. Training Loss: 0.071. Training Accuracy: 100.0. Test Loss: 1.027. Test Accuracy: 71.528.\n",
      "Epoch: 3100/4000. Training Loss: 0.061. Training Accuracy: 100.0. Test Loss: 1.046. Test Accuracy: 71.528.\n",
      "Epoch: 3200/4000. Training Loss: 0.052. Training Accuracy: 100.0. Test Loss: 1.057. Test Accuracy: 71.528.\n",
      "Epoch: 3300/4000. Training Loss: 0.044. Training Accuracy: 100.0. Test Loss: 1.071. Test Accuracy: 71.528.\n",
      "Epoch: 3400/4000. Training Loss: 0.037. Training Accuracy: 100.0. Test Loss: 1.096. Test Accuracy: 71.528.\n",
      "Epoch: 3500/4000. Training Loss: 0.032. Training Accuracy: 100.0. Test Loss: 1.107. Test Accuracy: 71.528.\n",
      "Epoch: 3600/4000. Training Loss: 0.027. Training Accuracy: 100.0. Test Loss: 1.103. Test Accuracy: 71.528.\n",
      "Epoch: 3700/4000. Training Loss: 0.023. Training Accuracy: 100.0. Test Loss: 1.125. Test Accuracy: 71.528.\n",
      "Epoch: 3800/4000. Training Loss: 0.02. Training Accuracy: 100.0. Test Loss: 1.15. Test Accuracy: 71.528.\n",
      "Epoch: 3900/4000. Training Loss: 0.017. Training Accuracy: 100.0. Test Loss: 1.16. Test Accuracy: 74.306.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model.train()\n",
    "    for x, y in zip(H_train, labels_train):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = bidirectional_lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "        loss_out.backward()\n",
    "        bidirectional_lstm_optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in zip(H_test, labels_test):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = bidirectional_lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word; Word Id; Label; Label Hat\n",
      "START 3 1 1\n",
      "It 48 5 5\n",
      "was 77 3 3\n",
      "a 4 2 2\n",
      "great 36 6 6\n",
      "play 58 4 4\n",
      ". 1 10 10\n",
      "END 2 11 11\n"
     ]
    }
   ],
   "source": [
    "sentence = data[\"sentences\"][0][0].item().split(\" \")\n",
    "sentence_idx = H_train[0][:8]\n",
    "labels = labels_train[0][:8]\n",
    "labels_hat = torch.round(nn.functional.softmax(bidirectional_lstm_model(sentence_idx.float().unsqueeze(-1)), dim=-1))\n",
    "\n",
    "print(\"Word; Word Id; Label; Label Hat\")\n",
    "for word, word_id, label, label_hat in zip(sentence, sentence_idx, labels, labels_hat):\n",
    "    print(word, word_id.item(), label.item(), (label_hat==1).nonzero().item()+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
