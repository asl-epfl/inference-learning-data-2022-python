{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 69 (Recurrent Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code runs simulations for Examples 4 and 7 in Chapter 69: Recurrent Neural Networks (vol. III)\n",
    "TEXT: A. H. Sayed, INFERENCE AND LEARNING FROM DATA, Cambridge University Press, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "DISCLAIMER:  This computer code is  provided  \"as is\"   without  any  guarantees.\n",
    "Practitioners  should  use it  at their own risk.  While  the  codes in  the text \n",
    "are useful for instructional purposes, they are not intended to serve as examples \n",
    "of full-blown or optimized designs. The author has made no attempt at optimizing \n",
    "the codes, perfecting them, or even checking them for absolute accuracy. In order \n",
    "to keep the codes at a level  that is  easy to follow by students, the author has \n",
    "often chosen to  sacrifice  performance or even programming elegance in  lieu  of \n",
    "simplicity. Students can use the computer codes to run variations of the examples \n",
    "shown in the text. \n",
    "</div>\n",
    "\n",
    "The Jupyter notebook and python codes are developed by Eduardo Faria Cabrera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "required libraries:\n",
    "    \n",
    "1. numpy\n",
    "2. matplotlib\n",
    "3. scipy\n",
    "4. torch\n",
    "5. tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 69.4 (Sentence Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we apply the RNN construction to analyze the structure of a sentence, as well as the mood that is reflected by the same sentence. By structure we mean that the RNN will classify the individual words into nouns, verbs, adjectives, adverbs, pronouns, and so forth. By mood we mean that the RNN will detect whether the sentence is reflecting a good or bad sentiment.\n",
    "\n",
    "We consider a dictionary consisting of $D=15$ sentences; this is of course a small training set and is only meant for illustration purposes. At the end of each sentence, we indicate its mood or sentiment (good or bad) for later use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\textnormal{  <sos> It was a great play . <eos>  }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> I had a bad experience with the rental car . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> Her flight was delayed for over five hours . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> She passed her exam easily . <eos>   }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> He regularly exercises and goes to the beach . <eos>   }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> The examination was hard and he is nervous . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> They are happy their team won the championship game . <eos>   }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> I do not like action movies . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> I love juicy fruits . <eos>   }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> He was nervous about his visit to the doctor . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> We enjoyed walking in the park . <eos>   }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> The plot in this book is terrible . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> The chair I bought is comfortable and affordable . <eos>   }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> The desk fit splendidly into the room . <eos>   }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> It rained and the program was unfortunately canceled . <eos>}&\\textnormal{ (bad) }\n",
    "\\end{array} \\tag{69.80}\n",
    "$$\n",
    "\n",
    "The beginning and end of each sentence are marked by the commands <sos> (start-of-sentence) and <eos>  (end-of-sentence). We parse through the sentences and collect the individual words (in lower-case letters) into a collection of  alphabetically ordered words (including the punctuation mark \".\" and the <eos> and <sos> symbols):\n",
    "\n",
    "$$\n",
    "\\textnormal{ words}=\\\\\n",
    "\\begin{array}{l}\n",
    "\\Bigl\\{\\textnormal{ \n",
    " \".\",     \"eos\",     \"sos\",     \"a\",     \"about\",     \"action\",     \"affordable\",     \"and\",     \"are\",     \"bad\",}\\\\ \\\\\n",
    "\\quad \\textnormal{  \"beach\",     \"book\",     \"bought\",     \"canceled\",     \"car\",     \"chair\",     \"championship\",     \"comfortable\",}\\\\ \\\\\n",
    "\\quad\\textnormal{  \"delayed\",     \"desk\",     \"do\",     \"doctor\",     \"easily\",     \"enjoyed\",     \"exam\",     \"examination\",}\\\\ \\\\\n",
    "\\quad\\textnormal{  \"exercises\",     \"experience\",     \"fit\",     \"five\",     \"flight\",     \"for\",     \"fruits\",     \"game\",     \"goes\",}\\\\ \\\\\n",
    "\\quad\\textnormal{  \"great\",     \"had\",     \"happy\",     \"hard\",     \"he\",     \"her\",     \"his\",     \"hours\",     \"i\",     \"in\",}\\\\ \\\\\n",
    "\\quad\\textnormal{  \"into\",     \"is\",     \"it\",     \"juicy\",     \"like\",     \"love\",     \"movies\",     \"nervous\",     \"not\",     \"over\",}\\\\ \\\\\n",
    "\\quad\\textnormal{  \"park\",     \"passed\",     \"play\",     \"plot\",     \"program\",     \"rained\",     \"regularly\",     \"rental\",     \"room\",}\\\\ \\\\\n",
    "\\quad\\textnormal{  \"she\",     \"splendidly\",     \"team\",     \"terrible\",     \"the\",     \"their\",     \"they\",     \"this\",     \"to\",}\\\\ \\\\\n",
    "\\quad\\textnormal{  \"unfortunately\",     \"visit\",     \"walking\",     \"was\",     \"we\",     \"with\",     \"won\"}\\Bigr\\}\n",
    "\\end{array} \\tag{69.81}\n",
    "$$\n",
    "\n",
    "There is a total of $M=80$ words in this collection. We use one-hot encoding to represent the words (i.e., we use the basis vectors from $\\mathbb{R}^{80}$). For example, the word \"about\" is the fifth word in the collection and it will be represented by the feature vector $h=e_5$. In this way, if we refer to the first sentence corresponding to $d=1$: \n",
    "\n",
    "$$\n",
    "\\textnormal{<sos> It was a great play . <eos> } \\tag{69.82}\n",
    "$$\n",
    "\n",
    "we find that it consists of a sequence of $N_1=8$ words with one-hot encoded vectors given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcll}\n",
    "h_0&=& e_3,&\\;\\;\\;(\\textnormal{ sos})\\\\\n",
    "h_1&=& e_{48},&\\;\\;\\;(\\textnormal{ it})\\\\\n",
    "h_2&=& e_{77},&\\;\\;\\;(\\textnormal{ was})\\\\\n",
    "h_3&=& e_4,&\\;\\;\\;(\\textnormal{ a})\\\\\n",
    "h_4&=& e_{36},&\\;\\;\\;(\\textnormal{ great})\\\\\n",
    "h_5&=& e_{58},&\\;\\;\\;(\\textnormal{ play})\\\\\n",
    "h_6&=& e_1,&\\;\\;\\;(\\textnormal{ .})\\\\\n",
    "h_7&=& e_2,&\\;\\;\\;(\\textnormal{ eos})\n",
    "\\end{array} \\tag{69.83}\n",
    "$$\n",
    "\n",
    "where the $\\{e_m\\}$ refer to basis vectors in $\\mathbb{R}^{80}$. We further associate a label with each word. There are $11$ label types:\n",
    "\n",
    "$$\n",
    "\\textnormal{ labels}\n",
    "=\\left\\{\\begin{array}{l}\\textnormal{ start, article, verb, noun, pronoun, adjective, adverb,}\\\\\n",
    "\\textnormal{ preposition, conjunction, punctuation, end}\n",
    "\\end{array}\\right\\} \\tag{69.84}\n",
    "$$\n",
    "\n",
    "We again use one-hot encoding to represent each label by using the basis vectors from $\\mathbb{R}^{11}$. For example, the word \"beach\" is a noun and it will be associated with the label $\\gamma=e_4$, where $e_4$ is the fourth basis vector in $\\mathbb{R}^{11}$. In this way, if we return to the same sentence \n",
    "(69.82), we find that its constituent words will be represented by the following feature vectors and labels:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c|c|c|c|c}\\hline\n",
    "&\\textnormal{ <sos>}& \\textnormal{ It} &\\textnormal{ was} &\\textnormal{ a} &\\textnormal{ great}& \\textnormal{ play} &{ .} &\\textnormal{ <eos>}\\\\\\hline\\hline\n",
    "n: &0&1&2&3&4&5&6&7 \\\\\n",
    "h_n: &e_3&e_{48}&e_{77}&e_4&e_{36}&e_{58}&e_1&e_2\\\\\n",
    "\\gamma_n: &e_1&e_5&e_3&e_2&e_6&e_4&e_{10}&e_{11}\\\\\\hline\n",
    "\\textnormal{ labels}:&\\textnormal{ start}&\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ article}&\\textnormal{ adjective}&\\textnormal{ noun}&\\textnormal{ punct.}&\\textnormal{ end}\\\\\\hline\n",
    "\\end{array} \\tag{69.85}\n",
    "$$\n",
    "\n",
    "We also associate with each sentence of index $d$ a mood label given by\n",
    "\n",
    "$$\n",
    "\\gamma_d=\\begin{bmatrix}1\\\\0\\end{bmatrix}\\;\\; (\\textnormal{ bad sentiment}),\\;\\;\\;\\;\n",
    "\\gamma_d=\\begin{bmatrix}0\\\\1\\end{bmatrix}\\;\\; (\\textnormal{ good sentiment}) \\tag{69.86}\n",
    "$$\n",
    "\n",
    "In this way, by following this construction, each sentence $d$ will have some length $N_d$ (number of words including start, end, and punctuation), and will be  represented by $N_d$ feature vectors $\\{h_n\\}\\in\\mathbb{R}^{80}$ (one for each word) with $n=0,1,\\ldots, N_{d}-1$, as well as $N_d$ labels $\\{\\gamma_n\\}\\in\\mathbb{R}^{11}$ (one for each word), and a single mood label $\\gamma_d\\in\\mathbb{R}^{2}$. All representations employ one-hot encoding. \n",
    "\n",
    "The first simulation trains an RNN with $P=30$ internal nodes using algorithm \n",
    "(\\ref{kajasxafAAAL.rnn.2}) applied to a cross-entropy empirical risk with $\\mu=0.01$ and $\\rho=0.0001$. We perform $2000$ runs over the $D=15$ sentences; at the start of each run, we reshuffle the sentences randomly. At the end of the training phase, we employ the parameters $\\{W^{\\star}, U^{\\star}, V^{\\star}, \\theta^{\\star}, \\alpha^{\\star}\\}$ to perform testing. We feed the following four test sentences into the RNN: \n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\textnormal{  <sos> I do not like fruits . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> He is happy with his car . <eos>  }&\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> She is nervous about the exam . <eos>  }&\\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> It was a comfortable game . <eos>   }&\\textnormal{ (good)}\n",
    "\\end{array} \\tag{69.87}\n",
    "$$\n",
    "\n",
    "We use the RNN to determine the predicted labels $\\widehat{\\gamma}_n$ for each word in every sentence. For a given estimate $\\widehat{\\gamma}_n\\in\\mathbb{R}^{11}$, the index of its highest entry determines the type of the word. The result of this simulation is shown below, where it is seen that the RNN successfully categorizes the word types in the four sentences (for brevity, we are removing the <sos> and <eos> word categories, which have been correctly identified as well):\n",
    "\n",
    "$$\n",
    "\\begin{array}{lllllllllllll}\n",
    "\\textnormal{ I}&\\textnormal{  do}&\\textnormal{ not}&\n",
    "\\textnormal{  like}&\\textnormal{  fruits}&\\textnormal{  .} \\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{  verb}&\\textnormal{ adverb}&\n",
    "\\textnormal{  verb}&\\textnormal{  noun}&\\textnormal{  punctuation} \\\\\\\\\n",
    "\\textnormal{ He}&\\textnormal{ is}&\\textnormal{ happy}&\\textnormal{ with}&\\textnormal{ his}&\\textnormal{ car}&\\textnormal{ .}\\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ adjective}&\\textnormal{ preposition}&\\textnormal{ pronoun}&\\textnormal{ noun}&\\textnormal{ punctuation}&\\\\\\\\\n",
    "\\textnormal{ She}&\\textnormal{ is}&\\textnormal{ nervous}&\\textnormal{ about}&\\textnormal{ the}&\\textnormal{ exam}&\\textnormal{ .}\\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ adjective}&\\textnormal{ preposition}&\\textnormal{ article}&\\textnormal{ noun}&\\textnormal{ punctuation}\\\\\\\\\n",
    "\\textnormal{ It}&\\textnormal{ was}&\\textnormal{ a}&\\textnormal{ comfortable}&\\textnormal{ game}&\\textnormal{ .}\\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ article}&\\textnormal{ adjective}&\n",
    "\\textnormal{ noun}&\\textnormal{ punctuation}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The second simulation uses algorithm (69.77) to perform sentiment analysis with $\\mu=0.001$. We perform $5000$ runs over the $D=15$ sentences; at the start of each run, we reshuffle the sentences randomly. At the end of the training phase, we employ the parameters $\\{W^{\\star}, U^{\\star}, V^{\\star}, \\theta^{\\star}, \\alpha^{\\star}\\}$ to perform sentiment analysis and compute $\\widehat{\\gamma}_d$. The index of the largest entry on $\\widehat{\\gamma}_d$ decides whether the mood is bad (first entry) or good (second entry).  We feed the above four test sentences and arrive at the following predicted moods:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|l|l}\\hline\\textnormal{ Sentence}&\\textnormal{ Actual mood}&\\textnormal{ Predicted mood}\\\\\\hline\\hline\n",
    "\\textnormal{  <sos> I do not like fruits . <eos>  }&\\textnormal{ (bad)} & \\color{red}\\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> He is happy with his car . <eos>  }&\\textnormal{ (good)} & \\textnormal{ (good)}\\\\\n",
    "\\textnormal{  <sos> She is nervous about the exam . <eos>  }&\\textnormal{ (bad)} & \\textnormal{ (bad)}\\\\\n",
    "\\textnormal{  <sos> It was a comfortable game . <eos>   }&\\textnormal{ (good)} & \\textnormal{ (good)}\\\\\\hline\n",
    "\\end{array} \\tag{69.89}\n",
    "$$\n",
    "\n",
    "\\smallskip\n",
    "\n",
    "\\noindent There is one error in the prediction of the mood of the first sentence. This is a contrived example that uses a small sample set for training and it is expected that performance can be improved for more extensive training with larger datasets. We obtained similar simulation results by performing 10,000 runs of the bidirectional RNN algorithm from Section 69.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"data/datafile.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = data[\"D\"].item()\n",
    "n_test = data[\"T\"].item()\n",
    "word_types = data[\"TW\"].item()\n",
    "features_size = data[\"M\"].item()\n",
    "P = 30 # size of hidden layer for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_train = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"H_train\"]])\n",
    "H_test = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"H_test\"]])\n",
    "\n",
    "labels_train = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"labels_train\"]])\n",
    "labels_test = torch.tensor([[idx.item() if idx.shape == (1, 1) else 0 for idx in sentence] for sentence in data[\"labels_test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAnalysis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size, word_types)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceAnalysis()\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4000. Training Loss: 2.537. Training Accuracy: 0.0. Test Loss: 2.493. Test Accuracy: 0.0.\n",
      "Epoch: 100/4000. Training Loss: 1.698. Training Accuracy: 8.919. Test Loss: 1.608. Test Accuracy: 11.806.\n",
      "Epoch: 200/4000. Training Loss: 1.508. Training Accuracy: 18.915. Test Loss: 1.385. Test Accuracy: 23.611.\n",
      "Epoch: 300/4000. Training Loss: 1.338. Training Accuracy: 21.785. Test Loss: 1.201. Test Accuracy: 29.514.\n",
      "Epoch: 400/4000. Training Loss: 1.193. Training Accuracy: 27.797. Test Loss: 1.071. Test Accuracy: 35.764.\n",
      "Epoch: 500/4000. Training Loss: 1.086. Training Accuracy: 42.079. Test Loss: 0.959. Test Accuracy: 50.347.\n",
      "Epoch: 600/4000. Training Loss: 1.007. Training Accuracy: 46.693. Test Loss: 0.863. Test Accuracy: 61.806.\n",
      "Epoch: 700/4000. Training Loss: 0.94. Training Accuracy: 49.497. Test Loss: 0.8. Test Accuracy: 64.583.\n",
      "Epoch: 800/4000. Training Loss: 0.885. Training Accuracy: 52.006. Test Loss: 0.775. Test Accuracy: 70.833.\n",
      "Epoch: 900/4000. Training Loss: 0.833. Training Accuracy: 54.884. Test Loss: 0.737. Test Accuracy: 70.833.\n",
      "Epoch: 1000/4000. Training Loss: 0.787. Training Accuracy: 55.44. Test Loss: 0.715. Test Accuracy: 70.833.\n",
      "Epoch: 1100/4000. Training Loss: 0.74. Training Accuracy: 60.054. Test Loss: 0.689. Test Accuracy: 73.958.\n",
      "Epoch: 1200/4000. Training Loss: 0.695. Training Accuracy: 60.61. Test Loss: 0.674. Test Accuracy: 77.083.\n",
      "Epoch: 1300/4000. Training Loss: 0.65. Training Accuracy: 61.957. Test Loss: 0.653. Test Accuracy: 77.083.\n",
      "Epoch: 1400/4000. Training Loss: 0.605. Training Accuracy: 63.724. Test Loss: 0.653. Test Accuracy: 77.083.\n",
      "Epoch: 1500/4000. Training Loss: 0.56. Training Accuracy: 65.492. Test Loss: 0.657. Test Accuracy: 77.083.\n",
      "Epoch: 1600/4000. Training Loss: 0.517. Training Accuracy: 67.209. Test Loss: 0.646. Test Accuracy: 77.083.\n",
      "Epoch: 1700/4000. Training Loss: 0.479. Training Accuracy: 69.324. Test Loss: 0.638. Test Accuracy: 82.639.\n",
      "Epoch: 1800/4000. Training Loss: 0.444. Training Accuracy: 73.637. Test Loss: 0.636. Test Accuracy: 79.861.\n",
      "Epoch: 1900/4000. Training Loss: 0.415. Training Accuracy: 76.317. Test Loss: 0.635. Test Accuracy: 79.861.\n",
      "Epoch: 2000/4000. Training Loss: 0.388. Training Accuracy: 82.593. Test Loss: 0.621. Test Accuracy: 79.861.\n",
      "Epoch: 2100/4000. Training Loss: 0.363. Training Accuracy: 83.199. Test Loss: 0.612. Test Accuracy: 79.861.\n",
      "Epoch: 2200/4000. Training Loss: 0.342. Training Accuracy: 83.292. Test Loss: 0.61. Test Accuracy: 79.861.\n",
      "Epoch: 2300/4000. Training Loss: 0.321. Training Accuracy: 83.292. Test Loss: 0.615. Test Accuracy: 82.639.\n",
      "Epoch: 2400/4000. Training Loss: 0.301. Training Accuracy: 85.985. Test Loss: 0.6. Test Accuracy: 82.639.\n",
      "Epoch: 2500/4000. Training Loss: 0.28. Training Accuracy: 85.591. Test Loss: 0.607. Test Accuracy: 79.861.\n",
      "Epoch: 2600/4000. Training Loss: 0.264. Training Accuracy: 86.197. Test Loss: 0.602. Test Accuracy: 77.083.\n",
      "Epoch: 2700/4000. Training Loss: 0.245. Training Accuracy: 88.15. Test Loss: 0.603. Test Accuracy: 77.083.\n",
      "Epoch: 2800/4000. Training Loss: 0.228. Training Accuracy: 89.497. Test Loss: 0.603. Test Accuracy: 79.861.\n",
      "Epoch: 2900/4000. Training Loss: 0.214. Training Accuracy: 90.052. Test Loss: 0.606. Test Accuracy: 79.861.\n",
      "Epoch: 3000/4000. Training Loss: 0.202. Training Accuracy: 91.214. Test Loss: 0.618. Test Accuracy: 77.083.\n",
      "Epoch: 3100/4000. Training Loss: 0.19. Training Accuracy: 91.214. Test Loss: 0.627. Test Accuracy: 79.861.\n",
      "Epoch: 3200/4000. Training Loss: 0.177. Training Accuracy: 93.362. Test Loss: 0.625. Test Accuracy: 82.986.\n",
      "Epoch: 3300/4000. Training Loss: 0.167. Training Accuracy: 93.362. Test Loss: 0.62. Test Accuracy: 82.986.\n",
      "Epoch: 3400/4000. Training Loss: 0.156. Training Accuracy: 94.769. Test Loss: 0.643. Test Accuracy: 83.333.\n",
      "Epoch: 3500/4000. Training Loss: 0.146. Training Accuracy: 94.769. Test Loss: 0.656. Test Accuracy: 86.111.\n",
      "Epoch: 3600/4000. Training Loss: 0.139. Training Accuracy: 94.769. Test Loss: 0.697. Test Accuracy: 80.556.\n",
      "Epoch: 3700/4000. Training Loss: 0.13. Training Accuracy: 95.992. Test Loss: 0.73. Test Accuracy: 80.556.\n",
      "Epoch: 3800/4000. Training Loss: 0.123. Training Accuracy: 95.931. Test Loss: 0.732. Test Accuracy: 80.556.\n",
      "Epoch: 3900/4000. Training Loss: 0.115. Training Accuracy: 96.764. Test Loss: 0.803. Test Accuracy: 80.556.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model.train()\n",
    "    for x, y in zip(H_train, labels_train):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "        loss_out.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in zip(H_test, labels_test):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word; Word Id; Label; Label Hat\n",
      "START 3 1 1\n",
      "It 48 5 5\n",
      "was 77 3 3\n",
      "a 4 2 2\n",
      "great 36 6 6\n",
      "play 58 4 4\n",
      ". 1 10 10\n",
      "END 2 11 11\n"
     ]
    }
   ],
   "source": [
    "sentence = data[\"sentences\"][0][0].item().split(\" \")\n",
    "sentence_idx = H_train[0][:8]\n",
    "labels = labels_train[0][:8]\n",
    "labels_hat = torch.round(nn.functional.softmax(model(sentence_idx.float().unsqueeze(-1)), dim=-1))\n",
    "\n",
    "print(\"Word; Word Id; Label; Label Hat\")\n",
    "for word, word_id, label, label_hat in zip(sentence, sentence_idx, labels, labels_hat):\n",
    "    print(word, word_id.item(), label.item(), (label_hat==1).nonzero().item()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_sentiment = torch.tensor([sentence.item() for sentence in data[\"sentiment_train\"]])\n",
    "labels_test_sentiment = torch.tensor([sentence.item() for sentence in data[\"sentiment_test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0][-1])\n",
    "        y = self.sigmoid(logits)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment = SentimentAnalysis()\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(model_sentiment.parameters(), lr=lr)\n",
    "loss = nn.BCELoss()\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000. Training Loss: 0.702. Training Accuracy: 46.667. Test Loss: 0.697. Test Accuracy: 50.0.\n",
      "Epoch: 100/1000. Training Loss: 0.593. Training Accuracy: 66.667. Test Loss: 0.561. Test Accuracy: 75.0.\n",
      "Epoch: 200/1000. Training Loss: 0.343. Training Accuracy: 93.333. Test Loss: 0.365. Test Accuracy: 75.0.\n",
      "Epoch: 300/1000. Training Loss: 0.053. Training Accuracy: 100.0. Test Loss: 0.239. Test Accuracy: 75.0.\n",
      "Epoch: 400/1000. Training Loss: 0.004. Training Accuracy: 100.0. Test Loss: 0.598. Test Accuracy: 75.0.\n",
      "Epoch: 500/1000. Training Loss: 0.001. Training Accuracy: 100.0. Test Loss: 1.002. Test Accuracy: 75.0.\n",
      "Epoch: 600/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.04. Test Accuracy: 75.0.\n",
      "Epoch: 700/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.091. Test Accuracy: 75.0.\n",
      "Epoch: 800/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.247. Test Accuracy: 75.0.\n",
      "Epoch: 900/1000. Training Loss: 0.0. Training Accuracy: 100.0. Test Loss: 1.086. Test Accuracy: 75.0.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model_sentiment.train()\n",
    "    for x, y in zip(H_train, labels_train_sentiment):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "\n",
    "        y_hat = model_sentiment(x)\n",
    "\n",
    "        loss_out = loss(y_hat, y.unsqueeze(-1).float())\n",
    "        loss_out.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(y_hat)\n",
    "\n",
    "        errors = (y != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[-1] - errors)/x.shape[-1]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model_sentiment.eval()\n",
    "    for x, y in zip(H_test, labels_test_sentiment):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "\n",
    "        y_hat = model_sentiment(x)\n",
    "\n",
    "        loss_out = loss(y_hat, y.unsqueeze(-1).float())\n",
    "\n",
    "        class_hat = torch.round(y_hat)\n",
    "\n",
    "        errors = (y != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[-1] - errors)/x.shape[-1]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data[\"sentences\"][0][0].item()\n",
    "sentence_idx = H_train[0][:8]\n",
    "label = labels_train_sentiment[0]\n",
    "labels_hat = torch.round(model_sentiment(sentence_idx.float().unsqueeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START It was a great play . END 1 1.0\n",
      "START I had a bad experience with the rental car . END 0 0.0\n",
      "START Her flight was delayed for over five hours . END 0 0.0\n",
      "START She passed her exam easily . END 1 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    sentence = data[\"sentences\"][0][i].item()\n",
    "    final_len = (H_train[i] == 2).nonzero().squeeze().item()+1\n",
    "    sentence_idx = H_train[i][:final_len]\n",
    "    label = labels_train_sentiment[i]\n",
    "    label_hat = torch.round(model_sentiment(sentence_idx.float().unsqueeze(-1)))\n",
    "    print(sentence, label.item(), label_hat.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 69.8 (Predicting word types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reconsider the problem from Example 69.4 involving a small training set with $D=15$ sentences for illustration purposes. Enhanced performance would require a larger amount of training data. In this example, we employ an LSTM implementation to predict the type of future words in a sentence. For example, consider the first sentence corresponding to $d=1$:\n",
    "\n",
    "$$\n",
    "\\textnormal{ <sos> It was a great play . <eos> } \\tag{69.230}\n",
    "$$\n",
    "\n",
    "The objective is for the network to predict that the word following the start-of-sentence <sos> will be a pronoun, and the one following the pronoun will be a verb, and the one following the verb will be an article, and so forth. We will do so by changing the label $\\gamma_n$ that is assigned to each word of index $n$.\n",
    "For example, if we refer to the tabular data in (69.85) for the first sentence, the entries in the last two rows corresponding to the values of $\\gamma_n$ (and their interpretation) will be shifted to the left and take the form shown below:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c|c|c|c|c}\\hline\n",
    "&\\textnormal{ <sos>}& \\textnormal{ It} &\\textnormal{ was} &\\textnormal{ a} &\\textnormal{ great}& \\textnormal{ play} &{ .} &\\textnormal{ <eos>}\\\\\\hline\\hline\n",
    "n: &0&1&2&3&4&5&6&7 \\\\\n",
    "h_n: &e_3&e_{48}&e_{77}&e_4&e_{36}&e_{58}&e_1&e_2\\\\\n",
    "\\gamma_n: &e_5&e_3&e_2&e_6&e_4&e_{10}&e_{11}&--\\\\\\hline\n",
    "\\textnormal{ labels}:&\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ article}&\\textnormal{ adjective}&\\textnormal{ noun}&\\textnormal{ punct.}&\\textnormal{ end}&--\\\\\\hline\n",
    "\\end{array} \\tag{69.231}\n",
    "$$\n",
    "\n",
    "For instance, the label associated with the word \"great\" will now be $e_4$, which corresponds to the type \"noun\". This means that the next word in the sentence is of the type \"noun\". We therefore adjust the labels for all words in the $D=15$ sentences in this manner and perform $5000$ runs of algorithm (69.220) over the $D=15$ sentences assuming a regularized cross-entropy risk function (where we use instead the expression $\\lambda_n=\\widehat{\\gamma}_n-\\gamma_n$ in the listing of the algorithm and the output layer involves a softmax construction). At the start of each run, we reshuffle the sentences randomly. At the end of the training phase, we employ the learned parameters  to perform testing. We feed the same four test sentences into the trained LSTM: \n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textnormal{  <sos> I do not like fruits . <eos>  }\\\\\n",
    "\\textnormal{  <sos> He is happy with his car . <eos>  }\\\\\n",
    "\\textnormal{  <sos> She is nervous about the exam . <eos>  }\\\\\n",
    "\\textnormal{  <sos> It was a comfortable game . <eos>   }\n",
    "\\end{array} \\tag{69.232}\n",
    "$$\n",
    "\n",
    "\\noindent We use the LSTM to predict the word type of future words in every sentence. For a given estimate $\\widehat{\\gamma}_n\\in\\mathbb{R}^{11}$, the index of its highest entry determines the type predicted for the next word. The result of this simulation is shown below. Some errors marked in color, occur due to the limited amount of data used to train the network in this example; the intent is to  illustrate the operation of the LSTM network and its training algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{array}{llllllllllllll}\n",
    "\\textnormal{ <sos>}&\\textnormal{ I}&\\textnormal{  do}&\\textnormal{ not}&\n",
    "\\textnormal{  like}&\\textnormal{  fruits}&\\textnormal{  .} \\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{  verb}&\\textnormal{ adverb}&\n",
    "\\textnormal{  verb}&\\textnormal{  noun}&\\textnormal{  punctuation} &\\\\\\\\\n",
    "\\textnormal{ <sos>}& \\textnormal{ He}&\\textnormal{ is}&\\textnormal{ happy}&\\textnormal{ with}&\\textnormal{ his}&\\textnormal{ car}&\\textnormal{ .}\\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ adjective}&\\color{red}\\textnormal{ pronoun}&\\color{red}\\textnormal{ noun}&\\textnormal{ noun}&\\textnormal{ punctuation}&\\\\\\\\\n",
    "\\textnormal{ <sos>}& \\textnormal{ She}&\\textnormal{ is}&\\textnormal{ nervous}&\\textnormal{ about}&\\textnormal{ the}&\\textnormal{ exam}&\\textnormal{ .}\\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ adjective}&\\textnormal{ preposition}&\\color{red}\\textnormal{ pronoun}&\\textnormal{ noun}&\\color{red}\\textnormal{ adverb}&\\\\\\\\\n",
    "\\textnormal{ <sos>}& \\textnormal{ It}&\\textnormal{ was}&\\textnormal{ a}&\\textnormal{ comfortable}&\\textnormal{ game}&\\textnormal{ .}\\\\\n",
    "\\textnormal{ pronoun}&\\textnormal{ verb}&\\textnormal{ article}&\\textnormal{ adjective}&\\color{red}\\textnormal{ conjunction}&\\color{red}\\textnormal{ noun}&\n",
    "\\end{array} \\tag{69.233}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAnalysisLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size, word_types)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SentenceAnalysisLSTM()\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4000. Training Loss: 2.419. Training Accuracy: 0.0. Test Loss: 2.399. Test Accuracy: 0.0.\n",
      "Epoch: 100/4000. Training Loss: 2.01. Training Accuracy: 0.0. Test Loss: 1.973. Test Accuracy: 0.0.\n",
      "Epoch: 200/4000. Training Loss: 1.734. Training Accuracy: 10.131. Test Loss: 1.642. Test Accuracy: 11.806.\n",
      "Epoch: 300/4000. Training Loss: 1.57. Training Accuracy: 19.656. Test Loss: 1.471. Test Accuracy: 23.611.\n",
      "Epoch: 400/4000. Training Loss: 1.447. Training Accuracy: 19.656. Test Loss: 1.35. Test Accuracy: 23.611.\n",
      "Epoch: 500/4000. Training Loss: 1.338. Training Accuracy: 23.09. Test Loss: 1.228. Test Accuracy: 26.389.\n",
      "Epoch: 600/4000. Training Loss: 1.245. Training Accuracy: 25.826. Test Loss: 1.114. Test Accuracy: 29.167.\n",
      "Epoch: 700/4000. Training Loss: 1.161. Training Accuracy: 31.281. Test Loss: 1.027. Test Accuracy: 40.972.\n",
      "Epoch: 800/4000. Training Loss: 1.081. Training Accuracy: 32.443. Test Loss: 0.955. Test Accuracy: 44.097.\n",
      "Epoch: 900/4000. Training Loss: 1.01. Training Accuracy: 44.847. Test Loss: 0.894. Test Accuracy: 59.028.\n",
      "Epoch: 1000/4000. Training Loss: 0.947. Training Accuracy: 47.146. Test Loss: 0.828. Test Accuracy: 62.153.\n",
      "Epoch: 1100/4000. Training Loss: 0.889. Training Accuracy: 48.368. Test Loss: 0.781. Test Accuracy: 62.153.\n",
      "Epoch: 1200/4000. Training Loss: 0.836. Training Accuracy: 48.924. Test Loss: 0.74. Test Accuracy: 62.153.\n",
      "Epoch: 1300/4000. Training Loss: 0.792. Training Accuracy: 50.136. Test Loss: 0.714. Test Accuracy: 62.153.\n",
      "Epoch: 1400/4000. Training Loss: 0.751. Training Accuracy: 54.176. Test Loss: 0.696. Test Accuracy: 68.056.\n",
      "Epoch: 1500/4000. Training Loss: 0.712. Training Accuracy: 57.132. Test Loss: 0.672. Test Accuracy: 71.181.\n",
      "Epoch: 1600/4000. Training Loss: 0.684. Training Accuracy: 57.687. Test Loss: 0.658. Test Accuracy: 71.181.\n",
      "Epoch: 1700/4000. Training Loss: 0.644. Training Accuracy: 59.733. Test Loss: 0.645. Test Accuracy: 73.958.\n",
      "Epoch: 1800/4000. Training Loss: 0.617. Training Accuracy: 62.056. Test Loss: 0.631. Test Accuracy: 73.958.\n",
      "Epoch: 1900/4000. Training Loss: 0.585. Training Accuracy: 64.625. Test Loss: 0.623. Test Accuracy: 73.958.\n",
      "Epoch: 2000/4000. Training Loss: 0.55. Training Accuracy: 67.472. Test Loss: 0.607. Test Accuracy: 71.181.\n",
      "Epoch: 2100/4000. Training Loss: 0.524. Training Accuracy: 68.684. Test Loss: 0.616. Test Accuracy: 71.181.\n",
      "Epoch: 2200/4000. Training Loss: 0.498. Training Accuracy: 71.378. Test Loss: 0.609. Test Accuracy: 73.958.\n",
      "Epoch: 2300/4000. Training Loss: 0.473. Training Accuracy: 72.526. Test Loss: 0.596. Test Accuracy: 71.181.\n",
      "Epoch: 2400/4000. Training Loss: 0.446. Training Accuracy: 76.529. Test Loss: 0.583. Test Accuracy: 73.958.\n",
      "Epoch: 2500/4000. Training Loss: 0.423. Training Accuracy: 78.196. Test Loss: 0.572. Test Accuracy: 73.958.\n",
      "Epoch: 2600/4000. Training Loss: 0.401. Training Accuracy: 78.061. Test Loss: 0.553. Test Accuracy: 74.306.\n",
      "Epoch: 2700/4000. Training Loss: 0.379. Training Accuracy: 78.894. Test Loss: 0.548. Test Accuracy: 74.306.\n",
      "Epoch: 2800/4000. Training Loss: 0.358. Training Accuracy: 78.061. Test Loss: 0.544. Test Accuracy: 74.306.\n",
      "Epoch: 2900/4000. Training Loss: 0.339. Training Accuracy: 82.601. Test Loss: 0.551. Test Accuracy: 74.306.\n",
      "Epoch: 3000/4000. Training Loss: 0.316. Training Accuracy: 83.115. Test Loss: 0.552. Test Accuracy: 74.306.\n",
      "Epoch: 3100/4000. Training Loss: 0.298. Training Accuracy: 83.948. Test Loss: 0.556. Test Accuracy: 71.181.\n",
      "Epoch: 3200/4000. Training Loss: 0.28. Training Accuracy: 87.285. Test Loss: 0.597. Test Accuracy: 71.181.\n",
      "Epoch: 3300/4000. Training Loss: 0.267. Training Accuracy: 88.446. Test Loss: 0.601. Test Accuracy: 74.306.\n",
      "Epoch: 3400/4000. Training Loss: 0.251. Training Accuracy: 89.002. Test Loss: 0.622. Test Accuracy: 74.306.\n",
      "Epoch: 3500/4000. Training Loss: 0.233. Training Accuracy: 90.348. Test Loss: 0.608. Test Accuracy: 77.083.\n",
      "Epoch: 3600/4000. Training Loss: 0.221. Training Accuracy: 90.904. Test Loss: 0.596. Test Accuracy: 77.083.\n",
      "Epoch: 3700/4000. Training Loss: 0.207. Training Accuracy: 92.251. Test Loss: 0.572. Test Accuracy: 79.861.\n",
      "Epoch: 3800/4000. Training Loss: 0.195. Training Accuracy: 91.973. Test Loss: 0.607. Test Accuracy: 80.208.\n",
      "Epoch: 3900/4000. Training Loss: 0.183. Training Accuracy: 94.029. Test Loss: 0.634. Test Accuracy: 79.861.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model.train()\n",
    "    for x, y in zip(H_train, labels_train):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "        loss_out.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in zip(H_test, labels_test):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word; Word Id; Label; Label Hat\n",
      "START 3 1 1\n",
      "It 48 5 5\n",
      "was 77 3 3\n",
      "a 4 2 2\n",
      "great 36 6 6\n",
      "play 58 4 4\n",
      ". 1 10 10\n",
      "END 2 11 11\n"
     ]
    }
   ],
   "source": [
    "sentence = data[\"sentences\"][0][0].item().split(\" \")\n",
    "sentence_idx = H_train[0][:8]\n",
    "labels = labels_train[0][:8]\n",
    "labels_hat = torch.round(nn.functional.softmax(lstm_model(sentence_idx.float().unsqueeze(-1)), dim=-1))\n",
    "\n",
    "print(\"Word; Word Id; Label; Label Hat\")\n",
    "for word, word_id, label, label_hat in zip(sentence, sentence_idx, labels, labels_hat):\n",
    "    print(word, word_id.item(), label.item(), (label_hat==1).nonzero().item()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAnalysisBiderectionalLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=1, hidden_size=features_size, num_layers=1, bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(features_size*2, word_types)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.rnn(x)\n",
    "        logits = self.linear(x_[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_lstm_model = SentenceAnalysisBiderectionalLSTM()\n",
    "lr = 1e-5\n",
    "bidirectional_lstm_optimizer = torch.optim.Adam(bidirectional_lstm_model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4000. Training Loss: 2.37. Training Accuracy: 0.0. Test Loss: 2.385. Test Accuracy: 0.0.\n",
      "Epoch: 100/4000. Training Loss: 1.763. Training Accuracy: 0.0. Test Loss: 1.704. Test Accuracy: 0.0.\n",
      "Epoch: 200/4000. Training Loss: 1.386. Training Accuracy: 29.18. Test Loss: 1.271. Test Accuracy: 35.417.\n",
      "Epoch: 300/4000. Training Loss: 1.191. Training Accuracy: 30.898. Test Loss: 1.072. Test Accuracy: 38.194.\n",
      "Epoch: 400/4000. Training Loss: 1.057. Training Accuracy: 41.152. Test Loss: 0.956. Test Accuracy: 55.903.\n",
      "Epoch: 500/4000. Training Loss: 0.955. Training Accuracy: 45.817. Test Loss: 0.864. Test Accuracy: 55.903.\n",
      "Epoch: 600/4000. Training Loss: 0.876. Training Accuracy: 49.363. Test Loss: 0.81. Test Accuracy: 62.153.\n",
      "Epoch: 700/4000. Training Loss: 0.81. Training Accuracy: 52.242. Test Loss: 0.764. Test Accuracy: 62.153.\n",
      "Epoch: 800/4000. Training Loss: 0.751. Training Accuracy: 55.028. Test Loss: 0.74. Test Accuracy: 62.153.\n",
      "Epoch: 900/4000. Training Loss: 0.697. Training Accuracy: 59.775. Test Loss: 0.729. Test Accuracy: 71.181.\n",
      "Epoch: 1000/4000. Training Loss: 0.646. Training Accuracy: 61.048. Test Loss: 0.711. Test Accuracy: 71.181.\n",
      "Epoch: 1100/4000. Training Loss: 0.597. Training Accuracy: 64.03. Test Loss: 0.694. Test Accuracy: 71.181.\n",
      "Epoch: 1200/4000. Training Loss: 0.55. Training Accuracy: 65.191. Test Loss: 0.697. Test Accuracy: 71.181.\n",
      "Epoch: 1300/4000. Training Loss: 0.508. Training Accuracy: 70.009. Test Loss: 0.713. Test Accuracy: 71.181.\n",
      "Epoch: 1400/4000. Training Loss: 0.467. Training Accuracy: 73.309. Test Loss: 0.718. Test Accuracy: 71.181.\n",
      "Epoch: 1500/4000. Training Loss: 0.43. Training Accuracy: 79.482. Test Loss: 0.739. Test Accuracy: 71.181.\n",
      "Epoch: 1600/4000. Training Loss: 0.394. Training Accuracy: 81.495. Test Loss: 0.755. Test Accuracy: 71.181.\n",
      "Epoch: 1700/4000. Training Loss: 0.358. Training Accuracy: 82.051. Test Loss: 0.765. Test Accuracy: 74.306.\n",
      "Epoch: 1800/4000. Training Loss: 0.326. Training Accuracy: 83.869. Test Loss: 0.765. Test Accuracy: 74.306.\n",
      "Epoch: 1900/4000. Training Loss: 0.295. Training Accuracy: 84.536. Test Loss: 0.782. Test Accuracy: 71.528.\n",
      "Epoch: 2000/4000. Training Loss: 0.265. Training Accuracy: 86.549. Test Loss: 0.805. Test Accuracy: 71.528.\n",
      "Epoch: 2100/4000. Training Loss: 0.238. Training Accuracy: 88.941. Test Loss: 0.842. Test Accuracy: 71.528.\n",
      "Epoch: 2200/4000. Training Loss: 0.212. Training Accuracy: 89.608. Test Loss: 0.844. Test Accuracy: 71.528.\n",
      "Epoch: 2300/4000. Training Loss: 0.188. Training Accuracy: 91.386. Test Loss: 0.879. Test Accuracy: 71.528.\n",
      "Epoch: 2400/4000. Training Loss: 0.167. Training Accuracy: 93.608. Test Loss: 0.897. Test Accuracy: 71.528.\n",
      "Epoch: 2500/4000. Training Loss: 0.147. Training Accuracy: 95.992. Test Loss: 0.912. Test Accuracy: 71.528.\n",
      "Epoch: 2600/4000. Training Loss: 0.129. Training Accuracy: 97.566. Test Loss: 0.946. Test Accuracy: 71.528.\n",
      "Epoch: 2700/4000. Training Loss: 0.111. Training Accuracy: 98.727. Test Loss: 0.952. Test Accuracy: 71.528.\n",
      "Epoch: 2800/4000. Training Loss: 0.096. Training Accuracy: 99.394. Test Loss: 0.967. Test Accuracy: 71.528.\n",
      "Epoch: 2900/4000. Training Loss: 0.082. Training Accuracy: 100.0. Test Loss: 0.997. Test Accuracy: 71.528.\n",
      "Epoch: 3000/4000. Training Loss: 0.071. Training Accuracy: 100.0. Test Loss: 1.027. Test Accuracy: 71.528.\n",
      "Epoch: 3100/4000. Training Loss: 0.061. Training Accuracy: 100.0. Test Loss: 1.046. Test Accuracy: 71.528.\n",
      "Epoch: 3200/4000. Training Loss: 0.052. Training Accuracy: 100.0. Test Loss: 1.057. Test Accuracy: 71.528.\n",
      "Epoch: 3300/4000. Training Loss: 0.044. Training Accuracy: 100.0. Test Loss: 1.071. Test Accuracy: 71.528.\n",
      "Epoch: 3400/4000. Training Loss: 0.037. Training Accuracy: 100.0. Test Loss: 1.096. Test Accuracy: 71.528.\n",
      "Epoch: 3500/4000. Training Loss: 0.032. Training Accuracy: 100.0. Test Loss: 1.107. Test Accuracy: 71.528.\n",
      "Epoch: 3600/4000. Training Loss: 0.027. Training Accuracy: 100.0. Test Loss: 1.103. Test Accuracy: 71.528.\n",
      "Epoch: 3700/4000. Training Loss: 0.023. Training Accuracy: 100.0. Test Loss: 1.125. Test Accuracy: 71.528.\n",
      "Epoch: 3800/4000. Training Loss: 0.02. Training Accuracy: 100.0. Test Loss: 1.15. Test Accuracy: 71.528.\n",
      "Epoch: 3900/4000. Training Loss: 0.017. Training Accuracy: 100.0. Test Loss: 1.16. Test Accuracy: 74.306.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    model.train()\n",
    "    for x, y in zip(H_train, labels_train):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = bidirectional_lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "        loss_out.backward()\n",
    "        bidirectional_lstm_optimizer.step()\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        train_loss.append(loss_out.item())\n",
    "        train_acc.append(acc)\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in zip(H_test, labels_test):\n",
    "        final_len = (x == 2).nonzero().squeeze().item()+1\n",
    "\n",
    "        x = x[:final_len].float().unsqueeze(-1)\n",
    "        y_ = y[:final_len].long()-1\n",
    "\n",
    "        logits = bidirectional_lstm_model(x)\n",
    "\n",
    "        loss_out = loss(logits, y_)\n",
    "\n",
    "        class_hat = torch.round(nn.functional.softmax(logits, dim=-1))\n",
    "        classes = nn.functional.one_hot(y_)\n",
    "\n",
    "        errors = (classes != class_hat).int().sum(axis=-1).nonzero().shape[0]\n",
    "        acc = (x.shape[0] - errors)/x.shape[0]*100\n",
    "\n",
    "\n",
    "        test_loss.append(loss_out.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs}. Training Loss: {round(sum(train_loss)/len(train_loss), 3)}. Training Accuracy: {round(sum(train_acc)/len(train_acc), 3)}. Test Loss: {round(sum(test_loss)/len(test_loss), 3)}. Test Accuracy: {round(sum(test_acc)/len(test_acc), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word; Word Id; Label; Label Hat\n",
      "START 3 1 1\n",
      "It 48 5 5\n",
      "was 77 3 3\n",
      "a 4 2 2\n",
      "great 36 6 6\n",
      "play 58 4 4\n",
      ". 1 10 10\n",
      "END 2 11 11\n"
     ]
    }
   ],
   "source": [
    "sentence = data[\"sentences\"][0][0].item().split(\" \")\n",
    "sentence_idx = H_train[0][:8]\n",
    "labels = labels_train[0][:8]\n",
    "labels_hat = torch.round(nn.functional.softmax(bidirectional_lstm_model(sentence_idx.float().unsqueeze(-1)), dim=-1))\n",
    "\n",
    "print(\"Word; Word Id; Label; Label Hat\")\n",
    "for word, word_id, label, label_hat in zip(sentence, sentence_idx, labels, labels_hat):\n",
    "    print(word, word_id.item(), label.item(), (label_hat==1).nonzero().item()+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
